{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"NERC Technical Documentation NERC welcomes your contributions These pages are hosted from a git repository and contributions are welcome! Fork this repo","title":"Home"},{"location":"#nerc-technical-documentation","text":"NERC welcomes your contributions These pages are hosted from a git repository and contributions are welcome! Fork this repo","title":"NERC Technical Documentation"},{"location":"about/","text":"About NERC We are currently in the pilot phase of the project and are focusing on developing the technology to make it easy for researchers to take advantage of a suite of services ( IaaS, PaaS, SaaS ) that are not readily available today. This includes: The creation of the building blocks needed for production cloud services Begin collaboration with Systems Engineers from other institutions with well established RC groups On-board select proof of concept use cases from institutions within the MGHPCC consortium and other institutions within Massachusetts The longer term objectives will be centered around activities that will focus on: Engaging with various OpenStack communities by sharing best practices and setting standards for deployments Connecting regularly with the Mass Open Cloud (MOC) leadership to understand when new technologies they are developing with RedHat, Inc. \u2013 and as part of the new NSF funded Open Cloud Testbed \u2013 might be ready for adoption into the production NERC environment Broadening the local deployment team of NERC to include partner universities within the MGHPCC consortium. Figure 1: NERC Overview NERC production services ( red ) stand on top of the existing NESE storage services ( blue ) are built on the strong foundation of MGHPCC ( green ) that provides core facility and network access. The Innovation Hub ( grey ) enables new technologies to be rapidly adopted by the NERC or NESE services. On the far left ( purple ) are the Research and Learning communities which are the primary customers of NERC. As users proceed down the stack of production services from Web-apps, that require more technical skills, the Cloud Facilitators ( orange ) in the middle guide and educate users on how to best use the services. For more information, view NERC's concept document.","title":"About"},{"location":"about/#about-nerc","text":"We are currently in the pilot phase of the project and are focusing on developing the technology to make it easy for researchers to take advantage of a suite of services ( IaaS, PaaS, SaaS ) that are not readily available today. This includes: The creation of the building blocks needed for production cloud services Begin collaboration with Systems Engineers from other institutions with well established RC groups On-board select proof of concept use cases from institutions within the MGHPCC consortium and other institutions within Massachusetts The longer term objectives will be centered around activities that will focus on: Engaging with various OpenStack communities by sharing best practices and setting standards for deployments Connecting regularly with the Mass Open Cloud (MOC) leadership to understand when new technologies they are developing with RedHat, Inc. \u2013 and as part of the new NSF funded Open Cloud Testbed \u2013 might be ready for adoption into the production NERC environment Broadening the local deployment team of NERC to include partner universities within the MGHPCC consortium. Figure 1: NERC Overview NERC production services ( red ) stand on top of the existing NESE storage services ( blue ) are built on the strong foundation of MGHPCC ( green ) that provides core facility and network access. The Innovation Hub ( grey ) enables new technologies to be rapidly adopted by the NERC or NESE services. On the far left ( purple ) are the Research and Learning communities which are the primary customers of NERC. As users proceed down the stack of production services from Web-apps, that require more technical skills, the Cloud Facilitators ( orange ) in the middle guide and educate users on how to best use the services. For more information, view NERC's concept document.","title":"About NERC"},{"location":"get-started/create-a-user-portal-account/","text":"User Account Types NERC offers two types of user accounts: a Principal Investigator (PI) Account and a General User Account . All General Users must be assigned by a PI to their approved project by an active NERC PI account or by delegated manager(s). Principal Investigator Eligibility Information MGHPCC consortium members, whereby they enter into an service agreement with MGHPCC for the NERC services. Non-members of MGHPCC can also be PIs of NERC Services, but must also have an active non-member agreement with MGHPCC. External research focused institutions will be considered on a case-by-case basis and are subject to an external customer cost structure. PI accounts are able to request Resource Allocations. A PI account enables a user to log into NERC's computational project space; apply for allocations of NERC resources and grant access to other users; and delegate responsibilities to other collaborators from the same institutions or elsewhere as managers using NERC\u2019s ColdFront interface . Getting Started Any faculty, staff, student, and external collaborator must request a user account through the MGHPCC Shared Services (MGHPCC-SS) Account Portal . This is a web-based, single point-of-entry to the NERC system that displays a user welcome page. The welcome page of the account registration site displays instructions on how to register a General User account on NERC, which are shown in the image below: There are two options: either register for a new account or to manage an existing one. If you are new to NERC and want to register as a new MGHPCC-SS user, click on the \"Register for an Account\" button. This will redirect you to a new web page which shows details about how to register for a new MGHPCC-SS user account. NERC uses CILogon that supports login either using your Institutional Identity Provider (IdP). Clicking the \"Begin MGHPCC-SS Account Creation Process\" button will initiate the account creation process. You will be redirected to a site managed by CILogon where you will select your institutional or commercial identity provider as shown below: Once selected, you will be redirected to your institutional or commercial identity provider where you will login as shown here: At the completion of that logon, your browser will be redirected to the MGHPCC-SS Account Management site where you will complete your MGHPCC-SS account creation process. After successful logon, your browser will redirect back to the MGHPCC-SS Registration Page and ask for review and confirmation of creating your account with fetched information to complete the account creation process. Very Important If you don't click the \"Create MGHPCC-SS Account\" button, your account will not be created! So this is a very important step, to review and also click on the \"Create MGHPCC-SS Account\" button to save your information. You can make any corrections that you need and fill in any blank fields i.e. \"Research Domain\" and then click the \"Create MGHPCC-SS Account\" button. This will automatically send an email to your email address with a link to validate and confirm your account information. Once you get an email and click on the provided validation web link, you will make sure that your user account is created and valid by viewing the following page: Help If you have an institutional identity, it's preferable that you use that identity to create your MGHPCC-SS account. Institutional identities are vetted by identity management teams and provide a higher level of confidence to resource owners when granting access to resources. You can only link one university account to a MGHPCC-SS account; if you have multiple university accounts you will only be able to link one of those accounts to your MGHPCC-SS account. If at a later date, you want to change which account is connected to your MGHPCC-SS identity you can by contacting help@nerc.mghpcc.org . How to update and modify your MGHPCC-SS account information? Login your MGHPCC-SS account Click on \"Manage Your MGHPCC-SS Account\" button Review your currently saved account information, change any fields that require correction or updates and then click on the \"Update MGHPCC-SS Account\" button. This will send an email to verify your updated account information so check your email address. Confirm and validate the new account details by verifying them by clicking the provided link on your email. How to request a Principal Investigator (PI) Account? The process for requesting and obtaining PI Account is relatively simple by filling out this NERC Principal Investigator (PI) Account Request form Alternatively, PI can request for a Principal Investigator (PI) user account by submitting a new ticket at the NERC's Support Ticketing System (osTicket) under \"NERC PI Account Request\" option on Help Topic as shown in the image below: Information So, once your PI user request is reviewed and approved by NERC's administration staff - you will recieve an email confirmation form NERC's support system i.e. help@nerc.mghpcc.org. You are going to get into NERC's ColdFront resource allocation management portal using the PI user role. If you want to request a ticket you can submit it here .","title":"How to Create a User Account"},{"location":"get-started/create-a-user-portal-account/#user-account-types","text":"NERC offers two types of user accounts: a Principal Investigator (PI) Account and a General User Account . All General Users must be assigned by a PI to their approved project by an active NERC PI account or by delegated manager(s). Principal Investigator Eligibility Information MGHPCC consortium members, whereby they enter into an service agreement with MGHPCC for the NERC services. Non-members of MGHPCC can also be PIs of NERC Services, but must also have an active non-member agreement with MGHPCC. External research focused institutions will be considered on a case-by-case basis and are subject to an external customer cost structure. PI accounts are able to request Resource Allocations. A PI account enables a user to log into NERC's computational project space; apply for allocations of NERC resources and grant access to other users; and delegate responsibilities to other collaborators from the same institutions or elsewhere as managers using NERC\u2019s ColdFront interface .","title":"User Account Types"},{"location":"get-started/create-a-user-portal-account/#getting-started","text":"Any faculty, staff, student, and external collaborator must request a user account through the MGHPCC Shared Services (MGHPCC-SS) Account Portal . This is a web-based, single point-of-entry to the NERC system that displays a user welcome page. The welcome page of the account registration site displays instructions on how to register a General User account on NERC, which are shown in the image below: There are two options: either register for a new account or to manage an existing one. If you are new to NERC and want to register as a new MGHPCC-SS user, click on the \"Register for an Account\" button. This will redirect you to a new web page which shows details about how to register for a new MGHPCC-SS user account. NERC uses CILogon that supports login either using your Institutional Identity Provider (IdP). Clicking the \"Begin MGHPCC-SS Account Creation Process\" button will initiate the account creation process. You will be redirected to a site managed by CILogon where you will select your institutional or commercial identity provider as shown below: Once selected, you will be redirected to your institutional or commercial identity provider where you will login as shown here: At the completion of that logon, your browser will be redirected to the MGHPCC-SS Account Management site where you will complete your MGHPCC-SS account creation process. After successful logon, your browser will redirect back to the MGHPCC-SS Registration Page and ask for review and confirmation of creating your account with fetched information to complete the account creation process. Very Important If you don't click the \"Create MGHPCC-SS Account\" button, your account will not be created! So this is a very important step, to review and also click on the \"Create MGHPCC-SS Account\" button to save your information. You can make any corrections that you need and fill in any blank fields i.e. \"Research Domain\" and then click the \"Create MGHPCC-SS Account\" button. This will automatically send an email to your email address with a link to validate and confirm your account information. Once you get an email and click on the provided validation web link, you will make sure that your user account is created and valid by viewing the following page: Help If you have an institutional identity, it's preferable that you use that identity to create your MGHPCC-SS account. Institutional identities are vetted by identity management teams and provide a higher level of confidence to resource owners when granting access to resources. You can only link one university account to a MGHPCC-SS account; if you have multiple university accounts you will only be able to link one of those accounts to your MGHPCC-SS account. If at a later date, you want to change which account is connected to your MGHPCC-SS identity you can by contacting help@nerc.mghpcc.org .","title":"Getting Started"},{"location":"get-started/create-a-user-portal-account/#how-to-update-and-modify-your-mghpcc-ss-account-information","text":"Login your MGHPCC-SS account Click on \"Manage Your MGHPCC-SS Account\" button Review your currently saved account information, change any fields that require correction or updates and then click on the \"Update MGHPCC-SS Account\" button. This will send an email to verify your updated account information so check your email address. Confirm and validate the new account details by verifying them by clicking the provided link on your email.","title":"How to update and modify your MGHPCC-SS account information?"},{"location":"get-started/create-a-user-portal-account/#how-to-request-a-principal-investigator-pi-account","text":"The process for requesting and obtaining PI Account is relatively simple by filling out this NERC Principal Investigator (PI) Account Request form Alternatively, PI can request for a Principal Investigator (PI) user account by submitting a new ticket at the NERC's Support Ticketing System (osTicket) under \"NERC PI Account Request\" option on Help Topic as shown in the image below: Information So, once your PI user request is reviewed and approved by NERC's administration staff - you will recieve an email confirmation form NERC's support system i.e. help@nerc.mghpcc.org. You are going to get into NERC's ColdFront resource allocation management portal using the PI user role. If you want to request a ticket you can submit it here .","title":"How to request a Principal Investigator (PI) Account?"},{"location":"get-started/get-an-allocation/","text":"What is NERC's ColdFront? NERC uses NERC\u2019s ColdFront interface , an open source resource allocation management system called ColdFront to provide a single point-of-entry for administration, reporting, and measuring scientific impact of NERC resources for PI. How to get access to NERC's ColdFront General Users who are not PIs or Managers on a project see a read-only view of the NERC's ColdFront. Whereas, once a PI Account request is granted, the PI will receive an email confirming the request approval and how to connect NERC\u2019s ColdFront. PI or project managers can use NERC's ColdFront as a self-service web-portal to do the following tasks: Add or manage or archive projects Request allocations that fall under projects to NERC\u2019s resources such as clusters, cloud resources, servers, storage, and software licenses Add/remove user access to/from allocated resources who is a member of the project without requiring system administrator interaction Elevate selected users to 'manager' status, allowing them to handle some of the PI asks such as request new resource allocations, add/remove users to/from resource allocations, add project data such as grants and publications Monitor resource utilization such as storage and cloud usage Receive email notifications for expiring/renewing access to resources as well as notifications when allocations change status - i.e. activated, expired, denied Provide information such as grants, publications, and other reportable data for periodic review by center director to demonstrate need for the resources What PIs need to fill in order to request a Project? Once logged in to NERC\u2019s ColdFront, PIs can choose Projects sub-menu located under the Project menu. On clicking the \"Add a project\" button will show interface like below: PIs need to specify appropriate title, description of their research work that you will accomplish at NERC in one to two paragraphs and Field of science or research domain and then click the \"Save\" button. Once saved successfully, PIs effectively become the \"manager\" of the project, and are free to add or remove users and also request resource allocation(s) to any Projects for which they are the PI. PIs are permitted to add users to their group, request new allocations, renew expiring allocations, and provide information such as publications and grant data. PIs can maintain all their research information under one project or if they require they can separate the work into multiple projects. Very Important Make sure to select NERC (OpenStack) on Resource option and specify your expected Units of computing. Be mindful, you can extend your current resource allocations on your current project later on. Resource Allocation Quotas The amount of quota to start out a resource allocation after approval, can be specified using an integer field in the resource allocation request form as shown above. The provided unit value is computed as PI or project managers request resource quota. The basic unit of computational resources is defined in terms of integer value that corresponds to multiple openstack resource quotas. For example, 1 Unit corresponds to: Resource Name Quota Amount x Unit Instances 1 vCPUs 2 GPU 0 RAM 4096 Volumes 2 Volume Storage(GB) 100 Object Storage(GB) 1 Information By default, 2 OpenStack Floating IPs , 10 Volume Snapshots and 10 Security Groups are provided to each approved projects regardless of units of requested quota units. Adding and removing User from the Project A user can only view projects they are on. PIs or managers can add or remove users from their respective projects by navigating to the Users section of the project. Once we click on the \"Add Users\" button, it will shows the following search interface: They can search for any users in the system that are not already part of the project by providing exact matched username or partial text of other multiple fields. The search results show details about the user account such as email address, username, first name, last name etc. as shown below: Thus, found user(s) can be selected and assigned directly to the available resource allocation(s) on the given project using this interface. While adding the users, their Role also can be selected from the dropdown options as either User or Manager. Once confirmed with selection of user(s) their roles and allocations, click on the \"Add Selected Users to Project\" button. Removing Users from the Project is straightforward by just clicking on the \"Remove Users\" button. Then it shows the following interface: PI or project managers can select the user(s) and then click on the \"Remove Selected Users From Project\" button. User Roles Access to ColdFront is role based so users see a read-only view of the allocation details for any allocations they are on. PIs see the same allocation details as general users and can also add project users to the allocation if they're not already on it. Even on the first time, PIs add any user to the project as the User role. Later PI or project managers can upgrade users on their project to the 'manager' role. This allows multiple managers on the same project. This provides the user with the same access and abilities as the PI. The only things a PI can do that a manager can't, is create a new project or archive a project. All other project related tasks that a PI can do, a manager on that project can accomplish as well. General User Accounts are not able to create/update projects and request Resource Allocations. Instead, these accounts must be associated with a Project that has Resources. General User accounts that are associated with a Project have access to view their project details and use all the resources associated with the Project on NERC. General Users (not PIs or Managers) can turn off email notifications at the project level. PIs also have the 'manager' status on a project. Managers can't turn off their notifications. This ensures they continue to get allocation expiration notification emails. Adding User to Manager Role To change a user's role to 'manager' click on the edit icon next to the user's name on the Project Detail page: Then toggle the \"Role\" from User to Manager: Very Important Make sure to click the \"Update\" button to save the change. PI and Manager Allocation View PIs and managers can view important details of the allocation including start and end dates, creation and last modified dates, users on the allocation and public allocation attributes. PIs and managers can add or remove users from allocations. Adding and removing project Users to project Resource Allocation Any available users on a given project can be added to resource allocation by clicking on the \"Add Users\" button as shown below: Once Clicked it will show the following interface where PIs can select the available user(s) on the checkboxes and click on the \"Add Selected Users to Allocation\" button. Very Important The desired user must already be on the project to be added to the allocation. Removing Users from the Resource Allocation is straightforward by just clicking on the \"Remove Users\" button. Then it shows the following interface: PI or project managers can select the user(s) on the checkboxes and then click on the \"Remove Selected Users From Project\" button. Request change to Resource Allocation to an existing project If past resource allocation is not sufficient for an existing project, PIs or project managers can request for change by clicking \"Request Change\" button on project resource allocation detail page as show below: This will bring up the detailed Quota attributes for that project as shown below: PI or project managers can provide a new value for the individual quota attributes, and give justification for the requested changes so that the NERC admin can review the change request and approve or deny based on justification and quota change request. Then submitting the change request, this will notify the NERC admin about it. Please wait untill the NERC admin approves/ deny the change request to see the change on your resource allocation for the selected project. Information PI or project managers can put the new values on the textboxes for ONLY quota attributes they want to change others they can left blank so that will not get changed! Once the request is processed by the NERC admin, any user can view that request change trails for the project by looking at the \"Allocation Change Requests\" section that looks like below: Any user can click on Action button to view the details about the change request. This will shows more details about the change request like below: Adding a new Resource Allocation to the project If one resource allocation is not sufficient for a project, PI or project managers can request for another allocation(s) by clicking on the \"Request Resource Allocation\" button on the Allocations section of the project details. This will show the page where all existing users for the project will be listed on the bottom of the request form. PIs can select all or only desired user(s) to request the resource allocations to be available on NERC\u2019s OpenStack. General User View General Users who are not PIs or Managers on a project see a read-only view of the allocation details. If a user is on a project but not a particular allocation, they will not be able to see the allocation in the Project view nor will they be able to access the Allocation detail page.","title":"How to Get An Allocation"},{"location":"get-started/get-an-allocation/#what-is-nercs-coldfront","text":"NERC uses NERC\u2019s ColdFront interface , an open source resource allocation management system called ColdFront to provide a single point-of-entry for administration, reporting, and measuring scientific impact of NERC resources for PI.","title":"What is NERC's ColdFront?"},{"location":"get-started/get-an-allocation/#how-to-get-access-to-nercs-coldfront","text":"General Users who are not PIs or Managers on a project see a read-only view of the NERC's ColdFront. Whereas, once a PI Account request is granted, the PI will receive an email confirming the request approval and how to connect NERC\u2019s ColdFront. PI or project managers can use NERC's ColdFront as a self-service web-portal to do the following tasks: Add or manage or archive projects Request allocations that fall under projects to NERC\u2019s resources such as clusters, cloud resources, servers, storage, and software licenses Add/remove user access to/from allocated resources who is a member of the project without requiring system administrator interaction Elevate selected users to 'manager' status, allowing them to handle some of the PI asks such as request new resource allocations, add/remove users to/from resource allocations, add project data such as grants and publications Monitor resource utilization such as storage and cloud usage Receive email notifications for expiring/renewing access to resources as well as notifications when allocations change status - i.e. activated, expired, denied Provide information such as grants, publications, and other reportable data for periodic review by center director to demonstrate need for the resources","title":"How to get access to NERC's ColdFront"},{"location":"get-started/get-an-allocation/#what-pis-need-to-fill-in-order-to-request-a-project","text":"Once logged in to NERC\u2019s ColdFront, PIs can choose Projects sub-menu located under the Project menu. On clicking the \"Add a project\" button will show interface like below: PIs need to specify appropriate title, description of their research work that you will accomplish at NERC in one to two paragraphs and Field of science or research domain and then click the \"Save\" button. Once saved successfully, PIs effectively become the \"manager\" of the project, and are free to add or remove users and also request resource allocation(s) to any Projects for which they are the PI. PIs are permitted to add users to their group, request new allocations, renew expiring allocations, and provide information such as publications and grant data. PIs can maintain all their research information under one project or if they require they can separate the work into multiple projects. Very Important Make sure to select NERC (OpenStack) on Resource option and specify your expected Units of computing. Be mindful, you can extend your current resource allocations on your current project later on.","title":"What PIs need to fill in order to request a Project?"},{"location":"get-started/get-an-allocation/#resource-allocation-quotas","text":"The amount of quota to start out a resource allocation after approval, can be specified using an integer field in the resource allocation request form as shown above. The provided unit value is computed as PI or project managers request resource quota. The basic unit of computational resources is defined in terms of integer value that corresponds to multiple openstack resource quotas. For example, 1 Unit corresponds to: Resource Name Quota Amount x Unit Instances 1 vCPUs 2 GPU 0 RAM 4096 Volumes 2 Volume Storage(GB) 100 Object Storage(GB) 1 Information By default, 2 OpenStack Floating IPs , 10 Volume Snapshots and 10 Security Groups are provided to each approved projects regardless of units of requested quota units.","title":"Resource Allocation Quotas"},{"location":"get-started/get-an-allocation/#adding-and-removing-user-from-the-project","text":"A user can only view projects they are on. PIs or managers can add or remove users from their respective projects by navigating to the Users section of the project. Once we click on the \"Add Users\" button, it will shows the following search interface: They can search for any users in the system that are not already part of the project by providing exact matched username or partial text of other multiple fields. The search results show details about the user account such as email address, username, first name, last name etc. as shown below: Thus, found user(s) can be selected and assigned directly to the available resource allocation(s) on the given project using this interface. While adding the users, their Role also can be selected from the dropdown options as either User or Manager. Once confirmed with selection of user(s) their roles and allocations, click on the \"Add Selected Users to Project\" button. Removing Users from the Project is straightforward by just clicking on the \"Remove Users\" button. Then it shows the following interface: PI or project managers can select the user(s) and then click on the \"Remove Selected Users From Project\" button.","title":"Adding and removing User from the Project"},{"location":"get-started/get-an-allocation/#user-roles","text":"Access to ColdFront is role based so users see a read-only view of the allocation details for any allocations they are on. PIs see the same allocation details as general users and can also add project users to the allocation if they're not already on it. Even on the first time, PIs add any user to the project as the User role. Later PI or project managers can upgrade users on their project to the 'manager' role. This allows multiple managers on the same project. This provides the user with the same access and abilities as the PI. The only things a PI can do that a manager can't, is create a new project or archive a project. All other project related tasks that a PI can do, a manager on that project can accomplish as well. General User Accounts are not able to create/update projects and request Resource Allocations. Instead, these accounts must be associated with a Project that has Resources. General User accounts that are associated with a Project have access to view their project details and use all the resources associated with the Project on NERC. General Users (not PIs or Managers) can turn off email notifications at the project level. PIs also have the 'manager' status on a project. Managers can't turn off their notifications. This ensures they continue to get allocation expiration notification emails.","title":"User Roles"},{"location":"get-started/get-an-allocation/#adding-user-to-manager-role","text":"To change a user's role to 'manager' click on the edit icon next to the user's name on the Project Detail page: Then toggle the \"Role\" from User to Manager: Very Important Make sure to click the \"Update\" button to save the change.","title":"Adding User to Manager Role"},{"location":"get-started/get-an-allocation/#pi-and-manager-allocation-view","text":"PIs and managers can view important details of the allocation including start and end dates, creation and last modified dates, users on the allocation and public allocation attributes. PIs and managers can add or remove users from allocations.","title":"PI and Manager Allocation View"},{"location":"get-started/get-an-allocation/#adding-and-removing-project-users-to-project-resource-allocation","text":"Any available users on a given project can be added to resource allocation by clicking on the \"Add Users\" button as shown below: Once Clicked it will show the following interface where PIs can select the available user(s) on the checkboxes and click on the \"Add Selected Users to Allocation\" button. Very Important The desired user must already be on the project to be added to the allocation. Removing Users from the Resource Allocation is straightforward by just clicking on the \"Remove Users\" button. Then it shows the following interface: PI or project managers can select the user(s) on the checkboxes and then click on the \"Remove Selected Users From Project\" button.","title":"Adding and removing project Users to project Resource Allocation"},{"location":"get-started/get-an-allocation/#request-change-to-resource-allocation-to-an-existing-project","text":"If past resource allocation is not sufficient for an existing project, PIs or project managers can request for change by clicking \"Request Change\" button on project resource allocation detail page as show below: This will bring up the detailed Quota attributes for that project as shown below: PI or project managers can provide a new value for the individual quota attributes, and give justification for the requested changes so that the NERC admin can review the change request and approve or deny based on justification and quota change request. Then submitting the change request, this will notify the NERC admin about it. Please wait untill the NERC admin approves/ deny the change request to see the change on your resource allocation for the selected project. Information PI or project managers can put the new values on the textboxes for ONLY quota attributes they want to change others they can left blank so that will not get changed! Once the request is processed by the NERC admin, any user can view that request change trails for the project by looking at the \"Allocation Change Requests\" section that looks like below: Any user can click on Action button to view the details about the change request. This will shows more details about the change request like below:","title":"Request change to Resource Allocation to an existing project"},{"location":"get-started/get-an-allocation/#adding-a-new-resource-allocation-to-the-project","text":"If one resource allocation is not sufficient for a project, PI or project managers can request for another allocation(s) by clicking on the \"Request Resource Allocation\" button on the Allocations section of the project details. This will show the page where all existing users for the project will be listed on the bottom of the request form. PIs can select all or only desired user(s) to request the resource allocations to be available on NERC\u2019s OpenStack.","title":"Adding a new Resource Allocation to the project"},{"location":"get-started/get-an-allocation/#general-user-view","text":"General Users who are not PIs or Managers on a project see a read-only view of the allocation details. If a user is on a project but not a particular allocation, they will not be able to see the allocation in the Project view nor will they be able to access the Allocation detail page.","title":"General User View"},{"location":"get-started/user-onboarding-on-NERC/","text":"User Onboarding Process Overview NERC\u2019s Research allocations are available to faculty members and researchers, including postdoctoral researchers and students, at a U.S. based institution in New England. In order to get access to resources provided by NERC\u2019s computational infrastructure, you must first register and obtain a user account. The overall user flow can be summarized using the following sequence diagram: All users including PI need to register to NERC via: https://regapp.mss.mghpcc.org/ . PI will send a request for a Principal Investigator (PI) user account role by submitting: NERC's PI Request Form . Alternatively, PI can request for a Principal Investigator (PI) user account role by submitting a new ticket at the NERC's Support Ticketing System (osTicket) under \"NERC PI Account Request\" option on Help Topic as shown in the image below: Principal Investigator Eligibility Information MGHPCC consortium members, whereby they enter into an service agreement with MGHPCC for the NERC services. Non-members of MGHPCC can also be PIs of NERC Services, but must also have an active non-member agreement with MGHPCC. External research focused institutions will be considered on a case-by-case basis and are subject to an external customer cost structure. Wait until the PI request gets approved by the NERC\u2019s admin . Once a PI request is approved , PI can add a new project and also search and add user(s) to the project - Other general user(s) can also see the project(s) once they are added to a project via: https://coldfront.mss.mghpcc.org . PI or project Manager can request resource allocation for the newly added project and select which user(s) can use the requested allocation.* Wait until the requested resource allocation gets approved by the NERC\u2019s admin . Once approved , PI and the corresponding project users can go to NERC Openstack horizon web interface: https://stack.nerc.mghpcc.org and they can start using the NERC\u2019s OpenStack resources based on the project quotas .","title":"User Onboarding Process"},{"location":"get-started/user-onboarding-on-NERC/#user-onboarding-process-overview","text":"NERC\u2019s Research allocations are available to faculty members and researchers, including postdoctoral researchers and students, at a U.S. based institution in New England. In order to get access to resources provided by NERC\u2019s computational infrastructure, you must first register and obtain a user account. The overall user flow can be summarized using the following sequence diagram: All users including PI need to register to NERC via: https://regapp.mss.mghpcc.org/ . PI will send a request for a Principal Investigator (PI) user account role by submitting: NERC's PI Request Form . Alternatively, PI can request for a Principal Investigator (PI) user account role by submitting a new ticket at the NERC's Support Ticketing System (osTicket) under \"NERC PI Account Request\" option on Help Topic as shown in the image below: Principal Investigator Eligibility Information MGHPCC consortium members, whereby they enter into an service agreement with MGHPCC for the NERC services. Non-members of MGHPCC can also be PIs of NERC Services, but must also have an active non-member agreement with MGHPCC. External research focused institutions will be considered on a case-by-case basis and are subject to an external customer cost structure. Wait until the PI request gets approved by the NERC\u2019s admin . Once a PI request is approved , PI can add a new project and also search and add user(s) to the project - Other general user(s) can also see the project(s) once they are added to a project via: https://coldfront.mss.mghpcc.org . PI or project Manager can request resource allocation for the newly added project and select which user(s) can use the requested allocation.* Wait until the requested resource allocation gets approved by the NERC\u2019s admin . Once approved , PI and the corresponding project users can go to NERC Openstack horizon web interface: https://stack.nerc.mghpcc.org and they can start using the NERC\u2019s OpenStack resources based on the project quotas .","title":"User Onboarding Process Overview"},{"location":"migration-moc-to-nerc/Step1/","text":"Creating NERC Project and Networks This process includes some waiting for emails and approvals. It is advised to start this process and then move to step 2 and continue with these steps once you recieve approval. Account Creation & Quota Request Register for your new NERC account here . Wait for an approval email. Register to be a PI for a NERC account here . Wait for an approval email. Request the quota necessary for all of your MOC Projects to be added to NERC here (link also in PI approval email). Log in with your institution login by clicking on Log in via OpenID Connect (highlighted in yellow above). Under Projects>> Click on the name of your project (highlighted in yellow above). Scroll down until you see Request Resource Allocation (highlighted in yellow above) and click on it. Fill out the Justification (highlighted in purple above) for the quota allocation. Using your \u201cMOC Instance information\u201d table you gathered from your MOC project calculate the total number of Instances, VCPUs, RAM and use your \u201cMOC Volume Information\u201d table to calculate Disk space you will need. Using the up and down arrows (highlighted in yellow above) or by entering the number manually select the multiple of 1 Instance, 2 vCPUs, 0 GPUs, 4GB RAM, 2 Volumes and 100GB Disk and 1GB Object Storage that you will need. For example if I need 2 instances 2 vCPUs, 3GB RAM, 3 Volumes and 30GB of storage I would type in 2 or click the up arrow once to select 2 units. Click Submit (highlighted in green above). Wait for your allocation approval email. Setup Login to the Dashboard Log into the NERC OpenStack Dashboard using your OpenID Connect password. Click Connect . Select your institution from the drop down (highlighted in yellow above). Click Log On (highlighted in purple). Follow your institution's log on instructions. Setup NERC Network You are then brought to the Project>Compute>Overview location of the Dashboard. This will look very familiar as the MOC and NERC Dashboard are quite similar. Follow the instructions here to set up your network/s (you may also use the default_network if you wish). The networks don't have to exactly match the MOC. You only need the networks for creating your new instances (and accessing them once we complete the migration). Follow the instructions here to set up your router/s (you may also use the default_router if you wish). Follow the instructions here to set up your Security Group/s. This is where you can use your \u201cMOC Security Group Information\u201d table to create similar Security Groups to the ones you had in the MOC. Follow the instructions here to set up your SSH Key-pair/s.","title":"Step 1 Creating NERC Project and Networks"},{"location":"migration-moc-to-nerc/Step1/#creating-nerc-project-and-networks","text":"This process includes some waiting for emails and approvals. It is advised to start this process and then move to step 2 and continue with these steps once you recieve approval.","title":"Creating NERC Project and Networks"},{"location":"migration-moc-to-nerc/Step1/#account-creation-quota-request","text":"Register for your new NERC account here . Wait for an approval email. Register to be a PI for a NERC account here . Wait for an approval email. Request the quota necessary for all of your MOC Projects to be added to NERC here (link also in PI approval email). Log in with your institution login by clicking on Log in via OpenID Connect (highlighted in yellow above). Under Projects>> Click on the name of your project (highlighted in yellow above). Scroll down until you see Request Resource Allocation (highlighted in yellow above) and click on it. Fill out the Justification (highlighted in purple above) for the quota allocation. Using your \u201cMOC Instance information\u201d table you gathered from your MOC project calculate the total number of Instances, VCPUs, RAM and use your \u201cMOC Volume Information\u201d table to calculate Disk space you will need. Using the up and down arrows (highlighted in yellow above) or by entering the number manually select the multiple of 1 Instance, 2 vCPUs, 0 GPUs, 4GB RAM, 2 Volumes and 100GB Disk and 1GB Object Storage that you will need. For example if I need 2 instances 2 vCPUs, 3GB RAM, 3 Volumes and 30GB of storage I would type in 2 or click the up arrow once to select 2 units. Click Submit (highlighted in green above). Wait for your allocation approval email.","title":"Account Creation &amp; Quota Request"},{"location":"migration-moc-to-nerc/Step1/#setup","text":"","title":"Setup"},{"location":"migration-moc-to-nerc/Step1/#login-to-the-dashboard","text":"Log into the NERC OpenStack Dashboard using your OpenID Connect password. Click Connect . Select your institution from the drop down (highlighted in yellow above). Click Log On (highlighted in purple). Follow your institution's log on instructions.","title":"Login to the Dashboard"},{"location":"migration-moc-to-nerc/Step1/#setup-nerc-network","text":"You are then brought to the Project>Compute>Overview location of the Dashboard. This will look very familiar as the MOC and NERC Dashboard are quite similar. Follow the instructions here to set up your network/s (you may also use the default_network if you wish). The networks don't have to exactly match the MOC. You only need the networks for creating your new instances (and accessing them once we complete the migration). Follow the instructions here to set up your router/s (you may also use the default_router if you wish). Follow the instructions here to set up your Security Group/s. This is where you can use your \u201cMOC Security Group Information\u201d table to create similar Security Groups to the ones you had in the MOC. Follow the instructions here to set up your SSH Key-pair/s.","title":"Setup NERC Network"},{"location":"migration-moc-to-nerc/Step2/","text":"Identify Volumes, Instances & Security Groups on the MOC that need to be Migrated to the NERC Please read the instructions in their entirety before proceeding. Allow yourself enough time to complete them. Volume Snapshots will not be migrated. If you have a Snapshot you wish to backup please \u201cCreate Volume\u201d from it first. Confirm Access and Login to MOC Dashboard Go to the MOC Dashboard . SSO / Google Login If you have SSO through your Institution or google select Institution Account from the dropdown. Click Connect . Click on University Logins (highlighted in yellow below) if you are using SSO with your Institution. Follow your Institution's login steps after that, and skip to Gathering MOC information for the Migration . Click Google (highlighted in purple above) if your SSO is through Google. Follow standard Google login steps to get in this way, and skip to Gathering MOC information for the Migration . Keystone Credentials If you have a standard login and password leave the dropdown as Keystone Credentials. Enter your User Name. Enter your Password. Click Connect. Don't know your login? If you do not know your login information please create a Password Reset ticket . Click Open a New Ticket (highlighted in yellow above). Click the dropdown and select Forgot Pass & SSO Account Link (highlighted in blue above). In the text field (highlighted in purple above) provide the Institution email, project you are working on and the email address you used to create the account. Click Create Ticket (highlighted in yellow above) and wait for the pinwheel. You will receive an email to let you know that the MOC support staff will get back to you. Gathering MOC information for the Migration You are then brought to the Project>Compute>Overview location of the Dashboard. Create Tables to hold your information Create 3 tables of all of your Instances, your Volumes and Security Groups, for example, if you have 2 instances, 3 volumes and 2 Security Groups like the samples below your lists might look like this: MOC Instance Information Table Instance Name MOC VCPUs MOC Disk MOC RAM MOC UUID Fedora_test 1 10GB 1GB 16a1bfc2-8c90-4361-8c13-64ab40bb6207 Ubuntu_Test 1 10GB 2GB 6a40079a-59f7-407c-9e66-23bc5b749a95 total 2 20GB 3GB MOC Volume Information Table MOC Volume Name MOC Disk MOC Attached To Bootable MOC UUID NERC Volume Name Fedora 10GiB Fedora_test Yes ea45c20b-434a-4c41-8bc6-f48256fc76a8 9c73295d-fdfa-4544-b8b8-a876cc0a1e86 10GiB Ubuntu_Test Yes 9c73295d-fdfa-4544-b8b8-a876cc0a1e86 Snapshot of Fed_Test 10GiB Fedora_test No ea45c20b-434a-4c41-8bc6-f48256fc76a8 total 30GiB MOC Security Group Information Table Security Group Name Direction Ether Type IP Protocol Port Range Remote IP Prefix ssh_only_test Ingress IPv4 TCP 22 0.0.0.0/0 ping_only_test Ingress IPv4 ICMP Any 0.0.0.0/0 Gather the Instance Information Gather the Instance UUIDs (of only the instances that you need to migrate to the NERC). Click Instances (highlighted in pink in image above) Click the Instance Name (highlighted in Yellow above) of the first instance you would like to gather data on. Locate the ID row (highlighted in green above) and copy and save the ID (highlighted in purple above). This is the UUID of your first Instance. Locate the RAM, VCPUs & Disk rows (highlighted in yellow) and copy and save the associated values (highlighted in pink). Repeat this section for each Instance you have. Gather the Volume Information Gather the Volume UUIDs (of only the volumes that you need to migrate to the NERC). Click Volumes dropdown. Select Volumes (highlighted in purple above). Click the Volume Name (highlighted in yellow above) of the first volume you would like to gather data on. The name might be the same as the ID (highlighted in blue above). Locate the ID row (highlighted in green above) and copy and save the ID (highlighted in purple above). This is the UUID of your first Volume. Locate the Size row (highlighted in yellow above) and copy and save the Volume size (highlighted in pink above). Locate the Bootable row (highlighted in gray above) and copy and save the Volume size (highlighted in red above). Locate the Attached To row (highlighted in blue above) and copy and save the Instance this Volume is attached to (highlighted in orange above). If the volume is not attached to an image it will state \u201cNot attached\u201d. Repeat this section for each Volume you have. Gather your Security Group Information If you already have all of your Security Group information outside of the OpenStack Dashboard skip to the section. Gather the Security Group information (of only the security groups that you need to migrate to the NERC). Click Network dropdown Click Security Groups (highlighted in yellow above). Click Manage Rules (highlighted in yellow above) of the first Security Group you would like to gather data on. Ignore the first 2 lines (highlighted in yellow above). Write down the important information for all lines after (highlighted in blue above). Direction, Ether Type, IP Protocol, Port Range, Remote IP Prefix, Remote Security Group. Repeat this section for each security group you have.","title":"Step 2 Gathering MOC Information"},{"location":"migration-moc-to-nerc/Step2/#identify-volumes-instances-security-groups-on-the-moc-that-need-to-be-migrated-to-the-nerc","text":"Please read the instructions in their entirety before proceeding. Allow yourself enough time to complete them. Volume Snapshots will not be migrated. If you have a Snapshot you wish to backup please \u201cCreate Volume\u201d from it first.","title":"Identify Volumes, Instances &amp; Security Groups on the MOC that need to be Migrated to the NERC"},{"location":"migration-moc-to-nerc/Step2/#confirm-access-and-login-to-moc-dashboard","text":"Go to the MOC Dashboard .","title":"Confirm Access and Login to MOC Dashboard"},{"location":"migration-moc-to-nerc/Step2/#sso-google-login","text":"If you have SSO through your Institution or google select Institution Account from the dropdown. Click Connect . Click on University Logins (highlighted in yellow below) if you are using SSO with your Institution. Follow your Institution's login steps after that, and skip to Gathering MOC information for the Migration . Click Google (highlighted in purple above) if your SSO is through Google. Follow standard Google login steps to get in this way, and skip to Gathering MOC information for the Migration .","title":"SSO / Google Login"},{"location":"migration-moc-to-nerc/Step2/#keystone-credentials","text":"If you have a standard login and password leave the dropdown as Keystone Credentials. Enter your User Name. Enter your Password. Click Connect.","title":"Keystone Credentials"},{"location":"migration-moc-to-nerc/Step2/#dont-know-your-login","text":"If you do not know your login information please create a Password Reset ticket . Click Open a New Ticket (highlighted in yellow above). Click the dropdown and select Forgot Pass & SSO Account Link (highlighted in blue above). In the text field (highlighted in purple above) provide the Institution email, project you are working on and the email address you used to create the account. Click Create Ticket (highlighted in yellow above) and wait for the pinwheel. You will receive an email to let you know that the MOC support staff will get back to you.","title":"Don't know your login?"},{"location":"migration-moc-to-nerc/Step2/#gathering-moc-information-for-the-migration","text":"You are then brought to the Project>Compute>Overview location of the Dashboard.","title":"Gathering MOC information for the Migration"},{"location":"migration-moc-to-nerc/Step2/#create-tables-to-hold-your-information","text":"Create 3 tables of all of your Instances, your Volumes and Security Groups, for example, if you have 2 instances, 3 volumes and 2 Security Groups like the samples below your lists might look like this:","title":"Create Tables to hold your information"},{"location":"migration-moc-to-nerc/Step2/#moc-instance-information-table","text":"Instance Name MOC VCPUs MOC Disk MOC RAM MOC UUID Fedora_test 1 10GB 1GB 16a1bfc2-8c90-4361-8c13-64ab40bb6207 Ubuntu_Test 1 10GB 2GB 6a40079a-59f7-407c-9e66-23bc5b749a95 total 2 20GB 3GB","title":"MOC Instance Information Table"},{"location":"migration-moc-to-nerc/Step2/#moc-volume-information-table","text":"MOC Volume Name MOC Disk MOC Attached To Bootable MOC UUID NERC Volume Name Fedora 10GiB Fedora_test Yes ea45c20b-434a-4c41-8bc6-f48256fc76a8 9c73295d-fdfa-4544-b8b8-a876cc0a1e86 10GiB Ubuntu_Test Yes 9c73295d-fdfa-4544-b8b8-a876cc0a1e86 Snapshot of Fed_Test 10GiB Fedora_test No ea45c20b-434a-4c41-8bc6-f48256fc76a8 total 30GiB","title":"MOC Volume Information Table"},{"location":"migration-moc-to-nerc/Step2/#moc-security-group-information-table","text":"Security Group Name Direction Ether Type IP Protocol Port Range Remote IP Prefix ssh_only_test Ingress IPv4 TCP 22 0.0.0.0/0 ping_only_test Ingress IPv4 ICMP Any 0.0.0.0/0","title":"MOC Security Group Information Table"},{"location":"migration-moc-to-nerc/Step2/#gather-the-instance-information","text":"Gather the Instance UUIDs (of only the instances that you need to migrate to the NERC). Click Instances (highlighted in pink in image above) Click the Instance Name (highlighted in Yellow above) of the first instance you would like to gather data on. Locate the ID row (highlighted in green above) and copy and save the ID (highlighted in purple above). This is the UUID of your first Instance. Locate the RAM, VCPUs & Disk rows (highlighted in yellow) and copy and save the associated values (highlighted in pink). Repeat this section for each Instance you have.","title":"Gather the Instance Information"},{"location":"migration-moc-to-nerc/Step2/#gather-the-volume-information","text":"Gather the Volume UUIDs (of only the volumes that you need to migrate to the NERC). Click Volumes dropdown. Select Volumes (highlighted in purple above). Click the Volume Name (highlighted in yellow above) of the first volume you would like to gather data on. The name might be the same as the ID (highlighted in blue above). Locate the ID row (highlighted in green above) and copy and save the ID (highlighted in purple above). This is the UUID of your first Volume. Locate the Size row (highlighted in yellow above) and copy and save the Volume size (highlighted in pink above). Locate the Bootable row (highlighted in gray above) and copy and save the Volume size (highlighted in red above). Locate the Attached To row (highlighted in blue above) and copy and save the Instance this Volume is attached to (highlighted in orange above). If the volume is not attached to an image it will state \u201cNot attached\u201d. Repeat this section for each Volume you have.","title":"Gather the Volume Information"},{"location":"migration-moc-to-nerc/Step2/#gather-your-security-group-information","text":"If you already have all of your Security Group information outside of the OpenStack Dashboard skip to the section. Gather the Security Group information (of only the security groups that you need to migrate to the NERC). Click Network dropdown Click Security Groups (highlighted in yellow above). Click Manage Rules (highlighted in yellow above) of the first Security Group you would like to gather data on. Ignore the first 2 lines (highlighted in yellow above). Write down the important information for all lines after (highlighted in blue above). Direction, Ether Type, IP Protocol, Port Range, Remote IP Prefix, Remote Security Group. Repeat this section for each security group you have.","title":"Gather your Security Group Information"},{"location":"migration-moc-to-nerc/Step3/","text":"Steps to Migrate Volumes from MOC to NERC Create a spreadsheet to track the values you will need The values you will want to keep track of are. Label Value MOCAccess MOCSecret NERCAccess NERCSecret MOCEndPoint https://kzn-swift.massopen.cloud NERCEndPoint https://stack.nerc.mghpcc.org:13808 MinIOVolume MOCVolumeBackupID ContainerName NERCVolumeBackupID NERCVolumeName It is also helpful to have a text editor open so that you can insert the values from the spreadsheet into the commands that need to be run. Create a New MOC Mirror to NERC Instance Follow the instructions here to set up your instance. When selecting the Image please select moc-nerc-migration (highlighted in yellow above). Once the Instance is Running move onto the next step Name your new instance something you will remember, MirrorMOC2NERC for example. Assign a Floating IP to your new instance. If you need assistance please review the Floating IP steps here . Your floating IPs will not be the same as the ones you had in the MOC. Please claim new floating IPs to use. SSH into the MirrorMOC2NERC Instance. The user to use for login is centos . If you have any trouble please review the SSH steps here . Setup Application Credentials Gather MOC Application Credentials Follow the instructions here to create your Application Credentials. Make sure to save the clouds.yaml as clouds_MOC.yaml . Gathering NERC Application Credentials Follow the instructions under the header Command Line setup here to create your Application Credentials. Make sure to save the clouds.yaml as clouds_NERC.yaml . Combine the two clouds.yaml files Make a copy of clouds_MOC.yaml and save as clouds.yaml Open clouds.yaml in a text editor of your choice. Change the openstack (highlighted in yellow above) value to moc (highlighted in yellow two images below). Open clouds_NERC.yaml in a text editor of your choice. Change the openstack (highlighted in yellow above) value to nerc (highlighted in green below). Highlight and copy everything from nerc to the end of the line that starts with auth_type Paste the copied text into clouds.yaml below the line that starts with auth_type. Your new clouds.yaml will look similar to the image above. For further instructions on clouds.yaml files go Here . Moving Application Credentials to VM SSH into the VM created at the top of this page for example MirrorMOC2NERC . Create the openstack config folder and empty clouds.yaml file. mkdir -p ~/.config/openstack cd ~/.config/openstack touch clouds.yaml Open the clouds.yaml file in your favorite text editor. (vi is preinstalled). Copy the entire text inside the clouds.yaml file on your local computer. Paste the contents of the local clouds.yaml file into the clouds.yaml on the VM. Save and exit your VM text editor. Confirm the Instances are Shut Down Confirm the instances are Shut Down. This is a very important step because we will be using the force modifier when we make our backup. The volume can become corrupted if the Instance is not in a Shut Down state. Log into the Instance page of the MOC Dashboard Check the Power State of all of the instances you plan to migrate volumes from are set to Shut Down (highlighted in yellow in image above). If they are not please do so from the Actions Column. Click the drop down arrow under actions. Select Shut Off Instance (blue arrow pointing to it in image above). Backup and Move Volume Data from MOC to NERC SSH into the VM created at the top of this page. For steps on how to do this please see instructions here . Create EC2 credentials in MOC & NERC Generate credentials for Kaizen with the command below. openstack --os-cloud moc ec2 credentials create Copy the access (circled in red above) and secret (circled in blue above) values into your table as <MOCAccess> and <MOCSecret> . Generate credentials for the NERC with the command below. openstack --os-cloud nerc ec2 credentials create Copy the access (circled in red above) and secret (circled in blue above) values into your table as as <NERCAccess> and <NERCSecret> . Find Object Store Endpoints Look up information on the object-store service in MOC with the command below. openstack --os-cloud moc catalog show object-store -c endpoints If the value is different than https://kzn-swift.massopen.cloud copy the base URL for this service (circled in red above). Look up information on the object-store service in NERC with the command below. openstack --os-cloud nerc catalog show object-store -c endpoints If the value is different than https://stack.nerc.mghpcc.org:13808 copy the base URL for this service (circled in red above). Configure minio client aliases Create a MinIO alias for MOC using the base URL of the \"public\" interface of the object-store service <MOCEndPoint> and the EC2 access key (ex. <MOCAccess> ) & secret key (ex. <MOCSecret> ) from your table. $ mc alias set moc https://kzn-swift.massopen.cloud <MOCAccess> <MOCSecret> mc: Configuration written to `/home/centos/.mc/config.json`. Please update your access credentials. mc: Successfully created `/home/centos/.mc/share`. mc: Initialized share uploads `/home/centos/.mc/share/uploads.json` file. mc: Initialized share downloads `/home/centos/.mc/share/downloads.json` file. Added `moc` successfully. Create a MinIO alias for NERC using the base URL of the \"public\" interface of the object-store service <NERCEndPoint> and the EC2 access key (ex. <NERCAccess> ) & secret key (ex. <NERCSecret> ) from your table. $ mc alias set nerc https://stack.nerc.mghpcc.org:13808 <NERCAccess> <NERCSecret> Added `nerc` successfully. Backup MOC Volumes Locate the desired Volume UUID from the table you created in Step 2 Gathering MOC Information . Add the first Volume ID from your table to the code below in the <MOCVolumeID> field and create a Container Name to replace the <ContainerName> field. Container Name should be easy to remember as well as unique so include your name. Maybe something like thomasa-backups . $ openstack --os-cloud moc volume backup create --force --container <ContainerName> <MOCVolumeID> +-------+---------------------+ | Field | Value | +-------+---------------------+ | id | <MOCVolumeBackupID> | | name | None | Copy down your <MOCVolumeBackupID> to your table. Wait for the backup to become available. You can run the command below to check on the status. If your volume is 25 or larger this might be a good time to go get a warm beverage or lunch. $ openstack --os-cloud moc volume backup list +---------------------+------+-------------+-----------+------+ | ID | Name | Description | Status | Size | +---------------------+------+-------------+-----------+------+ | <MOCVolumeBackupID> | None | None | creating | 10 | ... $ openstack --os-cloud moc volume backup list +---------------------+------+-------------+-----------+------+ | ID | Name | Description | Status | Size | +---------------------+------+-------------+-----------+------+ | <MOCVolumeBackupID> | None | None | available | 10 | Gather MinIO Volume data Get the volume information for future commands. Use the same <ContainerName> from when you created the volume backup. It is worth noting that this value shares the ID number with the VolumeID. $ mc ls moc/<ContainerName> [2022-04-29 09:35:16 EDT] 0B <MinIOVolume>/ Create a Container on NERC Create the NERC container that we will send the volume to. Use the same <ContainerName> from when you created the volume backup. $ mc mb nerc/<ContainerName> Bucket created successfully `nerc/<ContainerName>`. Mirror the Volume from MOC to NERC Using the volume label from MinIO <MinIOVolume> and the <ContainerName> for the command below you will kick off the move of your volume. This takes around 30 sec per GB of data in your volume. $ mc mirror moc/<ContainerName>/<MinIOVolume> nerc/<ContainerName>/<MinIOVolume> ...123a30e_sha256file: 2.61GB / 2.61GB [=========...=========] 42.15Mib/s 1m3s Copy the Backup Record from MOC to NERC Now that we've copied the backup data into the NERC environment, we need to register the backup with the NERC backup service. We do this by copying metadata from MOC. You will need the original <MOCVolumeBackupID> you used to create the original Backup. openstack --os-cloud moc volume backup record export -f value <MOCVolumeBackupID> > record.txt Next we will import the record into NERC. $ openstack --os-cloud nerc volume backup record import -f value $(cat record.txt) <NERCVolumeBackupID> None Copy <NERCVolumeBackupID> value into your table. Create an Empty Volume on NERC to Receive the Backup Create a volume in the NERC environment to receive the backup. This must be the same size or larger than the original volume which can be changed by modifying the <size> field. Remove the \"--bootable\" flag if you are not creating a bootable volume. The <NERCVolumeName> field can be any name you want, I would suggest something that will help you keep track of what instance you want to attach it to. Make sure to fill in the table you created in Step 2 with the <NERCVolumeName> value in the NERC Volume Name column. $ openstack --os-cloud nerc volume create --bootable --size <size> <NERCVolumeName> +---------------------+----------------+ | Field | Value | +---------------------+----------------+ | attachments | [] | | availability_zone | nova | ... | id | <NERCVolumeID> | ... | size | <size> | +---------------------+----------------+ Restore the Backup Restore the Backup to the Volume you just created. openstack --os-cloud nerc volume backup restore <NERCVolumeBackupID> <NERCVolumeName> Wait for the volume to shift from restoring-backup to available . $ openstack --os-cloud nerc volume list +----------------+------------+------------------+------+-------------+ | ID | Name | Status | Size | Attached to | +----------------+------------+------------------+------+-------------+ | <NERCVolumeID> | MOC Volume | restoring-backup | 3 | Migration | $ openstack --os-cloud nerc volume list +----------------+------------+-----------+------+-------------+ | ID | Name | Status | Size | Attached to | +----------------+------------+-----------+------+-------------+ | <NERCVolumeID> | MOC Volume | available | 3 | Migration | Repeat these Backup and Move Volume Data steps for each volume you need to migrate. Create NERC Instances Using MOC Volumes If you have volumes that need to be attached to an instance please follow the next steps. Follow the instructions here to set up your instance/s. Instead of using an Image for your Boot Source you will use a Volume (orange arrow in image below). Select the <NERCVolumeName> you created in step Create an Empty Volume on NERC to Recieve the Backup The Flavor will be important as this decides how much vCPUs, RAM, and Disk this instance will consume of your total. If for some reason the earlier approved resource quota is not sufficient you can request further quota by following these steps . Repeat this section for each instance you need to create.","title":"Step 3 Moving Volumes from MOC to NERC & Creating Instance"},{"location":"migration-moc-to-nerc/Step3/#steps-to-migrate-volumes-from-moc-to-nerc","text":"","title":"Steps to Migrate Volumes from MOC to NERC"},{"location":"migration-moc-to-nerc/Step3/#create-a-spreadsheet-to-track-the-values-you-will-need","text":"The values you will want to keep track of are. Label Value MOCAccess MOCSecret NERCAccess NERCSecret MOCEndPoint https://kzn-swift.massopen.cloud NERCEndPoint https://stack.nerc.mghpcc.org:13808 MinIOVolume MOCVolumeBackupID ContainerName NERCVolumeBackupID NERCVolumeName It is also helpful to have a text editor open so that you can insert the values from the spreadsheet into the commands that need to be run.","title":"Create a spreadsheet to track the values you will need"},{"location":"migration-moc-to-nerc/Step3/#create-a-new-moc-mirror-to-nerc-instance","text":"Follow the instructions here to set up your instance. When selecting the Image please select moc-nerc-migration (highlighted in yellow above). Once the Instance is Running move onto the next step Name your new instance something you will remember, MirrorMOC2NERC for example. Assign a Floating IP to your new instance. If you need assistance please review the Floating IP steps here . Your floating IPs will not be the same as the ones you had in the MOC. Please claim new floating IPs to use. SSH into the MirrorMOC2NERC Instance. The user to use for login is centos . If you have any trouble please review the SSH steps here .","title":"Create a New MOC Mirror to NERC Instance"},{"location":"migration-moc-to-nerc/Step3/#setup-application-credentials","text":"","title":"Setup Application Credentials"},{"location":"migration-moc-to-nerc/Step3/#gather-moc-application-credentials","text":"Follow the instructions here to create your Application Credentials. Make sure to save the clouds.yaml as clouds_MOC.yaml .","title":"Gather MOC Application Credentials"},{"location":"migration-moc-to-nerc/Step3/#gathering-nerc-application-credentials","text":"Follow the instructions under the header Command Line setup here to create your Application Credentials. Make sure to save the clouds.yaml as clouds_NERC.yaml .","title":"Gathering NERC Application Credentials"},{"location":"migration-moc-to-nerc/Step3/#combine-the-two-cloudsyaml-files","text":"Make a copy of clouds_MOC.yaml and save as clouds.yaml Open clouds.yaml in a text editor of your choice. Change the openstack (highlighted in yellow above) value to moc (highlighted in yellow two images below). Open clouds_NERC.yaml in a text editor of your choice. Change the openstack (highlighted in yellow above) value to nerc (highlighted in green below). Highlight and copy everything from nerc to the end of the line that starts with auth_type Paste the copied text into clouds.yaml below the line that starts with auth_type. Your new clouds.yaml will look similar to the image above. For further instructions on clouds.yaml files go Here .","title":"Combine the two clouds.yaml files"},{"location":"migration-moc-to-nerc/Step3/#moving-application-credentials-to-vm","text":"SSH into the VM created at the top of this page for example MirrorMOC2NERC . Create the openstack config folder and empty clouds.yaml file. mkdir -p ~/.config/openstack cd ~/.config/openstack touch clouds.yaml Open the clouds.yaml file in your favorite text editor. (vi is preinstalled). Copy the entire text inside the clouds.yaml file on your local computer. Paste the contents of the local clouds.yaml file into the clouds.yaml on the VM. Save and exit your VM text editor.","title":"Moving Application Credentials to VM"},{"location":"migration-moc-to-nerc/Step3/#confirm-the-instances-are-shut-down","text":"Confirm the instances are Shut Down. This is a very important step because we will be using the force modifier when we make our backup. The volume can become corrupted if the Instance is not in a Shut Down state. Log into the Instance page of the MOC Dashboard Check the Power State of all of the instances you plan to migrate volumes from are set to Shut Down (highlighted in yellow in image above). If they are not please do so from the Actions Column. Click the drop down arrow under actions. Select Shut Off Instance (blue arrow pointing to it in image above).","title":"Confirm the Instances are Shut Down"},{"location":"migration-moc-to-nerc/Step3/#backup-and-move-volume-data-from-moc-to-nerc","text":"SSH into the VM created at the top of this page. For steps on how to do this please see instructions here .","title":"Backup and Move Volume Data from MOC to NERC"},{"location":"migration-moc-to-nerc/Step3/#create-ec2-credentials-in-moc-nerc","text":"Generate credentials for Kaizen with the command below. openstack --os-cloud moc ec2 credentials create Copy the access (circled in red above) and secret (circled in blue above) values into your table as <MOCAccess> and <MOCSecret> . Generate credentials for the NERC with the command below. openstack --os-cloud nerc ec2 credentials create Copy the access (circled in red above) and secret (circled in blue above) values into your table as as <NERCAccess> and <NERCSecret> .","title":"Create EC2 credentials in MOC &amp; NERC"},{"location":"migration-moc-to-nerc/Step3/#find-object-store-endpoints","text":"Look up information on the object-store service in MOC with the command below. openstack --os-cloud moc catalog show object-store -c endpoints If the value is different than https://kzn-swift.massopen.cloud copy the base URL for this service (circled in red above). Look up information on the object-store service in NERC with the command below. openstack --os-cloud nerc catalog show object-store -c endpoints If the value is different than https://stack.nerc.mghpcc.org:13808 copy the base URL for this service (circled in red above).","title":"Find Object Store Endpoints"},{"location":"migration-moc-to-nerc/Step3/#configure-minio-client-aliases","text":"Create a MinIO alias for MOC using the base URL of the \"public\" interface of the object-store service <MOCEndPoint> and the EC2 access key (ex. <MOCAccess> ) & secret key (ex. <MOCSecret> ) from your table. $ mc alias set moc https://kzn-swift.massopen.cloud <MOCAccess> <MOCSecret> mc: Configuration written to `/home/centos/.mc/config.json`. Please update your access credentials. mc: Successfully created `/home/centos/.mc/share`. mc: Initialized share uploads `/home/centos/.mc/share/uploads.json` file. mc: Initialized share downloads `/home/centos/.mc/share/downloads.json` file. Added `moc` successfully. Create a MinIO alias for NERC using the base URL of the \"public\" interface of the object-store service <NERCEndPoint> and the EC2 access key (ex. <NERCAccess> ) & secret key (ex. <NERCSecret> ) from your table. $ mc alias set nerc https://stack.nerc.mghpcc.org:13808 <NERCAccess> <NERCSecret> Added `nerc` successfully.","title":"Configure minio client aliases"},{"location":"migration-moc-to-nerc/Step3/#backup-moc-volumes","text":"Locate the desired Volume UUID from the table you created in Step 2 Gathering MOC Information . Add the first Volume ID from your table to the code below in the <MOCVolumeID> field and create a Container Name to replace the <ContainerName> field. Container Name should be easy to remember as well as unique so include your name. Maybe something like thomasa-backups . $ openstack --os-cloud moc volume backup create --force --container <ContainerName> <MOCVolumeID> +-------+---------------------+ | Field | Value | +-------+---------------------+ | id | <MOCVolumeBackupID> | | name | None | Copy down your <MOCVolumeBackupID> to your table. Wait for the backup to become available. You can run the command below to check on the status. If your volume is 25 or larger this might be a good time to go get a warm beverage or lunch. $ openstack --os-cloud moc volume backup list +---------------------+------+-------------+-----------+------+ | ID | Name | Description | Status | Size | +---------------------+------+-------------+-----------+------+ | <MOCVolumeBackupID> | None | None | creating | 10 | ... $ openstack --os-cloud moc volume backup list +---------------------+------+-------------+-----------+------+ | ID | Name | Description | Status | Size | +---------------------+------+-------------+-----------+------+ | <MOCVolumeBackupID> | None | None | available | 10 |","title":"Backup MOC Volumes"},{"location":"migration-moc-to-nerc/Step3/#gather-minio-volume-data","text":"Get the volume information for future commands. Use the same <ContainerName> from when you created the volume backup. It is worth noting that this value shares the ID number with the VolumeID. $ mc ls moc/<ContainerName> [2022-04-29 09:35:16 EDT] 0B <MinIOVolume>/","title":"Gather MinIO Volume data"},{"location":"migration-moc-to-nerc/Step3/#create-a-container-on-nerc","text":"Create the NERC container that we will send the volume to. Use the same <ContainerName> from when you created the volume backup. $ mc mb nerc/<ContainerName> Bucket created successfully `nerc/<ContainerName>`.","title":"Create a Container on NERC"},{"location":"migration-moc-to-nerc/Step3/#mirror-the-volume-from-moc-to-nerc","text":"Using the volume label from MinIO <MinIOVolume> and the <ContainerName> for the command below you will kick off the move of your volume. This takes around 30 sec per GB of data in your volume. $ mc mirror moc/<ContainerName>/<MinIOVolume> nerc/<ContainerName>/<MinIOVolume> ...123a30e_sha256file: 2.61GB / 2.61GB [=========...=========] 42.15Mib/s 1m3s","title":"Mirror the Volume from MOC to NERC"},{"location":"migration-moc-to-nerc/Step3/#copy-the-backup-record-from-moc-to-nerc","text":"Now that we've copied the backup data into the NERC environment, we need to register the backup with the NERC backup service. We do this by copying metadata from MOC. You will need the original <MOCVolumeBackupID> you used to create the original Backup. openstack --os-cloud moc volume backup record export -f value <MOCVolumeBackupID> > record.txt Next we will import the record into NERC. $ openstack --os-cloud nerc volume backup record import -f value $(cat record.txt) <NERCVolumeBackupID> None Copy <NERCVolumeBackupID> value into your table.","title":"Copy the Backup Record from MOC to NERC"},{"location":"migration-moc-to-nerc/Step3/#create-an-empty-volume-on-nerc-to-receive-the-backup","text":"Create a volume in the NERC environment to receive the backup. This must be the same size or larger than the original volume which can be changed by modifying the <size> field. Remove the \"--bootable\" flag if you are not creating a bootable volume. The <NERCVolumeName> field can be any name you want, I would suggest something that will help you keep track of what instance you want to attach it to. Make sure to fill in the table you created in Step 2 with the <NERCVolumeName> value in the NERC Volume Name column. $ openstack --os-cloud nerc volume create --bootable --size <size> <NERCVolumeName> +---------------------+----------------+ | Field | Value | +---------------------+----------------+ | attachments | [] | | availability_zone | nova | ... | id | <NERCVolumeID> | ... | size | <size> | +---------------------+----------------+","title":"Create an Empty Volume on NERC to Receive the Backup"},{"location":"migration-moc-to-nerc/Step3/#restore-the-backup","text":"Restore the Backup to the Volume you just created. openstack --os-cloud nerc volume backup restore <NERCVolumeBackupID> <NERCVolumeName> Wait for the volume to shift from restoring-backup to available . $ openstack --os-cloud nerc volume list +----------------+------------+------------------+------+-------------+ | ID | Name | Status | Size | Attached to | +----------------+------------+------------------+------+-------------+ | <NERCVolumeID> | MOC Volume | restoring-backup | 3 | Migration | $ openstack --os-cloud nerc volume list +----------------+------------+-----------+------+-------------+ | ID | Name | Status | Size | Attached to | +----------------+------------+-----------+------+-------------+ | <NERCVolumeID> | MOC Volume | available | 3 | Migration | Repeat these Backup and Move Volume Data steps for each volume you need to migrate.","title":"Restore the Backup"},{"location":"migration-moc-to-nerc/Step3/#create-nerc-instances-using-moc-volumes","text":"If you have volumes that need to be attached to an instance please follow the next steps. Follow the instructions here to set up your instance/s. Instead of using an Image for your Boot Source you will use a Volume (orange arrow in image below). Select the <NERCVolumeName> you created in step Create an Empty Volume on NERC to Recieve the Backup The Flavor will be important as this decides how much vCPUs, RAM, and Disk this instance will consume of your total. If for some reason the earlier approved resource quota is not sufficient you can request further quota by following these steps . Repeat this section for each instance you need to create.","title":"Create NERC Instances Using MOC Volumes"},{"location":"migration-moc-to-nerc/Step4/","text":"Remove Volume Backups to Conserve Storage If you find yourself low on Volume Storage please follow the steps below to remove your old Volume Backups. If you are very low on space you can do this every time you finish copying a new volume to the NERC. If on the other hand you have plety of remaining space feel free to leave all of your Volume Backups as they are. SSH into the MirrorMOC2NERC Instance . The user to use for login is centos . If you have any trouble please review the SSH steps here . Check Remaining MOC Volume Storage Log into the MOC Dashboard and go to Project > Compute > Overview. Look at the Volume Storage meter (highlighted in yellow in image above). Delete MOC Volume Backups Gather a list of current MOC Volume Backups with the command below. $ openstack --os-cloud moc volume backup list +---------------------+------+-------------+-----------+------+ | ID | Name | Description | Status | Size | +---------------------+------+-------------+-----------+------+ | <MOCVolumeBackupID> | None | None | available | 10 | Only remove Volume Backups you are sure have been moved to the NERC. with the command below you can delete Volume Backups. openstack --os-cloud moc volume backup delete <MOCVolumeBackupID> Repeat the MOC Volume Backup section for all MOC Volume Backups you wish to remove. Check Remaing NERC Volume Storage Log into the NERC Dashboard and go to Project > Compute > Overview. Look at the Volume Storage meter (highlighted in yellow in image above). Delete NERC Volume Backups Gather a list of current NERC Volume Backups with the command below. $ openstack --os-cloud nerc volume backup list +---------------------+------+-------------+-----------+------+ | ID | Name | Description | Status | Size | +---------------------+------+-------------+-----------+------+ | <MOCVolumeBackupID> | None | None | available | 3 | Only remove Volume Backups you are sure have been migrated to NERC Volumes. Keep in mind that you might not have named the volume the same as on the MOC so check your table from Step 2 to confirm.You can confirm what Volumes you have in NERC with the following command. $ openstack --os-cloud nerc volume list +----------------+------------------+--------+------+----------------------------------+ | ID | Name | Status | Size | Attached to | +----------------+------------------+--------+------+----------------------------------+ | <NERCVolumeID> | <NERCVolumeName> | in-use | 3 | Attached to MOC2NERC on /dev/vda | To remove volume backups please use the command below. openstack --os-cloud nerc volume backup delete <MOCVolumeBackupID> Repeat the NERC Volume Backup section for all NERC Volume Backups you wish to remove.","title":"Step 4 Remove Volume Backups to Conserve Storage"},{"location":"migration-moc-to-nerc/Step4/#remove-volume-backups-to-conserve-storage","text":"If you find yourself low on Volume Storage please follow the steps below to remove your old Volume Backups. If you are very low on space you can do this every time you finish copying a new volume to the NERC. If on the other hand you have plety of remaining space feel free to leave all of your Volume Backups as they are. SSH into the MirrorMOC2NERC Instance . The user to use for login is centos . If you have any trouble please review the SSH steps here .","title":"Remove Volume Backups to Conserve Storage"},{"location":"migration-moc-to-nerc/Step4/#check-remaining-moc-volume-storage","text":"Log into the MOC Dashboard and go to Project > Compute > Overview. Look at the Volume Storage meter (highlighted in yellow in image above).","title":"Check Remaining MOC Volume Storage"},{"location":"migration-moc-to-nerc/Step4/#delete-moc-volume-backups","text":"Gather a list of current MOC Volume Backups with the command below. $ openstack --os-cloud moc volume backup list +---------------------+------+-------------+-----------+------+ | ID | Name | Description | Status | Size | +---------------------+------+-------------+-----------+------+ | <MOCVolumeBackupID> | None | None | available | 10 | Only remove Volume Backups you are sure have been moved to the NERC. with the command below you can delete Volume Backups. openstack --os-cloud moc volume backup delete <MOCVolumeBackupID> Repeat the MOC Volume Backup section for all MOC Volume Backups you wish to remove.","title":"Delete MOC Volume Backups"},{"location":"migration-moc-to-nerc/Step4/#check-remaing-nerc-volume-storage","text":"Log into the NERC Dashboard and go to Project > Compute > Overview. Look at the Volume Storage meter (highlighted in yellow in image above).","title":"Check Remaing NERC Volume Storage"},{"location":"migration-moc-to-nerc/Step4/#delete-nerc-volume-backups","text":"Gather a list of current NERC Volume Backups with the command below. $ openstack --os-cloud nerc volume backup list +---------------------+------+-------------+-----------+------+ | ID | Name | Description | Status | Size | +---------------------+------+-------------+-----------+------+ | <MOCVolumeBackupID> | None | None | available | 3 | Only remove Volume Backups you are sure have been migrated to NERC Volumes. Keep in mind that you might not have named the volume the same as on the MOC so check your table from Step 2 to confirm.You can confirm what Volumes you have in NERC with the following command. $ openstack --os-cloud nerc volume list +----------------+------------------+--------+------+----------------------------------+ | ID | Name | Status | Size | Attached to | +----------------+------------------+--------+------+----------------------------------+ | <NERCVolumeID> | <NERCVolumeName> | in-use | 3 | Attached to MOC2NERC on /dev/vda | To remove volume backups please use the command below. openstack --os-cloud nerc volume backup delete <MOCVolumeBackupID> Repeat the NERC Volume Backup section for all NERC Volume Backups you wish to remove.","title":"Delete NERC Volume Backups"},{"location":"openstack/","text":"OpenStack Tutorial Index If you're just starting out, we recommend starting from Access the OpenStack Dashboard and going through the tutorial in order. If you just need to review a specific step, you can find the page you need in the list below. Logging In Access the OpenStack Dashboard <<-- Start Here Dashboard Overview Access and Security Security Groups Create a Key Pair Create & Connect to the VM Launch a VM Available Images Available NOVA Flavors Assign a Floating IP SSH to Cloud VM Advanced OpenStack Topics Setting Up Your Own Network Set up your own Private Network Create a Router Domain or Host Name for your VM Domain Name System (DNS) OpenStack CLI OpenStack CLI Launch a VM using OpenStack CLI Persistent Storage Block Storage/ Volumes/ Cinder Object Storage/ Swift Backup your instance and volume Backup with snapshots Using Terraform to provision NERC resources Terraform on NERC Python SDK Python SDK Setting Up Your Own Images Microsoft Windows image","title":"OpenStack"},{"location":"openstack/#openstack-tutorial-index","text":"If you're just starting out, we recommend starting from Access the OpenStack Dashboard and going through the tutorial in order. If you just need to review a specific step, you can find the page you need in the list below.","title":"OpenStack Tutorial Index"},{"location":"openstack/#logging-in","text":"Access the OpenStack Dashboard <<-- Start Here Dashboard Overview","title":"Logging In"},{"location":"openstack/#access-and-security","text":"Security Groups Create a Key Pair","title":"Access and Security"},{"location":"openstack/#create-connect-to-the-vm","text":"Launch a VM Available Images Available NOVA Flavors Assign a Floating IP SSH to Cloud VM","title":"Create &amp; Connect to the VM"},{"location":"openstack/#advanced-openstack-topics","text":"","title":"Advanced OpenStack Topics"},{"location":"openstack/#setting-up-your-own-network","text":"Set up your own Private Network Create a Router","title":"Setting Up Your Own Network"},{"location":"openstack/#domain-or-host-name-for-your-vm","text":"Domain Name System (DNS)","title":"Domain or Host Name for your VM"},{"location":"openstack/#openstack-cli","text":"OpenStack CLI Launch a VM using OpenStack CLI","title":"OpenStack CLI"},{"location":"openstack/#persistent-storage","text":"Block Storage/ Volumes/ Cinder Object Storage/ Swift","title":"Persistent Storage"},{"location":"openstack/#backup-your-instance-and-volume","text":"Backup with snapshots","title":"Backup your instance and volume"},{"location":"openstack/#using-terraform-to-provision-nerc-resources","text":"Terraform on NERC","title":"Using Terraform to provision NERC resources"},{"location":"openstack/#python-sdk","text":"Python SDK","title":"Python SDK"},{"location":"openstack/#setting-up-your-own-images","text":"Microsoft Windows image","title":"Setting Up Your Own Images"},{"location":"openstack/access-and-security/create-a-key-pair/","text":"Create a Key-pair NOTE If you will be using PuTTY on Windows, please read this first . Add a Key Pair For security, the VM images have password authentication disabled by default, so you will need to use an SSH key pair to log in. You can view key pairs by clicking Project, then click Compute panel and choose Key Pairs from the tabs that appears. This shows the key pairs that are available for this project. Import a Key Pair Prerequisite You need ssh installed in your system You can create a key pair on your local machine, then upload the public key to the cloud. This is the recommended method . Open a terminal and type the following commands (in this example, we have named the key cloud.key, but you can name it anything you want): cd ~/.ssh ssh-keygen -t rsa -f ~/.ssh/cloud.key -C \"label_your_key\" Example: You will be prompted to create a passphrase for the key. IMPORTANT: Do not forget the passphrase! If you do, you will be unable to use your key. This process creates two files in your .ssh folder: cloud.key # private key - don\u2019t share this with anyone, and never upload # it anywhere ever cloud.key.pub # this is your public key Pro Tip The -C \"label\" field is not required, but it is useful to quickly identify different public keys later. You could use your email address as the label, or a user@host tag that identifies the computer the key is for. For example, if Bob has both a laptop and a desktop computer that he will, he might use -C \"Bob@laptop\" to label the key he generates on the laptop, and -C \"Bob@desktop\" for the desktop.* On your terminal: pbcopy < ~/.ssh/cloud.key.pub #copies the contents of public key to your clipboard Pro Tip If pbcopy isn't working, you can locate the hidden .ssh folder, open the file in your favorite text editor, and copy it to your clipboard. Go back to the Openstack Dashboard, where you should still be on the Key Pairs tab (If not, find it under Project -> Compute -> Key Pairs) Choose \"Import Public Key\". Give the key a name in the \"Key Pair Name\" Box, choose \"SSH Key\" as the Key Type dropdown option and paste the public key that you just copied in the \"Public Key\" text box. Click \"Import Public Key\". You will see your key pair appear in the list. You can now skip ahead to Adding the key to an ssh-agent . Create a Key Pair If you are having trouble creating a key pair with the instructions above, the Openstack dashboard can make one for you. Click \"Create a Key Pair\", and enter a name for the key pair. Click on \"Create a Key Pair\" button. You will be prompted to download a .pem file containing your private key. In the example, we have named the key 'cloud_key.pem', but you can name it anything. Save this file to your hard drive, for example in your Downloads folder. Copy this key inside the .ssh folder on your local machine/laptop, using the following steps: cd ~/Downloads # Navigate to the folder where you saved the .pem file mv cloud.pem ~/.ssh/ # This command will copy the key you downloaded to # your .ssh folder. cd ~/.ssh # Navigate to your .ssh folder chmod 400 cloud.pem # Change the permissions of the file To see your public key, navigate to Project -> Compute -> Key Pairs You should see your key in the list. If you click on the name of the newly added key, you will see a screen of information that includes details about your public key: The public key is the part of the key you distribute to VMs and remote servers. You may find it convenient to paste it into a file inside your .ssh folder, so you don't always need to log into the website to see it. Call the file something like cloud_key.pub to distinguish it from your private key. Important: Never share your private key with anyone, or upload it to a server! Adding your SSH key to the ssh-agent If you have many VMs, you will most likely be using one or two VMs with public IPs as a gateway to others which are not reachable from the internet. In order to be able to use your key for multiple SSH hops, do NOT copy your private key to the gateway VM! The correct method to use Agent Forwarding, which adds the key to an ssh-agent on your local machine and 'forwards' it over the SSH connection. If ssh-agent is not already running in background, you need to start the ssh-agent in the background. eval \"$(ssh-agent -s)\" > Agent pid 59566 Then, add the key to your ssh agent: cd ~/.ssh ssh-add cloud.key Identity added: cloud.key (test_user@laptop) Check that it is added with the command ssh-add -l 2048 SHA256:D0DLuODzs15j2OaZnA8I52aEeY3exRT2PCsUyAXgI24 test_user@laptop (RSA) Depending on your system, you might have to repeat these steps after you reboot or log out of your computer. You can always check if your ssh key is added by running the ssh-add -l command. A key with the default name id_rsa will be added by default at login, although you will still need to unlock it with your passphrase the first time you use it. Once the key is added, you will be able to forward it over an SSH connection, like this: ssh -A -i cloud.key <username>@<remote-host-IP> Connecting via SSH is discussed in more detail later in the tutorial ( SSH to Cloud VM ); for now, just proceed to the next step below. SSH keys with PuTTY on Windows PuTTY requires SSH keys to be in its own ppk format. To convert between OpenSSH keys used by OpenStack and PuTTY's format, you need a utility called PuTTYgen. If it was not installed when you originally installed PuTTY, you can get it here: Download PuTTY . You have 2 options for generating keys that will work with PuTTY: Generate an OpenSSH key with ssh-keygen or from the Horizon GUI using the instructions above, then use PuTTYgen to convert the private key to .ppk Generate a .ppk key with PuTTYgen, and import the provided OpenSSH public key to OpenStack using the 'Import a Key Pair' instructions above . There is a detailed walkthrough of how to use PuTTYgen here: Use SSH Keys with PuTTY on Windows .","title":"Create a Key Pair"},{"location":"openstack/access-and-security/create-a-key-pair/#create-a-key-pair","text":"NOTE If you will be using PuTTY on Windows, please read this first .","title":"Create a Key-pair"},{"location":"openstack/access-and-security/create-a-key-pair/#add-a-key-pair","text":"For security, the VM images have password authentication disabled by default, so you will need to use an SSH key pair to log in. You can view key pairs by clicking Project, then click Compute panel and choose Key Pairs from the tabs that appears. This shows the key pairs that are available for this project.","title":"Add a Key Pair"},{"location":"openstack/access-and-security/create-a-key-pair/#import-a-key-pair","text":"Prerequisite You need ssh installed in your system You can create a key pair on your local machine, then upload the public key to the cloud. This is the recommended method . Open a terminal and type the following commands (in this example, we have named the key cloud.key, but you can name it anything you want): cd ~/.ssh ssh-keygen -t rsa -f ~/.ssh/cloud.key -C \"label_your_key\" Example: You will be prompted to create a passphrase for the key. IMPORTANT: Do not forget the passphrase! If you do, you will be unable to use your key. This process creates two files in your .ssh folder: cloud.key # private key - don\u2019t share this with anyone, and never upload # it anywhere ever cloud.key.pub # this is your public key Pro Tip The -C \"label\" field is not required, but it is useful to quickly identify different public keys later. You could use your email address as the label, or a user@host tag that identifies the computer the key is for. For example, if Bob has both a laptop and a desktop computer that he will, he might use -C \"Bob@laptop\" to label the key he generates on the laptop, and -C \"Bob@desktop\" for the desktop.* On your terminal: pbcopy < ~/.ssh/cloud.key.pub #copies the contents of public key to your clipboard Pro Tip If pbcopy isn't working, you can locate the hidden .ssh folder, open the file in your favorite text editor, and copy it to your clipboard. Go back to the Openstack Dashboard, where you should still be on the Key Pairs tab (If not, find it under Project -> Compute -> Key Pairs) Choose \"Import Public Key\". Give the key a name in the \"Key Pair Name\" Box, choose \"SSH Key\" as the Key Type dropdown option and paste the public key that you just copied in the \"Public Key\" text box. Click \"Import Public Key\". You will see your key pair appear in the list. You can now skip ahead to Adding the key to an ssh-agent .","title":"Import a Key Pair"},{"location":"openstack/access-and-security/create-a-key-pair/#create-a-key-pair_1","text":"If you are having trouble creating a key pair with the instructions above, the Openstack dashboard can make one for you. Click \"Create a Key Pair\", and enter a name for the key pair. Click on \"Create a Key Pair\" button. You will be prompted to download a .pem file containing your private key. In the example, we have named the key 'cloud_key.pem', but you can name it anything. Save this file to your hard drive, for example in your Downloads folder. Copy this key inside the .ssh folder on your local machine/laptop, using the following steps: cd ~/Downloads # Navigate to the folder where you saved the .pem file mv cloud.pem ~/.ssh/ # This command will copy the key you downloaded to # your .ssh folder. cd ~/.ssh # Navigate to your .ssh folder chmod 400 cloud.pem # Change the permissions of the file To see your public key, navigate to Project -> Compute -> Key Pairs You should see your key in the list. If you click on the name of the newly added key, you will see a screen of information that includes details about your public key: The public key is the part of the key you distribute to VMs and remote servers. You may find it convenient to paste it into a file inside your .ssh folder, so you don't always need to log into the website to see it. Call the file something like cloud_key.pub to distinguish it from your private key. Important: Never share your private key with anyone, or upload it to a server!","title":"Create a Key Pair"},{"location":"openstack/access-and-security/create-a-key-pair/#adding-your-ssh-key-to-the-ssh-agent","text":"If you have many VMs, you will most likely be using one or two VMs with public IPs as a gateway to others which are not reachable from the internet. In order to be able to use your key for multiple SSH hops, do NOT copy your private key to the gateway VM! The correct method to use Agent Forwarding, which adds the key to an ssh-agent on your local machine and 'forwards' it over the SSH connection. If ssh-agent is not already running in background, you need to start the ssh-agent in the background. eval \"$(ssh-agent -s)\" > Agent pid 59566 Then, add the key to your ssh agent: cd ~/.ssh ssh-add cloud.key Identity added: cloud.key (test_user@laptop) Check that it is added with the command ssh-add -l 2048 SHA256:D0DLuODzs15j2OaZnA8I52aEeY3exRT2PCsUyAXgI24 test_user@laptop (RSA) Depending on your system, you might have to repeat these steps after you reboot or log out of your computer. You can always check if your ssh key is added by running the ssh-add -l command. A key with the default name id_rsa will be added by default at login, although you will still need to unlock it with your passphrase the first time you use it. Once the key is added, you will be able to forward it over an SSH connection, like this: ssh -A -i cloud.key <username>@<remote-host-IP> Connecting via SSH is discussed in more detail later in the tutorial ( SSH to Cloud VM ); for now, just proceed to the next step below.","title":"Adding your SSH key to the ssh-agent"},{"location":"openstack/access-and-security/create-a-key-pair/#ssh-keys-with-putty-on-windows","text":"PuTTY requires SSH keys to be in its own ppk format. To convert between OpenSSH keys used by OpenStack and PuTTY's format, you need a utility called PuTTYgen. If it was not installed when you originally installed PuTTY, you can get it here: Download PuTTY . You have 2 options for generating keys that will work with PuTTY: Generate an OpenSSH key with ssh-keygen or from the Horizon GUI using the instructions above, then use PuTTYgen to convert the private key to .ppk Generate a .ppk key with PuTTYgen, and import the provided OpenSSH public key to OpenStack using the 'Import a Key Pair' instructions above . There is a detailed walkthrough of how to use PuTTYgen here: Use SSH Keys with PuTTY on Windows .","title":"SSH keys with PuTTY on Windows"},{"location":"openstack/access-and-security/security-groups/","text":"Security Groups Before you launch an instance, you should add security group rules to enable users to ping and use SSH to connect to the instance. Security groups are sets of IP filter rules that define networking access and are applied to all instances within a project. To do so, you either add rules to the default security group Add a rule to the default security group or add a new security group with rules. You can view security groups by clicking Project, then click Network panel and choose Security Groups from the tabs that appears. You should see a \u2018default\u2019 security group. The default security group allows traffic only between members of the security group, so by default you can always connect between VMs in this group. However, it blocks all traffic from outside, including incoming SSH connections. In order to access instances via a public IP, an additional security group is needed. Security groups are very highly configurable, so you can create different security groups for different types of VMs used in your project. For example, for a VM that hosts a web page, you need a security group which allows access to ports 80 and 443. You can also limit access based on where the traffic originates, using either IP addresses or security groups to define the allowed sources. Create a new Security Group Click on \"Create Security Group\" Give your new group a name, and a brief description. You will see some existing rules: Let's create the new rule to allow SSH. Click on \"Add Rule\". You will see there are a lot of options you can configure on the Add Rule dialog box. Enter the following values: Rule: SSH Remote: CIDR CIDR: 0.0.0.0/0 Note To accept requests from a particular range of IP addresses, specify the IP address block in the CIDR box. The new rule now appears in the list. This signifies that any instances using this newly added Security Group will now have SSH port 22 open for requests from any IP address. Allowing Ping The default configuration blocks ping responses, so you will need to add an additional group and/or rule if you want your public IPs to respond to ping requests. Ping is ICMP traffic, so the easiest way to allow it is to add a new rule and choose \"ALL ICMP\" from the dropdown. In the Add Rule dialog box, enter the following values: Rule: All ICMP Direction: Ingress Remote: CIDR CIDR: 0.0.0.0/0 Instances will now accept all incoming ICMP packets.","title":"Security Groups"},{"location":"openstack/access-and-security/security-groups/#security-groups","text":"Before you launch an instance, you should add security group rules to enable users to ping and use SSH to connect to the instance. Security groups are sets of IP filter rules that define networking access and are applied to all instances within a project. To do so, you either add rules to the default security group Add a rule to the default security group or add a new security group with rules. You can view security groups by clicking Project, then click Network panel and choose Security Groups from the tabs that appears. You should see a \u2018default\u2019 security group. The default security group allows traffic only between members of the security group, so by default you can always connect between VMs in this group. However, it blocks all traffic from outside, including incoming SSH connections. In order to access instances via a public IP, an additional security group is needed. Security groups are very highly configurable, so you can create different security groups for different types of VMs used in your project. For example, for a VM that hosts a web page, you need a security group which allows access to ports 80 and 443. You can also limit access based on where the traffic originates, using either IP addresses or security groups to define the allowed sources.","title":"Security Groups"},{"location":"openstack/access-and-security/security-groups/#create-a-new-security-group","text":"Click on \"Create Security Group\" Give your new group a name, and a brief description. You will see some existing rules: Let's create the new rule to allow SSH. Click on \"Add Rule\". You will see there are a lot of options you can configure on the Add Rule dialog box. Enter the following values: Rule: SSH Remote: CIDR CIDR: 0.0.0.0/0 Note To accept requests from a particular range of IP addresses, specify the IP address block in the CIDR box. The new rule now appears in the list. This signifies that any instances using this newly added Security Group will now have SSH port 22 open for requests from any IP address.","title":"Create a new Security Group"},{"location":"openstack/access-and-security/security-groups/#allowing-ping","text":"The default configuration blocks ping responses, so you will need to add an additional group and/or rule if you want your public IPs to respond to ping requests. Ping is ICMP traffic, so the easiest way to allow it is to add a new rule and choose \"ALL ICMP\" from the dropdown. In the Add Rule dialog box, enter the following values: Rule: All ICMP Direction: Ingress Remote: CIDR CIDR: 0.0.0.0/0 Instances will now accept all incoming ICMP packets.","title":"Allowing Ping"},{"location":"openstack/advanced-openstack-topics/backup/backup-with-snapshots/","text":"Backup with snapshots When you start a new instance, you can choose the Instance Boot Source from the following list: boot from image boot from instance snapshot boot from volume boot from volume snapshot When the instance boots from image or from snapshot, the instance has an ephemeral disk. This means that the disk of the instance is not listed under the \"Volumes\" list. This also means that if you delete the instance all the data on that disk is lost forever. To make a backup of the ephemeral disk you will need to create a snapshot of instance itself. The resulting snapshot in this case will be a openstack image type. You will be able to find it listed in the \"Compute -> Images\" list. When the instance boots from volume this means that it will use an existing volume listed in the \"Volumes\" menu. In this case the volume is persistent and will not be deleted even if the instance is deleted, unless explicitely selected the Yes option on \"Delete Volume on Instance Delete\". Create and use Instance snapshots The OpenStack snapshot mechanism allows you to create new images from your instances while they are either running or stopped. This mainly serves two purposes: * As a backup mechanism: save the main disk of your instance to an image and later boot a new instance from this image with the saved data. As a templating mechanism: customise and upgrade a base image and save it to use as a template for new instances. How to create an instance snapshot Using the CLI Prerequisites To run the OpenStack CLI commands, you need to have: OpenStack CLI setup, see OpenStack Command Line setup for more information. To snapshot an instance to an image using the CLI, do this: Using the openstack client openstack server image create --name <name of my snapshot> --wait <instance name or uuid> To view newly created snapshot image openstack image show --fit-width <name of my snapshot> Using this snapshot, the VM can be rolled back to the previous state with a server rebuild. openstack server rebuild --image <name of my snapshot> <existing instance name or uuid> For e.g. openstack server image create --name my-snapshot --wait test-nerc-0 openstack image show --fit-width my-snapshot openstack server rebuild --image my-snapshot test-nerc-0 Important Information During the time it takes to do the snapshot, the machine can become unresponsive. Using Horizon dashboard Once you\u2019re logged in to NERC's Horizon dashboard, you can create a snapshot via the \"Compute -> Instances\" page by clicking on the \"Create snapshot\" action button on desired instance as shown below: Once created, you can find the image listed under Images in the Horizon dashboard: You have the option to launch this image as a new instance, or by clicking on the arrow next to Launch, create a volume from the image, edit details about the image, update the image metadata, or delete it: You can then select the snapshot when creating a new instance or directly click \"Launch\" button to use the snapshot image to launch a new instance. Live snapshots and data consistency We call a snapshot taken against a running instance with no downtime a \"live snapshot\". These snapshots are simply disk-only snapshots, and may be inconsistent if the instance's OS is not aware of the snapshot being taken. Take and use Volume Snapshots Volume snapshots You can also create snapshots of a volume, that then later can be used to create other volumes or to rollback to a precedent point in time. You can take a snapshot of volume that may or may not be attached to an instance. Snapshot of available volumes or volumes that are not attached to an instance does not affect the data on the volume. Snapshot of a volume serves as a backup for the persistent data on the volume at a given point in time. Snapshots are of the size of the actual data existing on the volume at the time at which the snapshot is taken. The creation of a snapshot takes a few seconds and it can be done while the volume is in-use. Warning Taking snapshots of volumes that are in use or attached to active instances can result in data inconsistency on the volume. Once you have the snapshot, you can use it to create other volumes based on this snapshot. Creation time for these volumes may depend on the type of the volume you are creating as it may entitle some data transfer. How to create a volume snapshot Using the OpenStack CLI Prerequisites To run the OpenStack CLI commands, you need to have: OpenStack CLI setup, see OpenStack Command Line setup for more information. To snapshot an instance to an image using the CLI, do this: Using the openstack client commands $ openstack volume snapshot create --volume <volume name or uuid> <name of my snapshot> For e.g. $ openstack volume snapshot create --volume test_volume my-volume-snapshot +-------------+--------------------------------------+ | Field | Value | +-------------+--------------------------------------+ | created_at | 2022-04-12T19:48:42.707250 | | description | None | | id | f1cf6846-4aba-4eb8-b3e4-2ff309f8f599 | | name | my-volume-snapshot | | properties | | | size | 25 | | status | creating | | updated_at | None | | volume_id | f2630d21-f8f5-4f02-adc7-14a3aa72cc9d | +-------------+--------------------------------------+ Important Information if the volume is in-use, you may need to specify --force You can list the volume snapshots with the following command. $ openstack volume snapshot list For e.g. $ openstack volume snapshot list +--------------------------------------+--------------------+-------------+-----------+------+ | ID | Name | Description | Status | Size | +--------------------------------------+--------------------+-------------+-----------+------+ | f1cf6846-4aba-4eb8-b3e4-2ff309f8f599 | my-volume-snapshot | None | available | 25 | +--------------------------------------+--------------------+-------------+-----------+------+ Once the volume snapshot is in available state, then you can create other volumes based on that snapshot. You don't need to specify the size of the volume, it will use the size of the snapshot. openstack volume create --description --source <name of my snapshot> \"Volume from an snapshot\" <volume name or uuid> You can delete the snapshots just by issuing the following command $ openstack volume snapshot delete <name of my snapshot> For e.g. $ openstack volume snapshot delete my-volume-snapshot Using NERC's Horizon dashboard Once you\u2019re logged in to NERC's Horizon dashboard, you can create a snapshot via the \"Volumes\" menu by clicking on the \"Create Snapshot\" action button on desired volume as shown below: In the dialog box that opens, enter a snapshot name and a brief description. Once a snapshot is created, you can manage them under the Volumes menu in the Horizon dashboard under Volume Snapshots: Create Volume from Snapshot: In the dialog box that opens, enter a volume name and a brief description. Any snapshots made into volumes can be found under Volumes: Information Keep in mind that any volumes and snapshots stored take up space in your project. Delete any you no longer need to conserve space.","title":"Backup with snapshots"},{"location":"openstack/advanced-openstack-topics/backup/backup-with-snapshots/#backup-with-snapshots","text":"When you start a new instance, you can choose the Instance Boot Source from the following list: boot from image boot from instance snapshot boot from volume boot from volume snapshot When the instance boots from image or from snapshot, the instance has an ephemeral disk. This means that the disk of the instance is not listed under the \"Volumes\" list. This also means that if you delete the instance all the data on that disk is lost forever. To make a backup of the ephemeral disk you will need to create a snapshot of instance itself. The resulting snapshot in this case will be a openstack image type. You will be able to find it listed in the \"Compute -> Images\" list. When the instance boots from volume this means that it will use an existing volume listed in the \"Volumes\" menu. In this case the volume is persistent and will not be deleted even if the instance is deleted, unless explicitely selected the Yes option on \"Delete Volume on Instance Delete\".","title":"Backup with snapshots"},{"location":"openstack/advanced-openstack-topics/backup/backup-with-snapshots/#create-and-use-instance-snapshots","text":"The OpenStack snapshot mechanism allows you to create new images from your instances while they are either running or stopped. This mainly serves two purposes: * As a backup mechanism: save the main disk of your instance to an image and later boot a new instance from this image with the saved data. As a templating mechanism: customise and upgrade a base image and save it to use as a template for new instances.","title":"Create and use Instance snapshots"},{"location":"openstack/advanced-openstack-topics/backup/backup-with-snapshots/#how-to-create-an-instance-snapshot","text":"","title":"How to create an instance snapshot"},{"location":"openstack/advanced-openstack-topics/backup/backup-with-snapshots/#using-the-cli","text":"Prerequisites To run the OpenStack CLI commands, you need to have: OpenStack CLI setup, see OpenStack Command Line setup for more information. To snapshot an instance to an image using the CLI, do this:","title":"Using the CLI"},{"location":"openstack/advanced-openstack-topics/backup/backup-with-snapshots/#using-the-openstack-client","text":"openstack server image create --name <name of my snapshot> --wait <instance name or uuid>","title":"Using the openstack client"},{"location":"openstack/advanced-openstack-topics/backup/backup-with-snapshots/#to-view-newly-created-snapshot-image","text":"openstack image show --fit-width <name of my snapshot> Using this snapshot, the VM can be rolled back to the previous state with a server rebuild. openstack server rebuild --image <name of my snapshot> <existing instance name or uuid> For e.g. openstack server image create --name my-snapshot --wait test-nerc-0 openstack image show --fit-width my-snapshot openstack server rebuild --image my-snapshot test-nerc-0 Important Information During the time it takes to do the snapshot, the machine can become unresponsive.","title":"To view newly created snapshot image"},{"location":"openstack/advanced-openstack-topics/backup/backup-with-snapshots/#using-horizon-dashboard","text":"Once you\u2019re logged in to NERC's Horizon dashboard, you can create a snapshot via the \"Compute -> Instances\" page by clicking on the \"Create snapshot\" action button on desired instance as shown below: Once created, you can find the image listed under Images in the Horizon dashboard: You have the option to launch this image as a new instance, or by clicking on the arrow next to Launch, create a volume from the image, edit details about the image, update the image metadata, or delete it: You can then select the snapshot when creating a new instance or directly click \"Launch\" button to use the snapshot image to launch a new instance. Live snapshots and data consistency We call a snapshot taken against a running instance with no downtime a \"live snapshot\". These snapshots are simply disk-only snapshots, and may be inconsistent if the instance's OS is not aware of the snapshot being taken.","title":"Using Horizon dashboard"},{"location":"openstack/advanced-openstack-topics/backup/backup-with-snapshots/#take-and-use-volume-snapshots","text":"","title":"Take and use Volume Snapshots"},{"location":"openstack/advanced-openstack-topics/backup/backup-with-snapshots/#volume-snapshots","text":"You can also create snapshots of a volume, that then later can be used to create other volumes or to rollback to a precedent point in time. You can take a snapshot of volume that may or may not be attached to an instance. Snapshot of available volumes or volumes that are not attached to an instance does not affect the data on the volume. Snapshot of a volume serves as a backup for the persistent data on the volume at a given point in time. Snapshots are of the size of the actual data existing on the volume at the time at which the snapshot is taken. The creation of a snapshot takes a few seconds and it can be done while the volume is in-use. Warning Taking snapshots of volumes that are in use or attached to active instances can result in data inconsistency on the volume. Once you have the snapshot, you can use it to create other volumes based on this snapshot. Creation time for these volumes may depend on the type of the volume you are creating as it may entitle some data transfer.","title":"Volume snapshots"},{"location":"openstack/advanced-openstack-topics/backup/backup-with-snapshots/#how-to-create-a-volume-snapshot","text":"","title":"How to create a volume snapshot"},{"location":"openstack/advanced-openstack-topics/backup/backup-with-snapshots/#using-the-openstack-cli","text":"Prerequisites To run the OpenStack CLI commands, you need to have: OpenStack CLI setup, see OpenStack Command Line setup for more information. To snapshot an instance to an image using the CLI, do this:","title":"Using the OpenStack CLI"},{"location":"openstack/advanced-openstack-topics/backup/backup-with-snapshots/#using-the-openstack-client-commands","text":"$ openstack volume snapshot create --volume <volume name or uuid> <name of my snapshot> For e.g. $ openstack volume snapshot create --volume test_volume my-volume-snapshot +-------------+--------------------------------------+ | Field | Value | +-------------+--------------------------------------+ | created_at | 2022-04-12T19:48:42.707250 | | description | None | | id | f1cf6846-4aba-4eb8-b3e4-2ff309f8f599 | | name | my-volume-snapshot | | properties | | | size | 25 | | status | creating | | updated_at | None | | volume_id | f2630d21-f8f5-4f02-adc7-14a3aa72cc9d | +-------------+--------------------------------------+ Important Information if the volume is in-use, you may need to specify --force You can list the volume snapshots with the following command. $ openstack volume snapshot list For e.g. $ openstack volume snapshot list +--------------------------------------+--------------------+-------------+-----------+------+ | ID | Name | Description | Status | Size | +--------------------------------------+--------------------+-------------+-----------+------+ | f1cf6846-4aba-4eb8-b3e4-2ff309f8f599 | my-volume-snapshot | None | available | 25 | +--------------------------------------+--------------------+-------------+-----------+------+ Once the volume snapshot is in available state, then you can create other volumes based on that snapshot. You don't need to specify the size of the volume, it will use the size of the snapshot. openstack volume create --description --source <name of my snapshot> \"Volume from an snapshot\" <volume name or uuid> You can delete the snapshots just by issuing the following command $ openstack volume snapshot delete <name of my snapshot> For e.g. $ openstack volume snapshot delete my-volume-snapshot","title":"Using the openstack client commands"},{"location":"openstack/advanced-openstack-topics/backup/backup-with-snapshots/#using-nercs-horizon-dashboard","text":"Once you\u2019re logged in to NERC's Horizon dashboard, you can create a snapshot via the \"Volumes\" menu by clicking on the \"Create Snapshot\" action button on desired volume as shown below: In the dialog box that opens, enter a snapshot name and a brief description. Once a snapshot is created, you can manage them under the Volumes menu in the Horizon dashboard under Volume Snapshots: Create Volume from Snapshot: In the dialog box that opens, enter a volume name and a brief description. Any snapshots made into volumes can be found under Volumes: Information Keep in mind that any volumes and snapshots stored take up space in your project. Delete any you no longer need to conserve space.","title":"Using NERC's Horizon dashboard"},{"location":"openstack/advanced-openstack-topics/domain-name-system/domain-names-for-your-vms/","text":"DNS services in NERC OpenStack What is DNS? The Domain Name System (DNS) is a ranked and distributed system for naming resources connected to a network, and works by storing various types of record, such as an IP address associated with a domain name. DNS simplifies the communication between computers and servers through a network and provides a user-friendly method for users to interact with and get the desired information. How to get user-friendly domain names for your NERC VMs? NERC does not currently offer integrated domain name service management. You can use one of the following methods to configure name resolution (DNS) for your NERC's virtual instances. 1. Using freely available free Dynamic DNS services Get a free domain or host name from no-ip.com or other free Dynamic DNS services . Here we will describe how to use No-IP to configure dynamic DNS. Step 1: Create your No-IP Account. During this process you can add your desired unique hostname with pre-existing domain name or you can choose to create your hostname later on. Step 2: Confirm Your Account by verifing your email address. Step 3: Log In to Your Account to view your dashboard. Step 4: Add Floating IP of your instance to the Hostname. Click on \"Modify\" to add your own floating IP attached to your NERC virtual instance. Then, browse your host or domain name as you setup during registration or later i.e. http://nerc.hopto.org on above example. Easy video tutorial can be found here . Having a free option is great for quick demonstrate your project but this has the following restrictions: 2. Using your local Research Computing (RC) department or academic institution's Central IT services You need to contact and work with your Research Computing department or academic institution's Central IT services to create A record for your hostname that maps to the address of a floating IP of your NERC virtual instance. A record: The primary DNS record used to connect your domain to an IP address that directs visitors to your website. 3. Using commercial DNS providers Alternatively, you can purchase a fully registered domain name or host name from commercial hosting providers and then register DNS records for your virtual instance from commercial cloud servies i.e. AWS Route53, Azure DNS, CloudFlare, Google Cloud Platform, GoDaddy, etc.","title":"Set up Domain names for your VMs"},{"location":"openstack/advanced-openstack-topics/domain-name-system/domain-names-for-your-vms/#dns-services-in-nerc-openstack","text":"","title":"DNS services in NERC OpenStack"},{"location":"openstack/advanced-openstack-topics/domain-name-system/domain-names-for-your-vms/#what-is-dns","text":"The Domain Name System (DNS) is a ranked and distributed system for naming resources connected to a network, and works by storing various types of record, such as an IP address associated with a domain name. DNS simplifies the communication between computers and servers through a network and provides a user-friendly method for users to interact with and get the desired information.","title":"What is DNS?"},{"location":"openstack/advanced-openstack-topics/domain-name-system/domain-names-for-your-vms/#how-to-get-user-friendly-domain-names-for-your-nerc-vms","text":"NERC does not currently offer integrated domain name service management. You can use one of the following methods to configure name resolution (DNS) for your NERC's virtual instances.","title":"How to get user-friendly domain names for your NERC VMs?"},{"location":"openstack/advanced-openstack-topics/domain-name-system/domain-names-for-your-vms/#1-using-freely-available-free-dynamic-dns-services","text":"Get a free domain or host name from no-ip.com or other free Dynamic DNS services . Here we will describe how to use No-IP to configure dynamic DNS. Step 1: Create your No-IP Account. During this process you can add your desired unique hostname with pre-existing domain name or you can choose to create your hostname later on. Step 2: Confirm Your Account by verifing your email address. Step 3: Log In to Your Account to view your dashboard. Step 4: Add Floating IP of your instance to the Hostname. Click on \"Modify\" to add your own floating IP attached to your NERC virtual instance. Then, browse your host or domain name as you setup during registration or later i.e. http://nerc.hopto.org on above example. Easy video tutorial can be found here . Having a free option is great for quick demonstrate your project but this has the following restrictions:","title":"1. Using freely available free Dynamic DNS services"},{"location":"openstack/advanced-openstack-topics/domain-name-system/domain-names-for-your-vms/#2-using-your-local-research-computing-rc-department-or-academic-institutions-central-it-services","text":"You need to contact and work with your Research Computing department or academic institution's Central IT services to create A record for your hostname that maps to the address of a floating IP of your NERC virtual instance. A record: The primary DNS record used to connect your domain to an IP address that directs visitors to your website.","title":"2. Using your local Research Computing (RC) department or academic institution's Central IT services"},{"location":"openstack/advanced-openstack-topics/domain-name-system/domain-names-for-your-vms/#3-using-commercial-dns-providers","text":"Alternatively, you can purchase a fully registered domain name or host name from commercial hosting providers and then register DNS records for your virtual instance from commercial cloud servies i.e. AWS Route53, Azure DNS, CloudFlare, Google Cloud Platform, GoDaddy, etc.","title":"3. Using commercial DNS providers"},{"location":"openstack/advanced-openstack-topics/openstack-cli/launch-a-VM-using-openstack-CLI/","text":"Launch a VM using OpenStack CLI First find the following details using openstack command, we would required these details during the creation of virtual machine. Flavor Image Network Security Group Key Name Get the flavor list using below openstack command: [user@laptop ~]$ openstack flavor list +--------------------------------------+------------+--------+------+-----------+-------+-----------+ | ID | Name | RAM | Disk | Ephemeral | VCPUs | Is Public | +--------------------------------------+------------+--------+------+-----------+-------+-----------+ | 12ded228-1a7f-4d35-b994-7dd394a6ca90 | gpu-a100.2 | 196608 | 20 | 0 | 24 | True | | 15581358-3e81-4cf2-a5b8-c0fd2ad771b4 | mem-a.8 | 65536 | 20 | 0 | 8 | True | | 17521416-0ecf-4d85-8d4c-ec6fd1bc5f9d | cpu-a.1 | 2048 | 20 | 0 | 1 | True | | 2b1dbea2-736d-4b85-b466-4410bba35f1e | cpu-a.8 | 16384 | 20 | 0 | 8 | True | | 2f33578f-c3df-4210-b369-84a998d77dac | mem-a.4 | 32768 | 20 | 0 | 4 | True | | 4498bfdb-5342-4e51-aa20-9ee74e522d59 | mem-a.1 | 8192 | 20 | 0 | 1 | True | | 4e43e6df-3637-4363-a7cd-732fbf9e7cfd | gpu-a100.4 | 393216 | 20 | 0 | 48 | True | | 7f2f5f4e-684b-4c24-bfc6-3fce9cf1f446 | mem-a.16 | 131072 | 20 | 0 | 16 | True | | 8c05db2f-6696-446b-9319-c32341a09c41 | cpu-a.16 | 32768 | 20 | 0 | 16 | True | | 9662b5b2-aeaa-4d56-9bd3-450deee668af | cpu-a.4 | 8192 | 20 | 0 | 4 | True | | b3377fdd-fd0f-4c88-9b4b-3b5c8ada0732 | gpu-a100.1 | 98304 | 20 | 0 | 12 | True | | e9125ab0-c8df-4488-a252-029c636cbd0f | mem-a.2 | 16384 | 20 | 0 | 2 | True | | ee6417bd-7cd4-4431-a6ce-d09f0fba3ba9 | cpu-a.2 | 4096 | 20 | 0 | 2 | True | +--------------------------------------+------------+--------+------+-----------+-------+-----------+ Get the image name and its ID, [user@laptop ~]$ openstack image list | grep centos | 41eafa05-c264-4840-8c17-746e6a388c2d | centos-7-x86_64 | active | Get Private Virtual network details, which will be attached to the VM: [user@laptop ~]$ openstack network list +--------------------------------------+-----------------+--------------------------------------+ | ID | Name | Subnets | +--------------------------------------+-----------------+--------------------------------------+ | 43613b84-e1fb-44a4-b1ea-c530edc49018 | provider | 1cbbb98d-3b57-4f6d-8053-46045904d910 | | 8a91900b-d43c-474d-b913-930283e0bf43 | default_network | e62ce2fd-b11c-44ce-b7cc-4ca943e75a23 | +--------------------------------------+-----------------+--------------------------------------+ Find the Security Group: [user@laptop ~]$ openstack security group list +--------------------------------------+----------------------------------+------------------------+----------------------------------+------+ | ID | Name | Description | Project | Tags | +--------------------------------------+----------------------------------+------------------------+----------------------------------+------+ | 8285530a-34e3-4d96-8e01-a7b309a91f9f | default | Default security group | 8ae3ae25c3a84c689cd24c48785ca23a | [] | | bbb738d0-45fb-4a9a-8bc4-a3eafeb49ba7 | ssh_only | | 8ae3ae25c3a84c689cd24c48785ca23a | [] | +--------------------------------------+----------------------------------+------------------------+----------------------------------+------+ Find the Key pair, in my case you can choose your own, [user@laptop ~]$ openstack keypair list | grep -i cloud_key | cloud_key | d5:ab:dc:1f:e5:08:44:7f:a6:21:47:23:85:32:cc:04 | ssh | Note Above details will be different for you based on your project and env. Launch an instance from an Image Now we have all the details, let\u2019s create a virtual machine using \"openstack server create\" command Syntax : $ openstack server create --flavor {Flavor-Name-Or-Flavor-ID } \\ --image {Image-Name-Or-Image-ID} \\ --nic net-id={Network-ID} \\ --user-data USER-DATA-FILE \\ --security-group {Security_Group_ID} \\ --key-name {Keypair-Name} \\ --property KEY=VALUE \\ <Instance_Name> Important Note If you boot an instance with an \" Instance_Name \" greater than 63 characters , Compute truncates it automatically when turning it into a hostname to ensure the correct functionality of dnsmasq . Optionally, you can provide a key name for access control and a security group for security. You can also include metadata key and value pairs: --key-name {Keypair-Name} . For example, you can add a description for your server by providing the --property description=\"My Server\" parameter. You can pass user data in a local file at instance launch by using the --user-data USER-DATA-FILE parameter. If you do not provide a key pair, you will be unable to access the instance. You can also place arbitrary local files into the instance file system at creation time by using the --file <dest-filename=source-filename> parameter. You can store up to five files. For example, if you have a special authorized keys file named special_authorized_keysfile that you want to put on the instance rather than using the regular SSH key injection, you can add the \u2013file option as shown in the following example. --file /root/.ssh/authorized_keys=special_authorized_keysfile To create a VM in Specific \" Availability Zone and compute Host \" specify --availability-zone {Availbility-Zone-Name}:{Compute-Host} in above syntax. Example: [user@laptop ~]$ openstack server create --flavor cpu-a.2 \\ --image centos-7-x86_64 \\ --nic net-id=8ee63932-464b-4999-af7e-949190d8fe93 \\ --security-group default \\ --key-name cloud_key \\ --property description=\"My Server\" \\ test_vm_using_cli NOTE: To get more help on \"openstack server create\" command , use: [user@laptop ~]$ openstack -h server create Detailed syntax: openstack server create (--image <image> | --volume <volume>) --flavor <flavor> [--security-group <security-group>] [--key-name <key-name>] [--property <key=value>] [--file <dest-filename=source-filename>] [--user-data <user-data>] [--availability-zone <zone-name>] [--block-device-mapping <dev-name=mapping>] [--nic <net-id=net-uuid,v4-fixed-ip=ip-addr,v6-fixed-ip=ip-addr,port-id=port-uuid,auto,none>] [--network <network>] [--port <port>] [--hint <key=value>] [--config-drive <config-drive-volume>|True] [--min <count>] [--max <count>] [--wait] <server-name> Note Similarly, we can lauch a VM using \"Volume\". Now Verify the test vm status using below commands: [user@laptop ~]$ openstack server list | grep test_vm_using_cli OR, [user@laptop ~]$ openstack server show test_vm_using_cli Associating a Floating IP to VM To Associate a floating IP to VM, first get the unused floating IP using the following command: [user@laptop ~]$ openstack floating ip list | grep None | head -2 | 071f08ac-cd10-4b89-aee4-856ead8e3ead | 169.144.107.154 | None | None | | 1baf4232-9cb7-4a44-8684-c604fa50ff60 | 169.144.107.184 | None | None | Now Associate the first IP to the server using following command: [user@laptop ~]$ openstack server add floating ip test_vm_using_cli 169.144.107.154 Use the following command to verify whether floating IP is assigned to the VM or not: [user@laptop ~]$ openstack server list | grep test_vm_using_cli | 056c0937-6222-4f49-8405-235b20d173dd | test_vm_using_cli | ACTIVE | ... nternal=192.168.15.62, 169.144.107.154 | Remove existing floating ip from the VM openstack server remove floating ip <INSTANCE_NAME_OR_ID> <FLOATING_IP_ADDRESS> Get all available security group in your project $ openstack security group list +--------------------------------------+----------+------------------------+----------------------------------+------+ | 3ca248ac-56ac-4e5f-a57c-777ed74bbd7c | default | Default security group | f01df1439b3141f8b76e68a3b58ef74a | [] | | 5cdc5f33-78fc-4af8-bf25-60b8d4e5db2a | ssh_only | Enable SSH access. | f01df1439b3141f8b76e68a3b58ef74a | [] | +--------------------------------------+----------+------------------------+----------------------------------+------+ Add existing security group to the VM openstack server add security group <INSTANCE_NAME_OR_ID> <SECURITY_GROUP> Example: openstack server add security group test_vm_using_cli ssh_only Remove existing security group from the VM openstack server remove security group <INSTANCE_NAME_OR_ID> <SECURITY_GROUP> Example: openstack server remove security group test_vm_using_cli ssh_only Alternatively , you can use the openstack port unset command to remove the group from a port: openstack port unset --security-group <SECURITY_GROUP> <PORT> Adding volume to the VM $ openstack server add volume [--device <device>] <INSTANCE_NAME_OR_ID> <VOLUME_NAME_OR_ID> Remove existing volume from the VM openstack server remove volume <INSTANCE_NAME_OR_ID> <volume> Deleting Virtual Machine from Command Line [user@laptop ~]$ openstack server delete test_vm_using_cli","title":"Launch a VM using OpenStack CLI"},{"location":"openstack/advanced-openstack-topics/openstack-cli/launch-a-VM-using-openstack-CLI/#launch-a-vm-using-openstack-cli","text":"First find the following details using openstack command, we would required these details during the creation of virtual machine. Flavor Image Network Security Group Key Name Get the flavor list using below openstack command: [user@laptop ~]$ openstack flavor list +--------------------------------------+------------+--------+------+-----------+-------+-----------+ | ID | Name | RAM | Disk | Ephemeral | VCPUs | Is Public | +--------------------------------------+------------+--------+------+-----------+-------+-----------+ | 12ded228-1a7f-4d35-b994-7dd394a6ca90 | gpu-a100.2 | 196608 | 20 | 0 | 24 | True | | 15581358-3e81-4cf2-a5b8-c0fd2ad771b4 | mem-a.8 | 65536 | 20 | 0 | 8 | True | | 17521416-0ecf-4d85-8d4c-ec6fd1bc5f9d | cpu-a.1 | 2048 | 20 | 0 | 1 | True | | 2b1dbea2-736d-4b85-b466-4410bba35f1e | cpu-a.8 | 16384 | 20 | 0 | 8 | True | | 2f33578f-c3df-4210-b369-84a998d77dac | mem-a.4 | 32768 | 20 | 0 | 4 | True | | 4498bfdb-5342-4e51-aa20-9ee74e522d59 | mem-a.1 | 8192 | 20 | 0 | 1 | True | | 4e43e6df-3637-4363-a7cd-732fbf9e7cfd | gpu-a100.4 | 393216 | 20 | 0 | 48 | True | | 7f2f5f4e-684b-4c24-bfc6-3fce9cf1f446 | mem-a.16 | 131072 | 20 | 0 | 16 | True | | 8c05db2f-6696-446b-9319-c32341a09c41 | cpu-a.16 | 32768 | 20 | 0 | 16 | True | | 9662b5b2-aeaa-4d56-9bd3-450deee668af | cpu-a.4 | 8192 | 20 | 0 | 4 | True | | b3377fdd-fd0f-4c88-9b4b-3b5c8ada0732 | gpu-a100.1 | 98304 | 20 | 0 | 12 | True | | e9125ab0-c8df-4488-a252-029c636cbd0f | mem-a.2 | 16384 | 20 | 0 | 2 | True | | ee6417bd-7cd4-4431-a6ce-d09f0fba3ba9 | cpu-a.2 | 4096 | 20 | 0 | 2 | True | +--------------------------------------+------------+--------+------+-----------+-------+-----------+ Get the image name and its ID, [user@laptop ~]$ openstack image list | grep centos | 41eafa05-c264-4840-8c17-746e6a388c2d | centos-7-x86_64 | active | Get Private Virtual network details, which will be attached to the VM: [user@laptop ~]$ openstack network list +--------------------------------------+-----------------+--------------------------------------+ | ID | Name | Subnets | +--------------------------------------+-----------------+--------------------------------------+ | 43613b84-e1fb-44a4-b1ea-c530edc49018 | provider | 1cbbb98d-3b57-4f6d-8053-46045904d910 | | 8a91900b-d43c-474d-b913-930283e0bf43 | default_network | e62ce2fd-b11c-44ce-b7cc-4ca943e75a23 | +--------------------------------------+-----------------+--------------------------------------+ Find the Security Group: [user@laptop ~]$ openstack security group list +--------------------------------------+----------------------------------+------------------------+----------------------------------+------+ | ID | Name | Description | Project | Tags | +--------------------------------------+----------------------------------+------------------------+----------------------------------+------+ | 8285530a-34e3-4d96-8e01-a7b309a91f9f | default | Default security group | 8ae3ae25c3a84c689cd24c48785ca23a | [] | | bbb738d0-45fb-4a9a-8bc4-a3eafeb49ba7 | ssh_only | | 8ae3ae25c3a84c689cd24c48785ca23a | [] | +--------------------------------------+----------------------------------+------------------------+----------------------------------+------+ Find the Key pair, in my case you can choose your own, [user@laptop ~]$ openstack keypair list | grep -i cloud_key | cloud_key | d5:ab:dc:1f:e5:08:44:7f:a6:21:47:23:85:32:cc:04 | ssh | Note Above details will be different for you based on your project and env.","title":"Launch a VM using OpenStack CLI"},{"location":"openstack/advanced-openstack-topics/openstack-cli/launch-a-VM-using-openstack-CLI/#launch-an-instance-from-an-image","text":"Now we have all the details, let\u2019s create a virtual machine using \"openstack server create\" command Syntax : $ openstack server create --flavor {Flavor-Name-Or-Flavor-ID } \\ --image {Image-Name-Or-Image-ID} \\ --nic net-id={Network-ID} \\ --user-data USER-DATA-FILE \\ --security-group {Security_Group_ID} \\ --key-name {Keypair-Name} \\ --property KEY=VALUE \\ <Instance_Name> Important Note If you boot an instance with an \" Instance_Name \" greater than 63 characters , Compute truncates it automatically when turning it into a hostname to ensure the correct functionality of dnsmasq . Optionally, you can provide a key name for access control and a security group for security. You can also include metadata key and value pairs: --key-name {Keypair-Name} . For example, you can add a description for your server by providing the --property description=\"My Server\" parameter. You can pass user data in a local file at instance launch by using the --user-data USER-DATA-FILE parameter. If you do not provide a key pair, you will be unable to access the instance. You can also place arbitrary local files into the instance file system at creation time by using the --file <dest-filename=source-filename> parameter. You can store up to five files. For example, if you have a special authorized keys file named special_authorized_keysfile that you want to put on the instance rather than using the regular SSH key injection, you can add the \u2013file option as shown in the following example. --file /root/.ssh/authorized_keys=special_authorized_keysfile To create a VM in Specific \" Availability Zone and compute Host \" specify --availability-zone {Availbility-Zone-Name}:{Compute-Host} in above syntax. Example: [user@laptop ~]$ openstack server create --flavor cpu-a.2 \\ --image centos-7-x86_64 \\ --nic net-id=8ee63932-464b-4999-af7e-949190d8fe93 \\ --security-group default \\ --key-name cloud_key \\ --property description=\"My Server\" \\ test_vm_using_cli NOTE: To get more help on \"openstack server create\" command , use: [user@laptop ~]$ openstack -h server create Detailed syntax: openstack server create (--image <image> | --volume <volume>) --flavor <flavor> [--security-group <security-group>] [--key-name <key-name>] [--property <key=value>] [--file <dest-filename=source-filename>] [--user-data <user-data>] [--availability-zone <zone-name>] [--block-device-mapping <dev-name=mapping>] [--nic <net-id=net-uuid,v4-fixed-ip=ip-addr,v6-fixed-ip=ip-addr,port-id=port-uuid,auto,none>] [--network <network>] [--port <port>] [--hint <key=value>] [--config-drive <config-drive-volume>|True] [--min <count>] [--max <count>] [--wait] <server-name> Note Similarly, we can lauch a VM using \"Volume\". Now Verify the test vm status using below commands: [user@laptop ~]$ openstack server list | grep test_vm_using_cli OR, [user@laptop ~]$ openstack server show test_vm_using_cli","title":"Launch an instance from an Image"},{"location":"openstack/advanced-openstack-topics/openstack-cli/launch-a-VM-using-openstack-CLI/#associating-a-floating-ip-to-vm","text":"To Associate a floating IP to VM, first get the unused floating IP using the following command: [user@laptop ~]$ openstack floating ip list | grep None | head -2 | 071f08ac-cd10-4b89-aee4-856ead8e3ead | 169.144.107.154 | None | None | | 1baf4232-9cb7-4a44-8684-c604fa50ff60 | 169.144.107.184 | None | None | Now Associate the first IP to the server using following command: [user@laptop ~]$ openstack server add floating ip test_vm_using_cli 169.144.107.154 Use the following command to verify whether floating IP is assigned to the VM or not: [user@laptop ~]$ openstack server list | grep test_vm_using_cli | 056c0937-6222-4f49-8405-235b20d173dd | test_vm_using_cli | ACTIVE | ... nternal=192.168.15.62, 169.144.107.154 |","title":"Associating a Floating IP to VM"},{"location":"openstack/advanced-openstack-topics/openstack-cli/launch-a-VM-using-openstack-CLI/#remove-existing-floating-ip-from-the-vm","text":"openstack server remove floating ip <INSTANCE_NAME_OR_ID> <FLOATING_IP_ADDRESS>","title":"Remove existing floating ip from the VM"},{"location":"openstack/advanced-openstack-topics/openstack-cli/launch-a-VM-using-openstack-CLI/#get-all-available-security-group-in-your-project","text":"$ openstack security group list +--------------------------------------+----------+------------------------+----------------------------------+------+ | 3ca248ac-56ac-4e5f-a57c-777ed74bbd7c | default | Default security group | f01df1439b3141f8b76e68a3b58ef74a | [] | | 5cdc5f33-78fc-4af8-bf25-60b8d4e5db2a | ssh_only | Enable SSH access. | f01df1439b3141f8b76e68a3b58ef74a | [] | +--------------------------------------+----------+------------------------+----------------------------------+------+","title":"Get all available security group in your project"},{"location":"openstack/advanced-openstack-topics/openstack-cli/launch-a-VM-using-openstack-CLI/#add-existing-security-group-to-the-vm","text":"openstack server add security group <INSTANCE_NAME_OR_ID> <SECURITY_GROUP> Example: openstack server add security group test_vm_using_cli ssh_only","title":"Add existing security group to the VM"},{"location":"openstack/advanced-openstack-topics/openstack-cli/launch-a-VM-using-openstack-CLI/#remove-existing-security-group-from-the-vm","text":"openstack server remove security group <INSTANCE_NAME_OR_ID> <SECURITY_GROUP> Example: openstack server remove security group test_vm_using_cli ssh_only Alternatively , you can use the openstack port unset command to remove the group from a port: openstack port unset --security-group <SECURITY_GROUP> <PORT>","title":"Remove existing security group from the VM"},{"location":"openstack/advanced-openstack-topics/openstack-cli/launch-a-VM-using-openstack-CLI/#adding-volume-to-the-vm","text":"$ openstack server add volume [--device <device>] <INSTANCE_NAME_OR_ID> <VOLUME_NAME_OR_ID>","title":"Adding volume to the VM"},{"location":"openstack/advanced-openstack-topics/openstack-cli/launch-a-VM-using-openstack-CLI/#remove-existing-volume-from-the-vm","text":"openstack server remove volume <INSTANCE_NAME_OR_ID> <volume>","title":"Remove existing volume from the VM"},{"location":"openstack/advanced-openstack-topics/openstack-cli/launch-a-VM-using-openstack-CLI/#deleting-virtual-machine-from-command-line","text":"[user@laptop ~]$ openstack server delete test_vm_using_cli","title":"Deleting Virtual Machine from Command Line"},{"location":"openstack/advanced-openstack-topics/openstack-cli/openstack-CLI/","text":"OpenStack CLI References OpenStack Command Line Client(CLI) Cheat Sheet The OpenStack CLI is designed for interactive use. OpenStackClient (aka OSC) is a command-line client for OpenStack that brings the command set for Compute, Identity, Image, Object Storage and Block Storage APIs together in a single shell with a uniform command structure. OpenStackClient is primarily configured using command line options and environment variables. Most of those settings can also be placed into a configuration file to simplify managing multiple cloud configurations. Most global options have a corresponding environment variable that may also be used to set the value. If both are present, the command-line option takes priority. It's also possible to call it from a bash script or similar, but typically it is too slow for heavy scripting use. Command Line setup To use the CLI, you must create an application credentials and set the appropriate environment variables. You can download the environment file with the credentials from the OpenStack dashboard . Log in to the NERC's OpenStack dashboard , choose the project for which you want to download the OpenStack RC file. Navigate to Identity > Application Credentials . Click on \"Create Application Credential\" button and provide a Name and Roles for the application credential. All other fields are optional and leaving the \"Secret\" field empty will set it to autogenerate (recommended). Important Note Please note that an application credential is only valid for a single project, and to access multiple projects you need to create an application credential for each. You can switch projects by clicking on the project name at the top right corner and choosing from the dropdown under \"Project\". After clicking \"Create Application Credential\" button, the ID and Secret will be displayed and you will be prompted to Download openrc file or to Download clouds.yaml . Both of these are different methods of configuring the client for CLI access. Please save the file. Configuration The CLI is configured via environment variables and command-line options as listed in Authentication . Configuration Files OpenStack RC File Find the file (by default it will be named the same as the application credential name with the suffix -openrc.sh where project is the name of your OpenStack project). Source your downloaded OpenStack RC File : [user@laptop ~]$ source app-cred-<Credential_Name>-openrc.sh Important Note When you source the file, environment variables are set for your current shell. The variables enable the OpenStack client commands to communicate with the OpenStack services that run in the cloud. This just stores your entry into the environment variable - there's no validation at this stage. You can inspect the downloaded file to retrieve the ID and Secret if necessary and see what other environment variables are set. clouds.yaml clouds.yaml is a configuration file that contains everything needed to connect to one or more clouds. It may contain private information and is generally considered private to a user. For more information on configuring the OpenStackClient with clouds.yaml please see the OpenStack documentation Install the OpenStack command-line clients For more information on configuring the OpenStackClient please see the OpenStack documentation OpenStack Hello World To test that you have everything configured, try out some commands. The following command lists all the images available to your project: [user@laptop ~]$ openstack image list +--------------------------------------+---------------------+--------+ | ID | Name | Status | +--------------------------------------+---------------------+--------+ | a9b48e65-0cf9-413a-8215-81439cd63966 | MS-Windows-2022 | active | | 41eafa05-c264-4840-8c17-746e6a388c2d | centos-7-x86_64 | active | | 41fa5991-89d5-45ae-8268-b22224c772b2 | debian-10-x86_64 | active | | 99194159-fcd1-4281-b3e1-15956c275692 | fedora-36-x86_64 | active | | cf1be3e5-b6f6-466e-bac4-abe7587921a8 | rocky-8-x86_64 | active | | 75a40234-702b-4ab7-9d83-f436b05827c9 | ubuntu-18.04-x86_64 | active | | 126a1c8a-1802-434f-bee3-c3b6c8def513 | ubuntu-20.04-x86_64 | active | | 8183fe83-1403-412c-8ef8-5608a5e09166 | ubuntu-22.04-x86_64 | active | +--------------------------------------+---------------------+--------+ If you have launched some instances already, the following command shows a list of your project's instances: [user@laptop ~]$ openstack server list +--------------------------------------+------------------+--------+----------------------------------------------+--------------------------+--------------+ | ID | Name | Status | Networks | Image | Flavor | +--------------------------------------+------------------+--------+----------------------------------------------+--------------------------+--------------+ | 1c96ba49-a20f-4c88-bbcf-93e2364365f5 | vm-test | ACTIVE | default_network=192.168.0.146, 199.94.60.4 | N/A (booted from volume) | cpu-a.4 | | dd0d8053-ab88-4d4f-b5bc-97e7e2fe035a | gpu-test | ACTIVE | default_network=192.168.0.146, 199.94.60.4 | N/A (booted from volume) | gpu-a100.1 | +--------------------------------------+------------------+--------+----------------------------------------------+--------------------------+--------------+ If you don't have any instances, you will get the error list index out of range , which is why we didn't suggest this command for your first test: [user@laptop ~]$ openstack server list list index out of range If you see this error: [user@laptop ~]$ openstack server list The request you have made requires authentication. (HTTP 401) (Request-ID: req-6a827bf3-d5e8-47f2-984c-b6edeeb2f7fb) Then your environment variables are likely not configured correctly. The most common reason is that you made a typo when entering your password. Try sourcing the OpenStack RC file again and retyping it. You can type openstack -h to see a list of available commands. Note This includes some admin-only commands. If you try one of these by mistake, you might see this output: [user@laptop ~]$ openstack user list You are not authorized to perform the requested action: identity:list_users. (HTTP 403) (Request-ID: req-cafe1e5c-8a71-44ab-bd21-0e0f25414062) Depending on your needs for API interaction, this might be sufficient. If you just occasionally want to run 1 or 2 of these commands from your terminal, you can do it manually or write a quick bash script that makes use of this CLI. However, this isn't a very optimized way to do complex interactions with OpenStack. For that, you want to write scripts that interact with the python SDK bindings directly. Pro Tip If you find yourself fiddling extensively with awk and grep to extract things like project IDs from the CLI output, it's time to move on to using the client libraries or the RESTful API directly in your scripts.","title":"OpenStack CLI"},{"location":"openstack/advanced-openstack-topics/openstack-cli/openstack-CLI/#openstack-cli","text":"","title":"OpenStack CLI"},{"location":"openstack/advanced-openstack-topics/openstack-cli/openstack-CLI/#references","text":"OpenStack Command Line Client(CLI) Cheat Sheet The OpenStack CLI is designed for interactive use. OpenStackClient (aka OSC) is a command-line client for OpenStack that brings the command set for Compute, Identity, Image, Object Storage and Block Storage APIs together in a single shell with a uniform command structure. OpenStackClient is primarily configured using command line options and environment variables. Most of those settings can also be placed into a configuration file to simplify managing multiple cloud configurations. Most global options have a corresponding environment variable that may also be used to set the value. If both are present, the command-line option takes priority. It's also possible to call it from a bash script or similar, but typically it is too slow for heavy scripting use.","title":"References"},{"location":"openstack/advanced-openstack-topics/openstack-cli/openstack-CLI/#command-line-setup","text":"To use the CLI, you must create an application credentials and set the appropriate environment variables. You can download the environment file with the credentials from the OpenStack dashboard . Log in to the NERC's OpenStack dashboard , choose the project for which you want to download the OpenStack RC file. Navigate to Identity > Application Credentials . Click on \"Create Application Credential\" button and provide a Name and Roles for the application credential. All other fields are optional and leaving the \"Secret\" field empty will set it to autogenerate (recommended). Important Note Please note that an application credential is only valid for a single project, and to access multiple projects you need to create an application credential for each. You can switch projects by clicking on the project name at the top right corner and choosing from the dropdown under \"Project\". After clicking \"Create Application Credential\" button, the ID and Secret will be displayed and you will be prompted to Download openrc file or to Download clouds.yaml . Both of these are different methods of configuring the client for CLI access. Please save the file.","title":"Command Line setup"},{"location":"openstack/advanced-openstack-topics/openstack-cli/openstack-CLI/#configuration","text":"The CLI is configured via environment variables and command-line options as listed in Authentication .","title":"Configuration"},{"location":"openstack/advanced-openstack-topics/openstack-cli/openstack-CLI/#configuration-files","text":"","title":"Configuration Files"},{"location":"openstack/advanced-openstack-topics/openstack-cli/openstack-CLI/#openstack-rc-file","text":"Find the file (by default it will be named the same as the application credential name with the suffix -openrc.sh where project is the name of your OpenStack project). Source your downloaded OpenStack RC File : [user@laptop ~]$ source app-cred-<Credential_Name>-openrc.sh Important Note When you source the file, environment variables are set for your current shell. The variables enable the OpenStack client commands to communicate with the OpenStack services that run in the cloud. This just stores your entry into the environment variable - there's no validation at this stage. You can inspect the downloaded file to retrieve the ID and Secret if necessary and see what other environment variables are set.","title":"OpenStack RC File"},{"location":"openstack/advanced-openstack-topics/openstack-cli/openstack-CLI/#cloudsyaml","text":"clouds.yaml is a configuration file that contains everything needed to connect to one or more clouds. It may contain private information and is generally considered private to a user. For more information on configuring the OpenStackClient with clouds.yaml please see the OpenStack documentation","title":"clouds.yaml"},{"location":"openstack/advanced-openstack-topics/openstack-cli/openstack-CLI/#install-the-openstack-command-line-clients","text":"For more information on configuring the OpenStackClient please see the OpenStack documentation","title":"Install the OpenStack command-line clients"},{"location":"openstack/advanced-openstack-topics/openstack-cli/openstack-CLI/#openstack-hello-world","text":"To test that you have everything configured, try out some commands. The following command lists all the images available to your project: [user@laptop ~]$ openstack image list +--------------------------------------+---------------------+--------+ | ID | Name | Status | +--------------------------------------+---------------------+--------+ | a9b48e65-0cf9-413a-8215-81439cd63966 | MS-Windows-2022 | active | | 41eafa05-c264-4840-8c17-746e6a388c2d | centos-7-x86_64 | active | | 41fa5991-89d5-45ae-8268-b22224c772b2 | debian-10-x86_64 | active | | 99194159-fcd1-4281-b3e1-15956c275692 | fedora-36-x86_64 | active | | cf1be3e5-b6f6-466e-bac4-abe7587921a8 | rocky-8-x86_64 | active | | 75a40234-702b-4ab7-9d83-f436b05827c9 | ubuntu-18.04-x86_64 | active | | 126a1c8a-1802-434f-bee3-c3b6c8def513 | ubuntu-20.04-x86_64 | active | | 8183fe83-1403-412c-8ef8-5608a5e09166 | ubuntu-22.04-x86_64 | active | +--------------------------------------+---------------------+--------+ If you have launched some instances already, the following command shows a list of your project's instances: [user@laptop ~]$ openstack server list +--------------------------------------+------------------+--------+----------------------------------------------+--------------------------+--------------+ | ID | Name | Status | Networks | Image | Flavor | +--------------------------------------+------------------+--------+----------------------------------------------+--------------------------+--------------+ | 1c96ba49-a20f-4c88-bbcf-93e2364365f5 | vm-test | ACTIVE | default_network=192.168.0.146, 199.94.60.4 | N/A (booted from volume) | cpu-a.4 | | dd0d8053-ab88-4d4f-b5bc-97e7e2fe035a | gpu-test | ACTIVE | default_network=192.168.0.146, 199.94.60.4 | N/A (booted from volume) | gpu-a100.1 | +--------------------------------------+------------------+--------+----------------------------------------------+--------------------------+--------------+ If you don't have any instances, you will get the error list index out of range , which is why we didn't suggest this command for your first test: [user@laptop ~]$ openstack server list list index out of range If you see this error: [user@laptop ~]$ openstack server list The request you have made requires authentication. (HTTP 401) (Request-ID: req-6a827bf3-d5e8-47f2-984c-b6edeeb2f7fb) Then your environment variables are likely not configured correctly. The most common reason is that you made a typo when entering your password. Try sourcing the OpenStack RC file again and retyping it. You can type openstack -h to see a list of available commands. Note This includes some admin-only commands. If you try one of these by mistake, you might see this output: [user@laptop ~]$ openstack user list You are not authorized to perform the requested action: identity:list_users. (HTTP 403) (Request-ID: req-cafe1e5c-8a71-44ab-bd21-0e0f25414062) Depending on your needs for API interaction, this might be sufficient. If you just occasionally want to run 1 or 2 of these commands from your terminal, you can do it manually or write a quick bash script that makes use of this CLI. However, this isn't a very optimized way to do complex interactions with OpenStack. For that, you want to write scripts that interact with the python SDK bindings directly. Pro Tip If you find yourself fiddling extensively with awk and grep to extract things like project IDs from the CLI output, it's time to move on to using the client libraries or the RESTful API directly in your scripts.","title":"OpenStack Hello World"},{"location":"openstack/advanced-openstack-topics/persistent-storage/object-storage/","text":"Object Storage OpenStack Object Storage (Swift) is a highly available, distributed, eventually consistent object/blob store. Object Storage is used to manage cost-effective and long-term preservation and storage of large amounts of data across clusters of standard server hardware. The common use cases include the storage, backup and archiving of unstructured data, such as documents, static web content, images, video files, and virtual machine images, etc. The end-users can interact with the object storage system through a RESTful HTTP API i.e. the Swift API or use one of the many client libraries that exist for all of the popular programming languages, such as Java, Python, Ruby, and C# based on provisioned quotas. Swift also supports and is compatible with Amazon's Simple Storage Service (S3) API that makes it easier for the end-users to move data between multiple storage end points and supports hybrid cloud setup. 1. Access by Web Interface i.e. Horizon Dashboard To get started, navigate to Project -> Object Store -> Containers. Create a Container In order to store objects, you need at least one Container to put them in. Containers are essentially top-level directories. Other services use the terminology buckets . Click Create Container. Give your container a name. Important Note The container name needs to be unique, not just within your project but across all of our OpenStack installation. If you get an error message after trying to create the container, try giving it a more unique name. For now, leave the \"Container Access\" set to Private . Upload a File Click on the name of your container, and click the Upload File icon as shown below: Click Browse and select a file from your local machine to upload. It can take a while to upload very large files, so if you're just testing it out you may want to use a small text file or similar. By default the File Name will be the same as the original file, but you can change it to another name. Click \"Upload File\". Your file will appear inside the container as shown below once successful: Using Folders Files stored by definition do not organize objects into folders, but you can use folders to keep your data organized. On the backend, the folder name is actually just prefixed to the object name, but from the web interface (and most other clients) it works just like a folder. To add a folder, click on the \"+ folder\" icon as shown below: Make a container public Making a container public allows you to send your collaborators a URL that gives access to the container's contents. Click on your container's name, then check the \"Public Access\" checkbox. Note that \"Public Access\" changes from \"Disabled\" to \"Link\". Click \"Link\" to see a list of object in the container. This is the URL of your container. Important Note Anyone who obtains the URL will be able to access the container, so this is not recommended as a way to share sensitive data with collaborators. In addition, everything inside a public container is public, so we recommend creating a separate container specifically for files that should be made public. To download the file test-file we would use the following url . Very Important Information Here 4c5bccef73c144679d44cbc96b42df4e is specific Tenant Id or Project Id . You can get this value when you click on the public container's Link on a new browser tab. Or , you can just click on \"Download\" next to the file's name as shown below: You can also interact with public objects using a utility such as curl : curl https://stack.nerc.mghpcc.org:13808/v1/AUTH_4c5bccef73c144679d44cbc96b42df4e/unique-container-test test-file To download a file: curl -o local-file.txt https://stack.nerc.mghpcc.org:13808/v1/AUTH_4c5bccef73c144679d44cbc96b42df4e/unique-container-test/test-file Make a container private You can make a public container private by clicking on your container's name, then uncheck the \"Public Access\" checkbox. Note that \"Public Access\" changes from \"Link\" to \"Disabled\". This will deactivate the public URL of the container and then it will show \"Disabled\". 2. Access by using APIs i. OpenStack CLI Prerequisites To run the OpenStack CLI commands, you need to have: OpenStack CLI setup, see OpenStack Command Line setup for more information. Some Object Storage management examples Create a container In order to create a container in the Object Storage service, you can use the OpenStack client with the following command. [user@laptop ~]$ openstack container create mycontainer +---------------------------------------+-------------+------------------------------------+ | account | container | x-trans-id | +---------------------------------------+-------------+------------------------------------+ | AUTH_4c5bccef73c144679d44cbc96b42df4e | mycontainer | txb875f426a011476785171-00624b37e8 | +---------------------------------------+-------------+------------------------------------+ Once created you can start adding objects. Manipulate objects in a container To upload files to a container you can use the following command $ openstack object create --name my_test_file mycontainer test_file.txt +--------------+-------------+----------------------------------+ | object | container | etag | +--------------+-------------+----------------------------------+ | my_test_file | mycontainer | e3024896943ee80422d1e5ff44423658 | +--------------+-------------+----------------------------------+ Once uploaded you can see the metadata through: $ openstack object show mycontainer my_test_file +----------------+---------------------------------------+ | Field | Value | +----------------+---------------------------------------+ | account | AUTH_4c5bccef73c144679d44cbc96b42df4e | | container | mycontainer | | content-length | 26 | | content-type | application/octet-stream | | etag | e3024896943ee80422d1e5ff44423658 | | last-modified | Mon, 04 Apr 2022 18:27:14 GMT | | object | my_test_file | +----------------+---------------------------------------+ You can save the contents of the object from your container to your local machine by using: $ openstack object save mycontainer my_test_file --file test_file.txt Very Important Please note that this will overwrite the file in the local directory. Finally you can delete the object with the following command $ openstack object delete mycontainer my_test_file Delete the container If you want to delete the container, you can use the following command $ openstack container delete mycontainer If the container has some data , you can trigger the recursive option to delete the objects internally. $ openstack container delete mycontainer Conflict (HTTP 409) (Request-ID: tx6b53c2b3e52d453e973b4-00624b400f) So, try to delete the container recursively using command $ openstack container delete --recursive mycontainer List existing containers You can check the existing containers with $ openstack container list +---------------+ | Name | +---------------+ | mycontainer | +---------------+ Swift quota utilization To check the overall space used, you can use the following command $ openstack object store account show +------------+---------------------------------------+ | Field | Value | +------------+---------------------------------------+ | Account | AUTH_4c5bccef73c144679d44cbc96b42df4e | | Bytes | 665 | | Containers | 1 | | Objects | 3 | +------------+---------------------------------------+ To check the space used by a specific container $ openstack container show mycontainer +----------------+---------------------------------------+ | Field | Value | +----------------+---------------------------------------+ | account | AUTH_4c5bccef73c144679d44cbc96b42df4e | | bytes_used | 665 | | container | mycontainer | | object_count | 3 | | read_acl | .r:*,.rlistings | | storage_policy | Policy-0 | +----------------+---------------------------------------+ ii. Swift Interface This is a python client for the Swift API. There's a Python API (the swiftclient module), and a command-line script ( swift ). This example uses a Python3 virtual environment, but you are free to choose any other method to create a local virtual environment like Conda . py -3 -m venv venv Activate the virtual environment by running: on Linux/Mac: source venv/bin/activate on Windows: venv\\Scripts\\activate Install Python Swift Client page at PyPi Once virtual environment is activated, install python-swiftclient and python-keystoneclient pip install python-swiftclient python-keystoneclient Swift authenticates using a user, tenant, and key, which map to your OpenStack username, project,and password. For this, you need to download the \"NERC's OpenStack RC File\" with the credentials for your NERC project from the NERC's OpenStack dashboard . Then you need to source that RC file using: source *-openrc.sh . You can read here on how to do this. By sourcing the \"NERC's OpenStack RC File\", you will set the all required environmental variablesand then type the following command to get a lits of your containers: swift list This will output your existing container on your project, for e.g. unique-container-test To upload a file to the above listed i.e. unique-container-test , you can run the following command: swift upload unique-container-test ./README.md Other helpful Swift commands: delete Delete a container or objects within a container. download Download objects from containers. list Lists the containers for the account or the objects for a container. post Updates meta information for the account, container, or object; creates containers if not present. copy Copies object, optionally adds meta stat Displays information for the account, container, or object. upload Uploads files or directories to the given container. capabilities List cluster capabilities. tempurl Create a temporary URL. auth Display auth related environment variables. bash_completion Outputs option and flag cli data ready for bash_completion. Helpful Tip Type swift -h to learn more about using the swift commands. The client has a --debug flag, which can be useful if you are facing any issues. iii. Using AWS CLI The Ceph Object Gateway supports basic operations through the Amazon S3 interface . You can use both high-level (s3) commands with the AWS CLI and API-Level (s3api) commands with the AWS CLI to access object storage on your NERC project. Prerequisites To run the s3 or s3api commands, you need to have: AWS CLI installed, see Installing or updating the latest version of the AWS CLI for more information. The NERC's Swift End Point URL: https://stack.nerc.mghpcc.org:13808 Understand these Amazon S3 terms: i. Bucket \u2013 A top-level Amazon S3 folder. ii. Prefix \u2013 An Amazon S3 folder in a bucket. iii. Object \u2013 Any item that's hosted in an Amazon S3 bucket. Configuring the AWS CLI To access this interface, you must login through the OpenStack Dashboard and navigate to \"Projects > API Access\" where you can click on \"Download OpenStack RC File\" and select \"EC2 Credentials\". This will download a file zip file including ec2rc.sh file that has content similar to shown below. The important parts are EC2_ACCESS_KEY and EC2_SECRET_KEY , keep them noted. #!/bin/bash NOVARC=$(readlink -f \"${BASH_SOURCE:-${0}}\" 2>/dev/null) || NOVARC=$(python -c 'import os,sys; print os.path.abspath(os.path.realpath(sys.argv[1]))' \"${BASH_SOURCE:-${0}}\") NOVA_KEY_DIR=${NOVARC%/*} export EC2_ACCESS_KEY=... export EC2_SECRET_KEY=... export EC2_URL=https://localhost/notimplemented export EC2_USER_ID=42 # nova does not use user id, but bundling requires it export EC2_PRIVATE_KEY=${NOVA_KEY_DIR}/pk.pem export EC2_CERT=${NOVA_KEY_DIR}/cert.pem export NOVA_CERT=${NOVA_KEY_DIR}/cacert.pem export EUCALYPTUS_CERT=${NOVA_CERT} # euca-bundle-image seems to require this set alias ec2-bundle-image=\"ec2-bundle-image --cert ${EC2_CERT} --privatekey ${EC2_PRIVATE_KEY} --user 42 --ec2cert ${NOVA_CERT}\" alias ec2-upload-bundle=\"ec2-upload-bundle -a ${EC2_ACCESS_KEY} -s ${EC2_SECRET_KEY} --url ${S3_URL} --ec2cert ${NOVA_CERT}\" Source the downloaded OpenStack RC File by using: source *-openrc.sh command. Sourcing the RC File will set the required ${OS_PROJECT_NAME} envrionment variable. Then run aws configuration command which requires the EC2_ACCESS_KEY and EC2_SECRET_KEY keys that you noted from ec2rc.sh file (above): $> aws configure --profile \"'${OS_PROJECT_NAME}'\" AWS Access Key ID [None]: <EC2_ACCESS_KEY> AWS Secret Access Key [None]: <EC2_SECRET_KEY> Default region name [None]: Default output format [None]: Information We need to have a profile that you use must have permissions to allow the AWS operations can be performed. Listing buckets using aws-cli i. Using s3api : $ aws --profile \"'${OS_PROJECT_NAME}'\" --endpoint-url=https://stack.nerc.mghpcc.org:13808 \\ s3api list-buckets { \"Buckets\": [ { \"Name\": \"unique-container-test\", \"CreationDate\": \"2009-02-03T16:45:09+00:00\" } ], \"Owner\": { \"DisplayName\": \"Test Project-f69dcff:mmunakami@fas.harvard.edu\", \"ID\": \"Test Project-f69dcff:mmunakami@fas.harvard.edu\" } } ii. Alternatively, you can do the same using s3 : aws --profile \"'${OS_PROJECT_NAME}'\" --endpoint-url=https://stack.nerc.mghpcc.org:13808 \\ s3 ls Output: 2009-02-03 11:45:09 unique-container-test To list contents inside bucket aws --profile \"'${OS_PROJECT_NAME}'\" --endpoint-url=https://stack.nerc.mghpcc.org:13808 \\ s3 ls s3://<your-bucket> To make a bucket aws --profile \"'${OS_PROJECT_NAME}'\" --endpoint-url=https://stack.nerc.mghpcc.org:13808 \\ s3 mb s3://<your-bucket> Adding/ Copying files from one container to another container Single file copy using cp command: The aws tool provides a cp command to move files to your s3 bucket: $ aws --profile \"'${OS_PROJECT_NAME}'\" --endpoint-url=https://stack.nerc.mghpcc.org:13808 \\ s3 cp <Your-file> s3://<your-bucket>/ Output: upload: .\\<Your-file> to s3://<your-bucket>/<Your-file> Whole directory copy using the --recursive flag $ aws --profile \"'${OS_PROJECT_NAME}'\" --endpoint-url=https://stack.nerc.mghpcc.org:13808 \\ s3 cp <Your-directory> s3://<your-bucket>/ --recursive Output: upload: <your-directory>/<file0> to s3://<your-bucket>/<file0> upload: <your-directory>/<file1> to s3://<your-bucket>/<file1> ... upload: <your-directory>/<fileN> to s3://<your-bucket>/<fileN> You can then use aws s3 ls to check that your files have been properly uploaded: aws --profile \"'${OS_PROJECT_NAME}'\" --endpoint-url=https://stack.nerc.mghpcc.org:13808 \\ s3 ls s3://<your-bucket>/ Output: 2022-04-04 16:32:38 <size> <file0> 2022-04-04 16:32:38 <size> <file1> ... 2022-04-04 16:25:50 <size> <fileN> Other Useful Flags Additionally, aws cp provides an --exclude flag to filter files not to be transferred, the syntax is: --exclude \"<regex>\" To delete an object from a bucket aws --profile \"'${OS_PROJECT_NAME}'\" --endpoint-url=https://stack.nerc.mghpcc.org:13808 \\ s3 rm s3://<your-bucket>/argparse-1.2.1.tar.gz To remove a bucket aws --profile \"'${OS_PROJECT_NAME}'\" --endpoint-url=https://stack.nerc.mghpcc.org:13808 \\ s3 rb s3://<your-bucket> iv. Using s3cmd S3cmd is a free command line tool and client for uploading, retrieving and managing data in Amazon S3 and other cloud storage service providers that use the S3 protocol. Prerequisites S3cmd installed, see Download and Install the latest version of the S3cmd for more information. Configuring s3cmd The EC2_ACCESS_KEY and EC2_SECRET_KEY keys that you noted from ec2rc.sh file can then be plugged into s3cfg config file. The .s3cfg file requires the following configuration to work with our Object storage service: # Setup endpoint host_base = stack.nerc.mghpcc.org:13808 host_bucket = stack.nerc.mghpcc.org:13808 use_https = True # Setup access keys access_key = 'YOUR_EC2_ACCESS_KEY_FROM_ec2rc_FILE' secret_key = 'YOUR_EC2_SECRET_KEY_FROM_ec2rc_FILE', #pragma: allowlist secret # Enable S3 v4 signature APIs signature_v2 = False We are assuming that the configuration file is placed in default location i.e. $HOME/.s3cfg . If it is not the case you need to add the parameter --config=FILE with the location of your configuration file to override the config location. Using s3cmd To list buckets Use the following command to list all s3 buckets s3cmd ls Or, $ s3cmd ls s3:// 2009-02-03 16:45 s3://nerc-test-container 2009-02-03 16:45 s3://second-mycontainer 2009-02-03 16:45 s3://unique-container-test Create a new bucket In order to create a bucket, you can use s3cmd with the following command $ s3cmd mb s3://mybucket Bucket 's3://mybucket/' created $ s3cmd ls 2009-02-03 16:45 s3://mybucket 2009-02-03 16:45 s3://nerc-test-container 2009-02-03 16:45 s3://second-mycontainer 2009-02-03 16:45 s3://unique-container-test To copy an object to bucket Below command will upload file file.txt to the bucket using s3cmd command. $ s3cmd put ~/file.txt s3://mybucket/ upload: 'file.txt' -> 's3://mybucket/file.txt' [1 of 1] 0 of 0 0% in 0s 0.00 B/s done s3cmd also allows to set additional properties to the objects stored. In the example below, we set the content type with the --mime-type option and the cache-control parameter to 1 hour with --add-header . s3cmd put --mime-type='application/json' --add-header='Cache-Control: max-age=3600' ~/file.txt s3://mybucket Uploading Directory in bucket If we need to upload entire directory use -r to upload it recursively as below. $ s3cmd put -r <your-directory> s3://mybucket/ upload: 'backup/hello.txt' -> 's3://mybucket/backup/hello.txt' [1 of 1] 0 of 0 0% in 0s 0.00 B/s done List the objects of bucket List the objects of the bucket using ls switch with s3cmd. $ s3cmd ls s3://mybucket/ DIR s3://mybucket/backup/ 2022-04-05 03:10 0 s3://mybucket/file.txt 2022-04-05 03:14 0 s3://mybucket/hello.txt To copy/ download an object to local system Use the following command to download files from the bucket: $ s3cmd get s3://mybucket/file.txt download: 's3://mybucket/file.txt' -> './file.txt' [1 of 1] 0 of 0 0% in 0s 0.00 B/s done To sync local file/directory to a bucket $ s3cmd sync newdemo s3://mybucket upload: 'newdemo/newdemo_file.txt' -> 's3://mybucket/newdemo/newdemo_file.txt' [1 of 1] 0 of 0 0% in 0s 0.00 B/s done To sync bucket or object with local filesystem $ s3cmd sync s3://unique-container-test otherlocalbucket download: 's3://unique-container-test/README.md' -> 'otherlocalbucket/README.md' [1 of 3] 653 of 653 100% in 0s 4.54 kB/s done download: 's3://unique-container-test/image.png' -> 'otherlocalbucket/image.png' [2 of 3] 0 of 0 0% in 0s 0.00 B/s done download: 's3://unique-container-test/test-file' -> 'otherlocalbucket/test-file' [3 of 3] 12 of 12 100% in 0s 83.83 B/s done Done. Downloaded 665 bytes in 1.0 seconds, 665.00 B/s. To delete an object from bucket You can delete files from the bucket with the following s3cmd command $ s3cmd del s3://unique-container-test/README.md delete: 's3://unique-container-test/README.md' To delete directory from bucket $ s3cmd del s3://mybucket/newdemo delete: 's3://mybucket/newdemo' To delete a bucket $ s3cmd rb s3://mybucket ERROR: S3 error: 409 (BucketNotEmpty): The bucket you tried to delete is not empty Important Information The above command failed because of the bucket was not empty! You can remove all objects inside the bucket and then use the command again. Or, you can run the following command with -r or --recursive flag i.e. s3cmd rb s3://mybucket -r or s3cmd rb s3://mybucket --recursive . v. Using rclone rclone is a convenient and performant command-line tool for transferring files and synchronizing directories directly between your local file systems and the NERC's containers. Prerequisites To run the rclone commands, you need to have: rclone installed, see Downloading and Installing the latest version of the rclone for more information. Configuring rclone First, you\u2019ll need to configure rclone . As the object storage systems have quite complicated authentication these are kept in a config file. If you run rclone config file you will see where the default location is for you. Note For Windows users, you many need to specify the full path to the rclone executable file, if its not included in your systems PATH variable. The EC2_ACCESS_KEY and EC2_SECRET_KEY keys that you noted from ec2rc.sh file can then be plugged into rclone config file. Edit the config file's content on the path location described by rclone config file command and add the following entry with the name [nerc] : [nerc] type = s3 env_auth = false provider = Other endpoint = https://stack.nerc.mghpcc.org:13808 acl = public-read access_key_id = 'YOUR_EC2_ACCESS_KEY_FROM_ec2rc_FILE' secret_access_key = 'YOUR_EC2_SECRET_KEY_FROM_ec2rc_FILE', #pragma: allowlist secret location_constraint = server_side_encryption = More about the config for AWS S3 compatible API can be seen here . Important Information Mind that if set env_auth = true then it will take variables from environment, so you shouldn't insert it in this case. OR, You can locally copy this content to a new config file and then use this flag to override the config location, e.g. rclone --config=FILE Interactive Configuration Run rclone config to setup. See rclone config docs for more details. Using rclone rclone supports many subcommands (see the complete list of rclone subcommands ). A few commonly used subcommands (assuming you configured the NERC Object Storage as nerc ): Listing the Containers and Files and Folders within a Container Once your Object Storage has been configured in rclone, you can then use the rclone interface to List all the Containers with the \"lsd\" command rclone lsd \"nerc:\" or, rclone lsd \"nerc:\" --config=rclone.conf For e.g., $ rclone lsd \"nerc:\" --config=rclone.conf -1 2009-02-03 11:45:09 -1 second-mycontainer -1 2009-02-03 11:45:09 -1 unique-container-test To list the files and folders available within a container i.e. \"unique-container-test\" in this case, within a container we can use the \"ls\" command $ rclone ls \"nerc:unique-container-test/\" 653 README.md 0 image.png 12 test-file Uploading and Downloading Files and Folders rclone support a variety of options to allow you to Copy, Sync and Move files from one destination to another. A simple example of this can be seen below, where we copy (Upload) the file \"upload.me\" to the <your-bucket> container: rclone copy \"./upload.me\" \"nerc:<your-bucket>/\" Another example, to copy (Download) the file \"upload.me\" from the <your-bucket> container to your local: rclone -P copy \"nerc:<your-bucket>/upload.me\" \"./\" Also, to Sync files into to the <your-bucket> container - try with --dry-run first rclone --dry-run sync /path/to/files nerc:<your-bucket> Then sync for real rclone sync /path/to/files nerc:<your-bucket> Mounting object storage on local filesystem Linux: First, you need to create a directory on which you will mount your filesystem: $ mkdir ~/mnt-rclone Then you can simply mount your object storage with: $ rclone -vv --vfs-cache-mode writes mount nerc: ~/mnt-rclone Windows: First you have to download Winfsp : WinFsp is an open source Windows File System Proxy which provides a FUSE emulation layer. Then you can simply mount your object storage with (no need to create the directory in advance): rclone -vv --vfs-cache-mode writes mount nerc: C:/mnt-rclone vfs-cache-mode flag enable file caching, you can use either writes or full option. For further explanation you can see official documentation . Now that your object storage is mounted, you can list, create and delete files in it. Unmount object storage To unmount, simply press CTRL-C and the mount will be interrupted. vi. Using client libraries a. The EC2_ACCESS_KEY and EC2_SECRET_KEY keys that you noted from ec2rc.sh file can then be plugged into your application. See below example using the Python Boto3 library , which connects through the S3 API interface through EC2 credentials, and perform some basic operations on available buckets and file that the user has access to. import boto3 # https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/s3.html#bucket s3 = boto3.resource('s3', aws_access_key_id='YOUR_EC2_ACCESS_KEY_FROM_ec2rc_FILE', aws_secret_access_key='YOUR_EC2_SECRET_KEY_FROM_ec2rc_FILE', #pragma: allowlist secret endpoint_url='https://stack.nerc.mghpcc.org:13808', ) # List all containers for bucket in s3.buckets.all(): print(' ->', bucket) # List all objects in a container i.e. unique-container-test is your current Container bucket = s3.Bucket('unique-container-test') for obj in bucket.objects.all(): print(' ->', obj) # Download an S3 object i.e. test-file a file available in your unique-container-test Container s3.Bucket('unique-container-test').download_file('test-file', './test-file.txt') # Add an image to the bucket # bucket.put_object(Body=open('image.png', mode='rb'), Key='image.png') We can configure the Python Boto3 library , to work with the saved aws profile. import boto3 # https://boto3.amazonaws.com/v1/documentation/api/latest/reference/core/session.html session = boto3.Session(profile_name='<YOUR_CONFIGURED_AWS_PROFILE_NAME>') # List all containers s3 = boto3.client('s3', endpoint_url='https://stack.nerc.mghpcc.org:13808',) response = s3.list_buckets() for bucket in response['Buckets']: print(' ->', bucket) b. The EC2_ACCESS_KEY and EC2_SECRET_KEY keys that you noted from ec2rc.sh file can then be plugged into your application. See below example using the Python Minio library , which connects through the S3 API interface through EC2 credentials, and perform some basic operations on available buckets and file that the user has access to. from minio import Minio # Create client with access key and secret key. # https://docs.min.io/docs/python-client-api-reference.html client = Minio( \"stack.nerc.mghpcc.org:13808\", access_key='YOUR_EC2_ACCESS_KEY_FROM_ec2rc_FILE', secret_key='YOUR_EC2_SECRET_KEY_FROM_ec2rc_FILE', #pragma: allowlist secret ) # List all containers buckets = client.list_buckets() for bucket in buckets: # print(bucket.name, bucket.creation_date) print(' ->', bucket) # Make 'nerc-test-container' container if not exist. found = client.bucket_exists(\"nerc-test-container\") if not found: client.make_bucket(\"nerc-test-container\") else: print(\"Bucket 'nerc-test-container' already exists\") # Upload './nerc-backup.zip' as object name 'nerc-backup-2022.zip' # to bucket 'nerc-test-container'. client.fput_object( \"nerc-test-container\", \"nerc-backup-2022.zip\", \"./nerc-backup.zip\", ) 3. Using Graphical User Interface (GUI) Tools i. Using WinSCP WinSCP is a popular and free open-source SFTP client, SCP client, and FTP client for Windows. Its main function is file transfer between a local and a remote computer, with some basic file management functionality using FTP, FTPS, SCP, SFTP, WebDAV or S3 file transfer protocols. Prerequisites WinSCP installed, see Download and Install the latest version of the WinSCP for more information. Go to WinSCP menu and open \"Options > Preferences\". When the \"Preferences\" dialog window appears, select \"Transfer\" in the options on the left pane. Click on \"Edit\" button. Then, on shown popup dialog box review the \"Common options\" group, uncheck the \"Preserve timestamp\" option as shown below: Configuring WinSCP Click on \"New Session\" tab button as shown below: Select \"Amazon S3\" from the \"File protocol\" dropdown options as shown below: Provide the following required endpoint information: \"Host name\" : \"stack.nerc.mghpcc.org\" \"Port number\" : \"13808\" The EC2_ACCESS_KEY and EC2_SECRET_KEY keys that you noted from ec2rc.sh file can then be plugged into \"Access key ID\" and \"Secret access key\" respectively. Helpful Tips You can save your above configured session with some preferred name by clicking the \"Save\" button and then giving a proper name to your session. So that next time you don't need to again manually enter all your configuration. Using WinSCP You can follow above step to manually add a new session next time you open WinSCP or, you can connect to your previously saved session (as listed on popup dialog will show your all saved session name list) that will show up by just clicking on the session name. Then click \"Login\" button to connect to your NERC project's Object Storage as shown below: ii. Using Cyberduck Cyberduck is a libre server and cloud storage browser for Mac and Windows. With an easy-to-use interface, connect to servers, enterprise file sharing, and cloud storage. Prerequisites Cyberduck installed, see Download and Install the latest version of the Cyberduck for more information. Configuring Cyberduck Click on \"Open Connection\" tab button as shown below: Select \"Amazon S3\" from the dropdown options as shown below: Provide the following required endpoint information: \"Server\" : \"stack.nerc.mghpcc.org\" \"Port\" : \"13808\" The EC2_ACCESS_KEY and EC2_SECRET_KEY keys that you noted from ec2rc.sh file can then be plugged into \"Access key ID\" and \"Secret Access Key\" respectively Using Cyberduck Then click \"Connect\" button to connect to your NERC project's Object Storage as shown below:","title":"Object Storage/ Swift"},{"location":"openstack/advanced-openstack-topics/persistent-storage/object-storage/#object-storage","text":"OpenStack Object Storage (Swift) is a highly available, distributed, eventually consistent object/blob store. Object Storage is used to manage cost-effective and long-term preservation and storage of large amounts of data across clusters of standard server hardware. The common use cases include the storage, backup and archiving of unstructured data, such as documents, static web content, images, video files, and virtual machine images, etc. The end-users can interact with the object storage system through a RESTful HTTP API i.e. the Swift API or use one of the many client libraries that exist for all of the popular programming languages, such as Java, Python, Ruby, and C# based on provisioned quotas. Swift also supports and is compatible with Amazon's Simple Storage Service (S3) API that makes it easier for the end-users to move data between multiple storage end points and supports hybrid cloud setup.","title":"Object Storage"},{"location":"openstack/advanced-openstack-topics/persistent-storage/object-storage/#1-access-by-web-interface-ie-horizon-dashboard","text":"To get started, navigate to Project -> Object Store -> Containers.","title":"1. Access by Web Interface i.e. Horizon Dashboard"},{"location":"openstack/advanced-openstack-topics/persistent-storage/object-storage/#create-a-container","text":"In order to store objects, you need at least one Container to put them in. Containers are essentially top-level directories. Other services use the terminology buckets . Click Create Container. Give your container a name. Important Note The container name needs to be unique, not just within your project but across all of our OpenStack installation. If you get an error message after trying to create the container, try giving it a more unique name. For now, leave the \"Container Access\" set to Private .","title":"Create a Container"},{"location":"openstack/advanced-openstack-topics/persistent-storage/object-storage/#upload-a-file","text":"Click on the name of your container, and click the Upload File icon as shown below: Click Browse and select a file from your local machine to upload. It can take a while to upload very large files, so if you're just testing it out you may want to use a small text file or similar. By default the File Name will be the same as the original file, but you can change it to another name. Click \"Upload File\". Your file will appear inside the container as shown below once successful:","title":"Upload a File"},{"location":"openstack/advanced-openstack-topics/persistent-storage/object-storage/#using-folders","text":"Files stored by definition do not organize objects into folders, but you can use folders to keep your data organized. On the backend, the folder name is actually just prefixed to the object name, but from the web interface (and most other clients) it works just like a folder. To add a folder, click on the \"+ folder\" icon as shown below:","title":"Using Folders"},{"location":"openstack/advanced-openstack-topics/persistent-storage/object-storage/#make-a-container-public","text":"Making a container public allows you to send your collaborators a URL that gives access to the container's contents. Click on your container's name, then check the \"Public Access\" checkbox. Note that \"Public Access\" changes from \"Disabled\" to \"Link\". Click \"Link\" to see a list of object in the container. This is the URL of your container. Important Note Anyone who obtains the URL will be able to access the container, so this is not recommended as a way to share sensitive data with collaborators. In addition, everything inside a public container is public, so we recommend creating a separate container specifically for files that should be made public. To download the file test-file we would use the following url . Very Important Information Here 4c5bccef73c144679d44cbc96b42df4e is specific Tenant Id or Project Id . You can get this value when you click on the public container's Link on a new browser tab. Or , you can just click on \"Download\" next to the file's name as shown below: You can also interact with public objects using a utility such as curl : curl https://stack.nerc.mghpcc.org:13808/v1/AUTH_4c5bccef73c144679d44cbc96b42df4e/unique-container-test test-file To download a file: curl -o local-file.txt https://stack.nerc.mghpcc.org:13808/v1/AUTH_4c5bccef73c144679d44cbc96b42df4e/unique-container-test/test-file","title":"Make a container public"},{"location":"openstack/advanced-openstack-topics/persistent-storage/object-storage/#make-a-container-private","text":"You can make a public container private by clicking on your container's name, then uncheck the \"Public Access\" checkbox. Note that \"Public Access\" changes from \"Link\" to \"Disabled\". This will deactivate the public URL of the container and then it will show \"Disabled\".","title":"Make a container private"},{"location":"openstack/advanced-openstack-topics/persistent-storage/object-storage/#2-access-by-using-apis","text":"","title":"2. Access by using APIs"},{"location":"openstack/advanced-openstack-topics/persistent-storage/object-storage/#i-openstack-cli","text":"Prerequisites To run the OpenStack CLI commands, you need to have: OpenStack CLI setup, see OpenStack Command Line setup for more information.","title":"i. OpenStack CLI"},{"location":"openstack/advanced-openstack-topics/persistent-storage/object-storage/#some-object-storage-management-examples","text":"","title":"Some Object Storage management examples"},{"location":"openstack/advanced-openstack-topics/persistent-storage/object-storage/#create-a-container_1","text":"In order to create a container in the Object Storage service, you can use the OpenStack client with the following command. [user@laptop ~]$ openstack container create mycontainer +---------------------------------------+-------------+------------------------------------+ | account | container | x-trans-id | +---------------------------------------+-------------+------------------------------------+ | AUTH_4c5bccef73c144679d44cbc96b42df4e | mycontainer | txb875f426a011476785171-00624b37e8 | +---------------------------------------+-------------+------------------------------------+ Once created you can start adding objects.","title":"Create a container"},{"location":"openstack/advanced-openstack-topics/persistent-storage/object-storage/#manipulate-objects-in-a-container","text":"To upload files to a container you can use the following command $ openstack object create --name my_test_file mycontainer test_file.txt +--------------+-------------+----------------------------------+ | object | container | etag | +--------------+-------------+----------------------------------+ | my_test_file | mycontainer | e3024896943ee80422d1e5ff44423658 | +--------------+-------------+----------------------------------+ Once uploaded you can see the metadata through: $ openstack object show mycontainer my_test_file +----------------+---------------------------------------+ | Field | Value | +----------------+---------------------------------------+ | account | AUTH_4c5bccef73c144679d44cbc96b42df4e | | container | mycontainer | | content-length | 26 | | content-type | application/octet-stream | | etag | e3024896943ee80422d1e5ff44423658 | | last-modified | Mon, 04 Apr 2022 18:27:14 GMT | | object | my_test_file | +----------------+---------------------------------------+ You can save the contents of the object from your container to your local machine by using: $ openstack object save mycontainer my_test_file --file test_file.txt Very Important Please note that this will overwrite the file in the local directory. Finally you can delete the object with the following command $ openstack object delete mycontainer my_test_file","title":"Manipulate objects in a container"},{"location":"openstack/advanced-openstack-topics/persistent-storage/object-storage/#delete-the-container","text":"If you want to delete the container, you can use the following command $ openstack container delete mycontainer If the container has some data , you can trigger the recursive option to delete the objects internally. $ openstack container delete mycontainer Conflict (HTTP 409) (Request-ID: tx6b53c2b3e52d453e973b4-00624b400f) So, try to delete the container recursively using command $ openstack container delete --recursive mycontainer","title":"Delete the container"},{"location":"openstack/advanced-openstack-topics/persistent-storage/object-storage/#list-existing-containers","text":"You can check the existing containers with $ openstack container list +---------------+ | Name | +---------------+ | mycontainer | +---------------+","title":"List existing containers"},{"location":"openstack/advanced-openstack-topics/persistent-storage/object-storage/#swift-quota-utilization","text":"To check the overall space used, you can use the following command $ openstack object store account show +------------+---------------------------------------+ | Field | Value | +------------+---------------------------------------+ | Account | AUTH_4c5bccef73c144679d44cbc96b42df4e | | Bytes | 665 | | Containers | 1 | | Objects | 3 | +------------+---------------------------------------+ To check the space used by a specific container $ openstack container show mycontainer +----------------+---------------------------------------+ | Field | Value | +----------------+---------------------------------------+ | account | AUTH_4c5bccef73c144679d44cbc96b42df4e | | bytes_used | 665 | | container | mycontainer | | object_count | 3 | | read_acl | .r:*,.rlistings | | storage_policy | Policy-0 | +----------------+---------------------------------------+","title":"Swift quota utilization"},{"location":"openstack/advanced-openstack-topics/persistent-storage/object-storage/#ii-swift-interface","text":"This is a python client for the Swift API. There's a Python API (the swiftclient module), and a command-line script ( swift ). This example uses a Python3 virtual environment, but you are free to choose any other method to create a local virtual environment like Conda . py -3 -m venv venv Activate the virtual environment by running: on Linux/Mac: source venv/bin/activate on Windows: venv\\Scripts\\activate","title":"ii. Swift Interface"},{"location":"openstack/advanced-openstack-topics/persistent-storage/object-storage/#install-python-swift-client-page-at-pypi","text":"Once virtual environment is activated, install python-swiftclient and python-keystoneclient pip install python-swiftclient python-keystoneclient Swift authenticates using a user, tenant, and key, which map to your OpenStack username, project,and password. For this, you need to download the \"NERC's OpenStack RC File\" with the credentials for your NERC project from the NERC's OpenStack dashboard . Then you need to source that RC file using: source *-openrc.sh . You can read here on how to do this. By sourcing the \"NERC's OpenStack RC File\", you will set the all required environmental variablesand then type the following command to get a lits of your containers: swift list This will output your existing container on your project, for e.g. unique-container-test To upload a file to the above listed i.e. unique-container-test , you can run the following command: swift upload unique-container-test ./README.md Other helpful Swift commands: delete Delete a container or objects within a container. download Download objects from containers. list Lists the containers for the account or the objects for a container. post Updates meta information for the account, container, or object; creates containers if not present. copy Copies object, optionally adds meta stat Displays information for the account, container, or object. upload Uploads files or directories to the given container. capabilities List cluster capabilities. tempurl Create a temporary URL. auth Display auth related environment variables. bash_completion Outputs option and flag cli data ready for bash_completion. Helpful Tip Type swift -h to learn more about using the swift commands. The client has a --debug flag, which can be useful if you are facing any issues.","title":"Install Python Swift Client page at PyPi"},{"location":"openstack/advanced-openstack-topics/persistent-storage/object-storage/#iii-using-aws-cli","text":"The Ceph Object Gateway supports basic operations through the Amazon S3 interface . You can use both high-level (s3) commands with the AWS CLI and API-Level (s3api) commands with the AWS CLI to access object storage on your NERC project. Prerequisites To run the s3 or s3api commands, you need to have: AWS CLI installed, see Installing or updating the latest version of the AWS CLI for more information. The NERC's Swift End Point URL: https://stack.nerc.mghpcc.org:13808 Understand these Amazon S3 terms: i. Bucket \u2013 A top-level Amazon S3 folder. ii. Prefix \u2013 An Amazon S3 folder in a bucket. iii. Object \u2013 Any item that's hosted in an Amazon S3 bucket.","title":"iii. Using AWS CLI"},{"location":"openstack/advanced-openstack-topics/persistent-storage/object-storage/#configuring-the-aws-cli","text":"To access this interface, you must login through the OpenStack Dashboard and navigate to \"Projects > API Access\" where you can click on \"Download OpenStack RC File\" and select \"EC2 Credentials\". This will download a file zip file including ec2rc.sh file that has content similar to shown below. The important parts are EC2_ACCESS_KEY and EC2_SECRET_KEY , keep them noted. #!/bin/bash NOVARC=$(readlink -f \"${BASH_SOURCE:-${0}}\" 2>/dev/null) || NOVARC=$(python -c 'import os,sys; print os.path.abspath(os.path.realpath(sys.argv[1]))' \"${BASH_SOURCE:-${0}}\") NOVA_KEY_DIR=${NOVARC%/*} export EC2_ACCESS_KEY=... export EC2_SECRET_KEY=... export EC2_URL=https://localhost/notimplemented export EC2_USER_ID=42 # nova does not use user id, but bundling requires it export EC2_PRIVATE_KEY=${NOVA_KEY_DIR}/pk.pem export EC2_CERT=${NOVA_KEY_DIR}/cert.pem export NOVA_CERT=${NOVA_KEY_DIR}/cacert.pem export EUCALYPTUS_CERT=${NOVA_CERT} # euca-bundle-image seems to require this set alias ec2-bundle-image=\"ec2-bundle-image --cert ${EC2_CERT} --privatekey ${EC2_PRIVATE_KEY} --user 42 --ec2cert ${NOVA_CERT}\" alias ec2-upload-bundle=\"ec2-upload-bundle -a ${EC2_ACCESS_KEY} -s ${EC2_SECRET_KEY} --url ${S3_URL} --ec2cert ${NOVA_CERT}\" Source the downloaded OpenStack RC File by using: source *-openrc.sh command. Sourcing the RC File will set the required ${OS_PROJECT_NAME} envrionment variable. Then run aws configuration command which requires the EC2_ACCESS_KEY and EC2_SECRET_KEY keys that you noted from ec2rc.sh file (above): $> aws configure --profile \"'${OS_PROJECT_NAME}'\" AWS Access Key ID [None]: <EC2_ACCESS_KEY> AWS Secret Access Key [None]: <EC2_SECRET_KEY> Default region name [None]: Default output format [None]: Information We need to have a profile that you use must have permissions to allow the AWS operations can be performed.","title":"Configuring the AWS CLI"},{"location":"openstack/advanced-openstack-topics/persistent-storage/object-storage/#listing-buckets-using-aws-cli","text":"i. Using s3api : $ aws --profile \"'${OS_PROJECT_NAME}'\" --endpoint-url=https://stack.nerc.mghpcc.org:13808 \\ s3api list-buckets { \"Buckets\": [ { \"Name\": \"unique-container-test\", \"CreationDate\": \"2009-02-03T16:45:09+00:00\" } ], \"Owner\": { \"DisplayName\": \"Test Project-f69dcff:mmunakami@fas.harvard.edu\", \"ID\": \"Test Project-f69dcff:mmunakami@fas.harvard.edu\" } } ii. Alternatively, you can do the same using s3 : aws --profile \"'${OS_PROJECT_NAME}'\" --endpoint-url=https://stack.nerc.mghpcc.org:13808 \\ s3 ls Output: 2009-02-03 11:45:09 unique-container-test","title":"Listing buckets using aws-cli"},{"location":"openstack/advanced-openstack-topics/persistent-storage/object-storage/#to-list-contents-inside-bucket","text":"aws --profile \"'${OS_PROJECT_NAME}'\" --endpoint-url=https://stack.nerc.mghpcc.org:13808 \\ s3 ls s3://<your-bucket>","title":"To list contents inside bucket"},{"location":"openstack/advanced-openstack-topics/persistent-storage/object-storage/#to-make-a-bucket","text":"aws --profile \"'${OS_PROJECT_NAME}'\" --endpoint-url=https://stack.nerc.mghpcc.org:13808 \\ s3 mb s3://<your-bucket>","title":"To make a bucket"},{"location":"openstack/advanced-openstack-topics/persistent-storage/object-storage/#adding-copying-files-from-one-container-to-another-container","text":"Single file copy using cp command: The aws tool provides a cp command to move files to your s3 bucket: $ aws --profile \"'${OS_PROJECT_NAME}'\" --endpoint-url=https://stack.nerc.mghpcc.org:13808 \\ s3 cp <Your-file> s3://<your-bucket>/ Output: upload: .\\<Your-file> to s3://<your-bucket>/<Your-file> Whole directory copy using the --recursive flag $ aws --profile \"'${OS_PROJECT_NAME}'\" --endpoint-url=https://stack.nerc.mghpcc.org:13808 \\ s3 cp <Your-directory> s3://<your-bucket>/ --recursive Output: upload: <your-directory>/<file0> to s3://<your-bucket>/<file0> upload: <your-directory>/<file1> to s3://<your-bucket>/<file1> ... upload: <your-directory>/<fileN> to s3://<your-bucket>/<fileN> You can then use aws s3 ls to check that your files have been properly uploaded: aws --profile \"'${OS_PROJECT_NAME}'\" --endpoint-url=https://stack.nerc.mghpcc.org:13808 \\ s3 ls s3://<your-bucket>/ Output: 2022-04-04 16:32:38 <size> <file0> 2022-04-04 16:32:38 <size> <file1> ... 2022-04-04 16:25:50 <size> <fileN> Other Useful Flags Additionally, aws cp provides an --exclude flag to filter files not to be transferred, the syntax is: --exclude \"<regex>\"","title":"Adding/ Copying files from one container to another container"},{"location":"openstack/advanced-openstack-topics/persistent-storage/object-storage/#to-delete-an-object-from-a-bucket","text":"aws --profile \"'${OS_PROJECT_NAME}'\" --endpoint-url=https://stack.nerc.mghpcc.org:13808 \\ s3 rm s3://<your-bucket>/argparse-1.2.1.tar.gz","title":"To delete an object from a bucket"},{"location":"openstack/advanced-openstack-topics/persistent-storage/object-storage/#to-remove-a-bucket","text":"aws --profile \"'${OS_PROJECT_NAME}'\" --endpoint-url=https://stack.nerc.mghpcc.org:13808 \\ s3 rb s3://<your-bucket>","title":"To remove a bucket"},{"location":"openstack/advanced-openstack-topics/persistent-storage/object-storage/#iv-using-s3cmd","text":"S3cmd is a free command line tool and client for uploading, retrieving and managing data in Amazon S3 and other cloud storage service providers that use the S3 protocol. Prerequisites S3cmd installed, see Download and Install the latest version of the S3cmd for more information.","title":"iv. Using s3cmd"},{"location":"openstack/advanced-openstack-topics/persistent-storage/object-storage/#configuring-s3cmd","text":"The EC2_ACCESS_KEY and EC2_SECRET_KEY keys that you noted from ec2rc.sh file can then be plugged into s3cfg config file. The .s3cfg file requires the following configuration to work with our Object storage service: # Setup endpoint host_base = stack.nerc.mghpcc.org:13808 host_bucket = stack.nerc.mghpcc.org:13808 use_https = True # Setup access keys access_key = 'YOUR_EC2_ACCESS_KEY_FROM_ec2rc_FILE' secret_key = 'YOUR_EC2_SECRET_KEY_FROM_ec2rc_FILE', #pragma: allowlist secret # Enable S3 v4 signature APIs signature_v2 = False We are assuming that the configuration file is placed in default location i.e. $HOME/.s3cfg . If it is not the case you need to add the parameter --config=FILE with the location of your configuration file to override the config location.","title":"Configuring s3cmd"},{"location":"openstack/advanced-openstack-topics/persistent-storage/object-storage/#using-s3cmd","text":"","title":"Using s3cmd"},{"location":"openstack/advanced-openstack-topics/persistent-storage/object-storage/#to-list-buckets","text":"Use the following command to list all s3 buckets s3cmd ls Or, $ s3cmd ls s3:// 2009-02-03 16:45 s3://nerc-test-container 2009-02-03 16:45 s3://second-mycontainer 2009-02-03 16:45 s3://unique-container-test","title":"To list buckets"},{"location":"openstack/advanced-openstack-topics/persistent-storage/object-storage/#create-a-new-bucket","text":"In order to create a bucket, you can use s3cmd with the following command $ s3cmd mb s3://mybucket Bucket 's3://mybucket/' created $ s3cmd ls 2009-02-03 16:45 s3://mybucket 2009-02-03 16:45 s3://nerc-test-container 2009-02-03 16:45 s3://second-mycontainer 2009-02-03 16:45 s3://unique-container-test","title":"Create a new bucket"},{"location":"openstack/advanced-openstack-topics/persistent-storage/object-storage/#to-copy-an-object-to-bucket","text":"Below command will upload file file.txt to the bucket using s3cmd command. $ s3cmd put ~/file.txt s3://mybucket/ upload: 'file.txt' -> 's3://mybucket/file.txt' [1 of 1] 0 of 0 0% in 0s 0.00 B/s done s3cmd also allows to set additional properties to the objects stored. In the example below, we set the content type with the --mime-type option and the cache-control parameter to 1 hour with --add-header . s3cmd put --mime-type='application/json' --add-header='Cache-Control: max-age=3600' ~/file.txt s3://mybucket","title":"To copy an object to bucket"},{"location":"openstack/advanced-openstack-topics/persistent-storage/object-storage/#uploading-directory-in-bucket","text":"If we need to upload entire directory use -r to upload it recursively as below. $ s3cmd put -r <your-directory> s3://mybucket/ upload: 'backup/hello.txt' -> 's3://mybucket/backup/hello.txt' [1 of 1] 0 of 0 0% in 0s 0.00 B/s done","title":"Uploading Directory in bucket"},{"location":"openstack/advanced-openstack-topics/persistent-storage/object-storage/#list-the-objects-of-bucket","text":"List the objects of the bucket using ls switch with s3cmd. $ s3cmd ls s3://mybucket/ DIR s3://mybucket/backup/ 2022-04-05 03:10 0 s3://mybucket/file.txt 2022-04-05 03:14 0 s3://mybucket/hello.txt","title":"List the objects of bucket"},{"location":"openstack/advanced-openstack-topics/persistent-storage/object-storage/#to-copy-download-an-object-to-local-system","text":"Use the following command to download files from the bucket: $ s3cmd get s3://mybucket/file.txt download: 's3://mybucket/file.txt' -> './file.txt' [1 of 1] 0 of 0 0% in 0s 0.00 B/s done","title":"To copy/ download an object to local system"},{"location":"openstack/advanced-openstack-topics/persistent-storage/object-storage/#to-sync-local-filedirectory-to-a-bucket","text":"$ s3cmd sync newdemo s3://mybucket upload: 'newdemo/newdemo_file.txt' -> 's3://mybucket/newdemo/newdemo_file.txt' [1 of 1] 0 of 0 0% in 0s 0.00 B/s done","title":"To sync local file/directory to a bucket"},{"location":"openstack/advanced-openstack-topics/persistent-storage/object-storage/#to-sync-bucket-or-object-with-local-filesystem","text":"$ s3cmd sync s3://unique-container-test otherlocalbucket download: 's3://unique-container-test/README.md' -> 'otherlocalbucket/README.md' [1 of 3] 653 of 653 100% in 0s 4.54 kB/s done download: 's3://unique-container-test/image.png' -> 'otherlocalbucket/image.png' [2 of 3] 0 of 0 0% in 0s 0.00 B/s done download: 's3://unique-container-test/test-file' -> 'otherlocalbucket/test-file' [3 of 3] 12 of 12 100% in 0s 83.83 B/s done Done. Downloaded 665 bytes in 1.0 seconds, 665.00 B/s.","title":"To sync bucket or object with local filesystem"},{"location":"openstack/advanced-openstack-topics/persistent-storage/object-storage/#to-delete-an-object-from-bucket","text":"You can delete files from the bucket with the following s3cmd command $ s3cmd del s3://unique-container-test/README.md delete: 's3://unique-container-test/README.md'","title":"To delete an object from bucket"},{"location":"openstack/advanced-openstack-topics/persistent-storage/object-storage/#to-delete-directory-from-bucket","text":"$ s3cmd del s3://mybucket/newdemo delete: 's3://mybucket/newdemo'","title":"To delete directory from bucket"},{"location":"openstack/advanced-openstack-topics/persistent-storage/object-storage/#to-delete-a-bucket","text":"$ s3cmd rb s3://mybucket ERROR: S3 error: 409 (BucketNotEmpty): The bucket you tried to delete is not empty Important Information The above command failed because of the bucket was not empty! You can remove all objects inside the bucket and then use the command again. Or, you can run the following command with -r or --recursive flag i.e. s3cmd rb s3://mybucket -r or s3cmd rb s3://mybucket --recursive .","title":"To delete a bucket"},{"location":"openstack/advanced-openstack-topics/persistent-storage/object-storage/#v-using-rclone","text":"rclone is a convenient and performant command-line tool for transferring files and synchronizing directories directly between your local file systems and the NERC's containers. Prerequisites To run the rclone commands, you need to have: rclone installed, see Downloading and Installing the latest version of the rclone for more information.","title":"v. Using rclone"},{"location":"openstack/advanced-openstack-topics/persistent-storage/object-storage/#configuring-rclone","text":"First, you\u2019ll need to configure rclone . As the object storage systems have quite complicated authentication these are kept in a config file. If you run rclone config file you will see where the default location is for you. Note For Windows users, you many need to specify the full path to the rclone executable file, if its not included in your systems PATH variable. The EC2_ACCESS_KEY and EC2_SECRET_KEY keys that you noted from ec2rc.sh file can then be plugged into rclone config file. Edit the config file's content on the path location described by rclone config file command and add the following entry with the name [nerc] : [nerc] type = s3 env_auth = false provider = Other endpoint = https://stack.nerc.mghpcc.org:13808 acl = public-read access_key_id = 'YOUR_EC2_ACCESS_KEY_FROM_ec2rc_FILE' secret_access_key = 'YOUR_EC2_SECRET_KEY_FROM_ec2rc_FILE', #pragma: allowlist secret location_constraint = server_side_encryption = More about the config for AWS S3 compatible API can be seen here . Important Information Mind that if set env_auth = true then it will take variables from environment, so you shouldn't insert it in this case. OR, You can locally copy this content to a new config file and then use this flag to override the config location, e.g. rclone --config=FILE Interactive Configuration Run rclone config to setup. See rclone config docs for more details.","title":"Configuring rclone"},{"location":"openstack/advanced-openstack-topics/persistent-storage/object-storage/#using-rclone","text":"rclone supports many subcommands (see the complete list of rclone subcommands ). A few commonly used subcommands (assuming you configured the NERC Object Storage as nerc ):","title":"Using rclone"},{"location":"openstack/advanced-openstack-topics/persistent-storage/object-storage/#listing-the-containers-and-files-and-folders-within-a-container","text":"Once your Object Storage has been configured in rclone, you can then use the rclone interface to List all the Containers with the \"lsd\" command rclone lsd \"nerc:\" or, rclone lsd \"nerc:\" --config=rclone.conf For e.g., $ rclone lsd \"nerc:\" --config=rclone.conf -1 2009-02-03 11:45:09 -1 second-mycontainer -1 2009-02-03 11:45:09 -1 unique-container-test To list the files and folders available within a container i.e. \"unique-container-test\" in this case, within a container we can use the \"ls\" command $ rclone ls \"nerc:unique-container-test/\" 653 README.md 0 image.png 12 test-file","title":"Listing the Containers and Files and Folders within a Container"},{"location":"openstack/advanced-openstack-topics/persistent-storage/object-storage/#uploading-and-downloading-files-and-folders","text":"rclone support a variety of options to allow you to Copy, Sync and Move files from one destination to another. A simple example of this can be seen below, where we copy (Upload) the file \"upload.me\" to the <your-bucket> container: rclone copy \"./upload.me\" \"nerc:<your-bucket>/\" Another example, to copy (Download) the file \"upload.me\" from the <your-bucket> container to your local: rclone -P copy \"nerc:<your-bucket>/upload.me\" \"./\" Also, to Sync files into to the <your-bucket> container - try with --dry-run first rclone --dry-run sync /path/to/files nerc:<your-bucket> Then sync for real rclone sync /path/to/files nerc:<your-bucket>","title":"Uploading and Downloading Files and Folders"},{"location":"openstack/advanced-openstack-topics/persistent-storage/object-storage/#mounting-object-storage-on-local-filesystem","text":"Linux: First, you need to create a directory on which you will mount your filesystem: $ mkdir ~/mnt-rclone Then you can simply mount your object storage with: $ rclone -vv --vfs-cache-mode writes mount nerc: ~/mnt-rclone Windows: First you have to download Winfsp : WinFsp is an open source Windows File System Proxy which provides a FUSE emulation layer. Then you can simply mount your object storage with (no need to create the directory in advance): rclone -vv --vfs-cache-mode writes mount nerc: C:/mnt-rclone vfs-cache-mode flag enable file caching, you can use either writes or full option. For further explanation you can see official documentation . Now that your object storage is mounted, you can list, create and delete files in it.","title":"Mounting object storage on local filesystem"},{"location":"openstack/advanced-openstack-topics/persistent-storage/object-storage/#unmount-object-storage","text":"To unmount, simply press CTRL-C and the mount will be interrupted.","title":"Unmount object storage"},{"location":"openstack/advanced-openstack-topics/persistent-storage/object-storage/#vi-using-client-libraries","text":"a. The EC2_ACCESS_KEY and EC2_SECRET_KEY keys that you noted from ec2rc.sh file can then be plugged into your application. See below example using the Python Boto3 library , which connects through the S3 API interface through EC2 credentials, and perform some basic operations on available buckets and file that the user has access to. import boto3 # https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/s3.html#bucket s3 = boto3.resource('s3', aws_access_key_id='YOUR_EC2_ACCESS_KEY_FROM_ec2rc_FILE', aws_secret_access_key='YOUR_EC2_SECRET_KEY_FROM_ec2rc_FILE', #pragma: allowlist secret endpoint_url='https://stack.nerc.mghpcc.org:13808', ) # List all containers for bucket in s3.buckets.all(): print(' ->', bucket) # List all objects in a container i.e. unique-container-test is your current Container bucket = s3.Bucket('unique-container-test') for obj in bucket.objects.all(): print(' ->', obj) # Download an S3 object i.e. test-file a file available in your unique-container-test Container s3.Bucket('unique-container-test').download_file('test-file', './test-file.txt') # Add an image to the bucket # bucket.put_object(Body=open('image.png', mode='rb'), Key='image.png') We can configure the Python Boto3 library , to work with the saved aws profile. import boto3 # https://boto3.amazonaws.com/v1/documentation/api/latest/reference/core/session.html session = boto3.Session(profile_name='<YOUR_CONFIGURED_AWS_PROFILE_NAME>') # List all containers s3 = boto3.client('s3', endpoint_url='https://stack.nerc.mghpcc.org:13808',) response = s3.list_buckets() for bucket in response['Buckets']: print(' ->', bucket) b. The EC2_ACCESS_KEY and EC2_SECRET_KEY keys that you noted from ec2rc.sh file can then be plugged into your application. See below example using the Python Minio library , which connects through the S3 API interface through EC2 credentials, and perform some basic operations on available buckets and file that the user has access to. from minio import Minio # Create client with access key and secret key. # https://docs.min.io/docs/python-client-api-reference.html client = Minio( \"stack.nerc.mghpcc.org:13808\", access_key='YOUR_EC2_ACCESS_KEY_FROM_ec2rc_FILE', secret_key='YOUR_EC2_SECRET_KEY_FROM_ec2rc_FILE', #pragma: allowlist secret ) # List all containers buckets = client.list_buckets() for bucket in buckets: # print(bucket.name, bucket.creation_date) print(' ->', bucket) # Make 'nerc-test-container' container if not exist. found = client.bucket_exists(\"nerc-test-container\") if not found: client.make_bucket(\"nerc-test-container\") else: print(\"Bucket 'nerc-test-container' already exists\") # Upload './nerc-backup.zip' as object name 'nerc-backup-2022.zip' # to bucket 'nerc-test-container'. client.fput_object( \"nerc-test-container\", \"nerc-backup-2022.zip\", \"./nerc-backup.zip\", )","title":"vi. Using client libraries"},{"location":"openstack/advanced-openstack-topics/persistent-storage/object-storage/#3-using-graphical-user-interface-gui-tools","text":"","title":"3. Using Graphical User Interface (GUI) Tools"},{"location":"openstack/advanced-openstack-topics/persistent-storage/object-storage/#i-using-winscp","text":"WinSCP is a popular and free open-source SFTP client, SCP client, and FTP client for Windows. Its main function is file transfer between a local and a remote computer, with some basic file management functionality using FTP, FTPS, SCP, SFTP, WebDAV or S3 file transfer protocols. Prerequisites WinSCP installed, see Download and Install the latest version of the WinSCP for more information. Go to WinSCP menu and open \"Options > Preferences\". When the \"Preferences\" dialog window appears, select \"Transfer\" in the options on the left pane. Click on \"Edit\" button. Then, on shown popup dialog box review the \"Common options\" group, uncheck the \"Preserve timestamp\" option as shown below:","title":"i. Using WinSCP"},{"location":"openstack/advanced-openstack-topics/persistent-storage/object-storage/#configuring-winscp","text":"Click on \"New Session\" tab button as shown below: Select \"Amazon S3\" from the \"File protocol\" dropdown options as shown below: Provide the following required endpoint information: \"Host name\" : \"stack.nerc.mghpcc.org\" \"Port number\" : \"13808\" The EC2_ACCESS_KEY and EC2_SECRET_KEY keys that you noted from ec2rc.sh file can then be plugged into \"Access key ID\" and \"Secret access key\" respectively. Helpful Tips You can save your above configured session with some preferred name by clicking the \"Save\" button and then giving a proper name to your session. So that next time you don't need to again manually enter all your configuration.","title":"Configuring WinSCP"},{"location":"openstack/advanced-openstack-topics/persistent-storage/object-storage/#using-winscp","text":"You can follow above step to manually add a new session next time you open WinSCP or, you can connect to your previously saved session (as listed on popup dialog will show your all saved session name list) that will show up by just clicking on the session name. Then click \"Login\" button to connect to your NERC project's Object Storage as shown below:","title":"Using WinSCP"},{"location":"openstack/advanced-openstack-topics/persistent-storage/object-storage/#ii-using-cyberduck","text":"Cyberduck is a libre server and cloud storage browser for Mac and Windows. With an easy-to-use interface, connect to servers, enterprise file sharing, and cloud storage. Prerequisites Cyberduck installed, see Download and Install the latest version of the Cyberduck for more information.","title":"ii. Using Cyberduck"},{"location":"openstack/advanced-openstack-topics/persistent-storage/object-storage/#configuring-cyberduck","text":"Click on \"Open Connection\" tab button as shown below: Select \"Amazon S3\" from the dropdown options as shown below: Provide the following required endpoint information: \"Server\" : \"stack.nerc.mghpcc.org\" \"Port\" : \"13808\" The EC2_ACCESS_KEY and EC2_SECRET_KEY keys that you noted from ec2rc.sh file can then be plugged into \"Access key ID\" and \"Secret Access Key\" respectively","title":"Configuring Cyberduck"},{"location":"openstack/advanced-openstack-topics/persistent-storage/object-storage/#using-cyberduck","text":"Then click \"Connect\" button to connect to your NERC project's Object Storage as shown below:","title":"Using Cyberduck"},{"location":"openstack/advanced-openstack-topics/persistent-storage/volumes/","text":"Persistent Storage Volumes A volume is a detachable block storage device, similar to a USB hard drive. You can attach a volume to only one instance. Volumes are the Block Storage devices that you attach to instances to enable persistent storage. Users can attach a volume to a running instance or detach a volume and attach it to another instance at any time. Ownership of volumes can be transferred to another project. Some uses for volumes: Persistent data storage for ephemeral instances. Transfer of data between projects Bootable image where disk changes persist Mounting the disk of one instance to another for troubleshooting Navigate to Project -> Volumes -> Volumes. Create an empty volume An empty volume is like an unformatted USB stick. We'll attach it to an instance, create a filesystem on it, and mount it to the instance. Click \"Create Volume\". In the Create Volume dialog box, give your volume a name. The description field is optional. Choose \"empty volume\" from the Source dropdown. This will create a volume that is like an unformatted hard disk. Choose a size (In GiB) for your volume. Leave Type and Availibility Zone as it as. Only admin to the NERC OpenStack will be able to manage volume types. Click \"Create Volume\" button. In a few moments, the newly created volume will appear in the Volumes list with the Status \"Available\". Attach the volume to an instance In the Actions column, click the dropdown and select \"Manage Attachments\". From the menu, choose the instance you want to connect the volume to from Attach to Instance, and click \"Attach Volume\". The volume now has a status of \"In-use\" and \"Attached To\" column shows which instance it is attached to, and what device name it has. This will be something like /dev/vdb but it can vary depending on the state of your instance, and whether you have attached volumes before. Make note of the device name of your volume. Format and mount the volume SSH to your instance. You should now see the volume as an additional disk in the output of sudo fdisk -l or lsblk . # lsblk NAME MAJ:MIN RM SIZE RO TYPE MOUNTPOINT ... vda 254:0 0 10G 0 disk \u251c\u2500vda1 254:1 0 9.9G 0 part / \u251c\u2500vda14 254:14 0 4M 0 part \u2514\u2500vda15 254:15 0 106M 0 part /boot/efi vdb 254:16 0 1G 0 disk We see the volume here as the disk 'vdb', which matches the /dev/vdb/ we noted in \"Attached To\" column. Create a filesystem on the volume and mount it - in the example we create an ext4 filesystem: Run the following commands as root user: # mkfs.ext4 /dev/vdb # mkdir /mnt/test_volume # mount /dev/vdb /mnt/test_volume The volume is now available at the mount point: # lsblk NAME MAJ:MIN RM SIZE RO TYPE MOUNTPOINT ... vda 254:0 0 10G 0 disk \u251c\u2500vda1 254:1 0 9.9G 0 part / \u251c\u2500vda14 254:14 0 4M 0 part \u2514\u2500vda15 254:15 0 106M 0 part /boot/efi vdb 254:16 0 1G 0 disk /mnt/test_volume If you place data in the directory /mnt/test_volume , detach the volume, and mount it to another instance, the second instance will have access to the data. Important Note In this case it's easy to spot because there is only one additional disk attached to the instance, but it's important to keep track of the device name, especially if you have multiple volumes attached. Detach a volume To detach a mounted volume by going back to \"Manage Attachments\" and choosing Detach Volume. This will popup the following interface to proceed: Attach an existing volume to an instance Once it is successfully detached, you can use \"Manage Attachments\" to attach it to another instance if desired as explaned before. OR, You can attach the existing volume (Detached!) to the new instance as shown below: After this run the following commands as root user to mount it: # mkdir /mnt/test_volume # mount /dev/vdb /mnt/test_volume All the previous data from previous instance will be available under the mounted folder at /mnt/test_volume . Very Important Note Also, a given volume might not get the same device name the second time you attach it to an instance. Delete volumes When you delete an instance, the data of its attached volumes is not destroyed. Navigate to Project -> Volumes -> Volumes. Select the volume or volumes that you want to delete. Click \"Delete Volumes\" button. In the Confirm Delete Volumes window, click the Delete Volumes button to confirm the action. Create Volume from Image You can create a volume from an existing image. If the image is bootable, you can use the volume to launch an instance. Click \"Create Volume\". This time, in the Create Volume dialog box, for Volume Source, choose 'Image'. From the 'Use Image as a Source' dropdown, choose the image you'd like to use. To use this volume to launch an instance, you can choose Boot From Volume in the Instance \"Select Boot Source\" dropdown when creating an instance, then select your volume from the Volumes list. Make sure 'Delete Volume on Instance Delete' is selected \"No\" if you want the volume to persist even after the instance is terminated. Note Only one instance at a time can be booted from a given volume. Transfer a Volume You may wish to transfer a volume to a different project. Important The volume to be transferred must not be attached to an instance. This can be examined by looking into \"Status\" column of the volume i.e. it need to be \"Available\" instead of \"In-use\" and \"Attached To\" column need to be empty . Navigate to Project -> Volumes -> Volumes. Select the volume that you want to transfer and then click the dropdown next to the \"Edit volume\" and choose \"Create Transfer\". Give the transfer a name. You will see a screen like shown below. Be sure to capture the Transfer ID and the Authorization Key . Important Note You can always get the transfer ID later if needed, but there is no way to retrieve the key. If the key is lost before the transfer is completed, you will have to cancel the pending transfer and create a new one. Then the volume will show the status like below: Assuming you have access to the receiving project, switch to it using the Project dropdown at the top right. If you don't have access to the receiving project, give the transfer ID and Authorization Key to a collaborator who does, and have them complete the next steps. In the receiving project, go to the Volumes tab, and click \"Accept Transfer\" button as shown below: Enter the \"Transfer ID\" and the \"Authorization Key\" that were captured when the transfer was created in the previous project. The volume should now appear in the Volumes list of the receiving project as shown below: Important Note Any pending transfers can be cancelled if they are not yet accepted, but there is no way to \"undo\" a transfer once it is complete. To send the volume back to the original project, a new transfer would be required.","title":"Block Storage/ Volumes/ Cinder"},{"location":"openstack/advanced-openstack-topics/persistent-storage/volumes/#persistent-storage","text":"","title":"Persistent Storage"},{"location":"openstack/advanced-openstack-topics/persistent-storage/volumes/#volumes","text":"A volume is a detachable block storage device, similar to a USB hard drive. You can attach a volume to only one instance. Volumes are the Block Storage devices that you attach to instances to enable persistent storage. Users can attach a volume to a running instance or detach a volume and attach it to another instance at any time. Ownership of volumes can be transferred to another project. Some uses for volumes: Persistent data storage for ephemeral instances. Transfer of data between projects Bootable image where disk changes persist Mounting the disk of one instance to another for troubleshooting Navigate to Project -> Volumes -> Volumes.","title":"Volumes"},{"location":"openstack/advanced-openstack-topics/persistent-storage/volumes/#create-an-empty-volume","text":"An empty volume is like an unformatted USB stick. We'll attach it to an instance, create a filesystem on it, and mount it to the instance. Click \"Create Volume\". In the Create Volume dialog box, give your volume a name. The description field is optional. Choose \"empty volume\" from the Source dropdown. This will create a volume that is like an unformatted hard disk. Choose a size (In GiB) for your volume. Leave Type and Availibility Zone as it as. Only admin to the NERC OpenStack will be able to manage volume types. Click \"Create Volume\" button. In a few moments, the newly created volume will appear in the Volumes list with the Status \"Available\".","title":"Create an empty volume"},{"location":"openstack/advanced-openstack-topics/persistent-storage/volumes/#attach-the-volume-to-an-instance","text":"In the Actions column, click the dropdown and select \"Manage Attachments\". From the menu, choose the instance you want to connect the volume to from Attach to Instance, and click \"Attach Volume\". The volume now has a status of \"In-use\" and \"Attached To\" column shows which instance it is attached to, and what device name it has. This will be something like /dev/vdb but it can vary depending on the state of your instance, and whether you have attached volumes before. Make note of the device name of your volume.","title":"Attach the volume to an instance"},{"location":"openstack/advanced-openstack-topics/persistent-storage/volumes/#format-and-mount-the-volume","text":"SSH to your instance. You should now see the volume as an additional disk in the output of sudo fdisk -l or lsblk . # lsblk NAME MAJ:MIN RM SIZE RO TYPE MOUNTPOINT ... vda 254:0 0 10G 0 disk \u251c\u2500vda1 254:1 0 9.9G 0 part / \u251c\u2500vda14 254:14 0 4M 0 part \u2514\u2500vda15 254:15 0 106M 0 part /boot/efi vdb 254:16 0 1G 0 disk We see the volume here as the disk 'vdb', which matches the /dev/vdb/ we noted in \"Attached To\" column. Create a filesystem on the volume and mount it - in the example we create an ext4 filesystem: Run the following commands as root user: # mkfs.ext4 /dev/vdb # mkdir /mnt/test_volume # mount /dev/vdb /mnt/test_volume The volume is now available at the mount point: # lsblk NAME MAJ:MIN RM SIZE RO TYPE MOUNTPOINT ... vda 254:0 0 10G 0 disk \u251c\u2500vda1 254:1 0 9.9G 0 part / \u251c\u2500vda14 254:14 0 4M 0 part \u2514\u2500vda15 254:15 0 106M 0 part /boot/efi vdb 254:16 0 1G 0 disk /mnt/test_volume If you place data in the directory /mnt/test_volume , detach the volume, and mount it to another instance, the second instance will have access to the data. Important Note In this case it's easy to spot because there is only one additional disk attached to the instance, but it's important to keep track of the device name, especially if you have multiple volumes attached.","title":"Format and mount the volume"},{"location":"openstack/advanced-openstack-topics/persistent-storage/volumes/#detach-a-volume","text":"To detach a mounted volume by going back to \"Manage Attachments\" and choosing Detach Volume. This will popup the following interface to proceed:","title":"Detach a volume"},{"location":"openstack/advanced-openstack-topics/persistent-storage/volumes/#attach-an-existing-volume-to-an-instance","text":"Once it is successfully detached, you can use \"Manage Attachments\" to attach it to another instance if desired as explaned before. OR, You can attach the existing volume (Detached!) to the new instance as shown below: After this run the following commands as root user to mount it: # mkdir /mnt/test_volume # mount /dev/vdb /mnt/test_volume All the previous data from previous instance will be available under the mounted folder at /mnt/test_volume . Very Important Note Also, a given volume might not get the same device name the second time you attach it to an instance.","title":"Attach an existing volume to an instance"},{"location":"openstack/advanced-openstack-topics/persistent-storage/volumes/#delete-volumes","text":"When you delete an instance, the data of its attached volumes is not destroyed. Navigate to Project -> Volumes -> Volumes. Select the volume or volumes that you want to delete. Click \"Delete Volumes\" button. In the Confirm Delete Volumes window, click the Delete Volumes button to confirm the action.","title":"Delete volumes"},{"location":"openstack/advanced-openstack-topics/persistent-storage/volumes/#create-volume-from-image","text":"You can create a volume from an existing image. If the image is bootable, you can use the volume to launch an instance. Click \"Create Volume\". This time, in the Create Volume dialog box, for Volume Source, choose 'Image'. From the 'Use Image as a Source' dropdown, choose the image you'd like to use. To use this volume to launch an instance, you can choose Boot From Volume in the Instance \"Select Boot Source\" dropdown when creating an instance, then select your volume from the Volumes list. Make sure 'Delete Volume on Instance Delete' is selected \"No\" if you want the volume to persist even after the instance is terminated. Note Only one instance at a time can be booted from a given volume.","title":"Create Volume from Image"},{"location":"openstack/advanced-openstack-topics/persistent-storage/volumes/#transfer-a-volume","text":"You may wish to transfer a volume to a different project. Important The volume to be transferred must not be attached to an instance. This can be examined by looking into \"Status\" column of the volume i.e. it need to be \"Available\" instead of \"In-use\" and \"Attached To\" column need to be empty . Navigate to Project -> Volumes -> Volumes. Select the volume that you want to transfer and then click the dropdown next to the \"Edit volume\" and choose \"Create Transfer\". Give the transfer a name. You will see a screen like shown below. Be sure to capture the Transfer ID and the Authorization Key . Important Note You can always get the transfer ID later if needed, but there is no way to retrieve the key. If the key is lost before the transfer is completed, you will have to cancel the pending transfer and create a new one. Then the volume will show the status like below: Assuming you have access to the receiving project, switch to it using the Project dropdown at the top right. If you don't have access to the receiving project, give the transfer ID and Authorization Key to a collaborator who does, and have them complete the next steps. In the receiving project, go to the Volumes tab, and click \"Accept Transfer\" button as shown below: Enter the \"Transfer ID\" and the \"Authorization Key\" that were captured when the transfer was created in the previous project. The volume should now appear in the Volumes list of the receiving project as shown below: Important Note Any pending transfers can be cancelled if they are not yet accepted, but there is no way to \"undo\" a transfer once it is complete. To send the volume back to the original project, a new transfer would be required.","title":"Transfer a Volume"},{"location":"openstack/advanced-openstack-topics/python-sdk/python-SDK/","text":"References Python SDK page at PyPi OpenStack Python SDK User Guide From the Python SDK page at Pypi: Definition openstacksdk is a client library for building applications to work with OpenStack clouds. The project aims to provide a consistent and complete set of interactions with OpenStack's many services, along with complete documentation, examples, and tools. If you need to plug OpenStack into existing scripts using another language, there are a variety of other SDKs at various levels of active development. A list of known SDKs is maintained on the official OpenStack wiki. Known SDKs","title":"Python SDK"},{"location":"openstack/advanced-openstack-topics/python-sdk/python-SDK/#references","text":"Python SDK page at PyPi OpenStack Python SDK User Guide From the Python SDK page at Pypi: Definition openstacksdk is a client library for building applications to work with OpenStack clouds. The project aims to provide a consistent and complete set of interactions with OpenStack's many services, along with complete documentation, examples, and tools. If you need to plug OpenStack into existing scripts using another language, there are a variety of other SDKs at various levels of active development. A list of known SDKs is maintained on the official OpenStack wiki. Known SDKs","title":"References"},{"location":"openstack/advanced-openstack-topics/setting-up-a-network/create-a-router/","text":"Create a Router A router acts as a gateway for external connectivity. By connecting your private network to the public network via a router, you can connect your instance to the Internet, install packages, etc. without needing to associate it with a public IP address. You can view routers by clicking Project, then click Network panel and choose Routers from the tabs that appears. Click \"Create Network\" button on the right side of the screen. In the Create Router dialog box, specify a name for the router. From the External Network dropdown, select the \u2018provider\u2019 network, and click \"Create Router\" button. This will set the Gateway for the new router to public network. The new router is now displayed in the Routers tab. You should now see the router in the Network Topology view. (It also appears under Project -> Network -> Routers). Notice that it is now connected to the public network, but not your private network. Set Internal Interface on the Router In order to route between your private network and the outside world, you must give the router an interface on your private network. Perform the following steps in order to To connect a private network to the newly created router: a. On the Routers tab, click the name of the router. b. On the Router Details page, click the Interfaces tab, then click Add Interface. c. In the Add Interface dialog box, select a Subnet. Optionally, in the Add Interface dialog box, set an IP Address for the router interface for the selected subnet. If you choose not to set the IP Address value, then by default OpenStack Networking uses the first host IP address in the subnet. The Router Name and Router ID fields are automatically updated. d. Click \"Add Interface\". The Router will now appear connected to the private network in Network Topology tab. OR, You can set Internal Interface on the Router From the Network Topology view, click on the router you just created, and click \u2018Add Interface\u2019 on the popup that appears. Then, this will show Add Interface dialog box. So, you just complete steps b to c as mentioned above.","title":"Create a Router"},{"location":"openstack/advanced-openstack-topics/setting-up-a-network/create-a-router/#create-a-router","text":"A router acts as a gateway for external connectivity. By connecting your private network to the public network via a router, you can connect your instance to the Internet, install packages, etc. without needing to associate it with a public IP address. You can view routers by clicking Project, then click Network panel and choose Routers from the tabs that appears. Click \"Create Network\" button on the right side of the screen. In the Create Router dialog box, specify a name for the router. From the External Network dropdown, select the \u2018provider\u2019 network, and click \"Create Router\" button. This will set the Gateway for the new router to public network. The new router is now displayed in the Routers tab. You should now see the router in the Network Topology view. (It also appears under Project -> Network -> Routers). Notice that it is now connected to the public network, but not your private network.","title":"Create a Router"},{"location":"openstack/advanced-openstack-topics/setting-up-a-network/create-a-router/#set-internal-interface-on-the-router","text":"In order to route between your private network and the outside world, you must give the router an interface on your private network. Perform the following steps in order to To connect a private network to the newly created router: a. On the Routers tab, click the name of the router. b. On the Router Details page, click the Interfaces tab, then click Add Interface. c. In the Add Interface dialog box, select a Subnet. Optionally, in the Add Interface dialog box, set an IP Address for the router interface for the selected subnet. If you choose not to set the IP Address value, then by default OpenStack Networking uses the first host IP address in the subnet. The Router Name and Router ID fields are automatically updated. d. Click \"Add Interface\". The Router will now appear connected to the private network in Network Topology tab. OR, You can set Internal Interface on the Router From the Network Topology view, click on the router you just created, and click \u2018Add Interface\u2019 on the popup that appears. Then, this will show Add Interface dialog box. So, you just complete steps b to c as mentioned above.","title":"Set Internal Interface on the Router"},{"location":"openstack/advanced-openstack-topics/setting-up-a-network/set-up-a-private-network/","text":"Set up a Private Network Default Network for your Project During your project setup, NERC will setup a default network, router and interface for your project that is ready-to-use. Create Your Own Private Network You can view/ create your/ existing network topology by clicking Project, then click Network panel and choose Network Topology from the tabs that appears. This shows public network which is accessible to all projects. Click on \"Networks\" tab and then click \"Create Network\" button on the right side of the screen. In the Create Network dialog box, specify the following values. Network tab: Network Name: Specify a name to identify the network. Admin State: The state to start the network in. Create Subnet: Select this check box to create a subnet Give your network a name, and leave the two checkboxes for \"Admin State\" and \"Create Subnet\" with the default settings. Subnet tab: You do not have to specify a subnet when you create a network, but if you do not specify a subnet, the network can not be attached to an instance. Subnet Name: Specify a name for the subnet. Network Address: Specify the IP address for the subnet. For your private networks, you should use IP addresses which fall within the ranges that are specifically reserved for private networks: 10.0.0.0/8 172.16.0.0/12 192.168.0.0/16 In the example below, we configure a network containing addresses 192.168.0.1 to 192.168.0.255 using CIDR 192.168.0.0/24 Technically, your private network will still work if you choose any IP outside these ranges, but this causes problems with connecting to IPs in the outside world - so don't do it! IP Version: Select IPv4 or IPv6. Gateway IP: Specify an IP address for a specific gateway. This parameter is optional. Disable Gateway: Select this check box to disable a gateway IP address. Subnet Details tab Enable DHCP: Select this check box to enable DHCP so that your VM instances will automatically be assigned an IP on the subnet. Allocation Pools: Specify IP address pools. DNS Name Servers: Specify a name for the DNS server. If you use '8.8.8.8' (you may recognize this as one of Google's public name servers). Host Routes: Specify the IP address of host routes. For now, you can leave the Allocation Pools and Host Routes boxes empty and click on \"Create\" button. But here we specify Allocation Pools of 192.168.0.2,192.168.0.254 . The Network Topology should now show your virtual private network next to the public network.","title":"Set up a Private Network"},{"location":"openstack/advanced-openstack-topics/setting-up-a-network/set-up-a-private-network/#set-up-a-private-network","text":"Default Network for your Project During your project setup, NERC will setup a default network, router and interface for your project that is ready-to-use.","title":"Set up a Private Network"},{"location":"openstack/advanced-openstack-topics/setting-up-a-network/set-up-a-private-network/#create-your-own-private-network","text":"You can view/ create your/ existing network topology by clicking Project, then click Network panel and choose Network Topology from the tabs that appears. This shows public network which is accessible to all projects. Click on \"Networks\" tab and then click \"Create Network\" button on the right side of the screen. In the Create Network dialog box, specify the following values. Network tab: Network Name: Specify a name to identify the network. Admin State: The state to start the network in. Create Subnet: Select this check box to create a subnet Give your network a name, and leave the two checkboxes for \"Admin State\" and \"Create Subnet\" with the default settings. Subnet tab: You do not have to specify a subnet when you create a network, but if you do not specify a subnet, the network can not be attached to an instance. Subnet Name: Specify a name for the subnet. Network Address: Specify the IP address for the subnet. For your private networks, you should use IP addresses which fall within the ranges that are specifically reserved for private networks: 10.0.0.0/8 172.16.0.0/12 192.168.0.0/16 In the example below, we configure a network containing addresses 192.168.0.1 to 192.168.0.255 using CIDR 192.168.0.0/24 Technically, your private network will still work if you choose any IP outside these ranges, but this causes problems with connecting to IPs in the outside world - so don't do it! IP Version: Select IPv4 or IPv6. Gateway IP: Specify an IP address for a specific gateway. This parameter is optional. Disable Gateway: Select this check box to disable a gateway IP address. Subnet Details tab Enable DHCP: Select this check box to enable DHCP so that your VM instances will automatically be assigned an IP on the subnet. Allocation Pools: Specify IP address pools. DNS Name Servers: Specify a name for the DNS server. If you use '8.8.8.8' (you may recognize this as one of Google's public name servers). Host Routes: Specify the IP address of host routes. For now, you can leave the Allocation Pools and Host Routes boxes empty and click on \"Create\" button. But here we specify Allocation Pools of 192.168.0.2,192.168.0.254 . The Network Topology should now show your virtual private network next to the public network.","title":"Create Your Own Private Network"},{"location":"openstack/advanced-openstack-topics/setting-up-your-own-images/how-to-build-windows-image/","text":"Virtual Machine Image Guide An OpenStack Compute cloud needs to have virtual machine images in order to launch an instance. A virtual machine image is a single file which contains a virtual disk that has a bootable operating system installed on it. Very Important The provided Windows Server 2022 R2 image is for evaluation only. This evaluation edition expires in 180 days . This is intended to evaluate if the product is right for you. This is on user discretion to update, extend, and handle licensing issues for future usages. Existing Microsoft Windows Image Cloudbase Solutions provides Microsoft Windows Server 2022 R2 Standard Evaluation for OpenStack . This includes the required support for hypervisor-specific drivers (Hyper-V / KVM). Also integrated are the guest initialization tools (Cloudbase-Init), security updates, proper performance, and security configurations as well as the final Sysprep. How to Build and Upload your custom Microsoft Windows Image Overall Process To create a new image, you will need the installation CD or DVD ISO file for the guest operating system. You will also need access to a virtualization tool. You can use KVM hypervisor for this. Or, if you have a GUI desktop virtualization tool (such as, virt-manager , VMware Fusion or VirtualBox), you can use that instead. Convert the file to QCOW2 (KVM, Xen) once you are done. You can customize and build the new image manually on your own system and then upload the image to the NERC's OpenStack Compute cloud. Please follow the following steps which describes how to obtain, create, and modify virtual machine images that are compatible with the NERC's OpenStack. 1. Prerequisite Follow these steps to prepare the installation a. Download a Windows Server 2022 installation ISO file. Evaluation images are available on the Microsoft website ( registration required ). b. Download the signed VirtIO drivers ISO file from the Fedora website . c. Install Virtual Machine Manager on your local Windows 10 machine using WSL: Enable WSL on your local Windows 10 subsystem for Linux: The steps given here are straightforward, however, before following them make sure on Windows 10, you have WSL enabled and have at least Ubuntu 20.04 or above LTS version running over it. If you don\u2019t know how to do that then see our tutorial on how to enable WSL and install Ubuntu over it . Download and install MobaXterm: MobaXterm is a free application that can be downloaded using this link . After downloading, install it like any other normal Windows software. Open MobaXterm and run WSL Linux: As you open this advanced terminal for Windows 10, WSL installed Ubuntu app will show on the left side panel of it. Double click on that to start the WSL session. Install Virt-Manager: sudo apt update sudo apt install virt-manager Run Virtual Machine Manager: Start the Virtual Machine Manager running this command on the opened terminal: virt-manager as shown below: This will open Virt-Manager as following: Connect QEMU/KVM user session on Virt-Manager: 2. Create a virtual machine Create a virtual machine with the storage set to a 15 GB qcow2 disk image using Virtual Machine Manager Please set 15 GB disk image size as shown below: Set the virtual machine name and also make sure \"Customize configuration before install\" is selected as shown below: 3. Customize the Virtual machine Enable the VirtIO driver. By default, the Windows installer does not detect the disk. Click Add Hardware > select CDROM device and attach to downloaded virtio-win-* ISO file: Make sure the NIC is using the virtio Device model as shown below: Make sure to set proper order of Boot Options as shown below, so that CDROM with Windows ISO is set on the first and Apply the order change. After this please begin windows installation by clicking on \"Begin Installation\" button. Click \"Apply\" button. 4. Continue with the Windows installation You need to continue with the Windows installation process. When prompted you can choose \"Windows Server 2022 Standard Evaluation (Desktop Experinece)\" option as shown below: Load VirtIO SCSI drivers and network drivers by choosing an installation target when prompted. Click Load driver and browse the file system. Select the E:\\virtio-win-*\\viostor\\2k22\\amd64 folder. When converting an image file with Windows, ensure the virtio driver is installed. Otherwise, you will get a blue screen when launching the image due to lack of the virtio driver. The Windows installer displays a list of drivers to install. Select the VirtIO SCSI drivers. Click Load driver again and browse the file system, and select the E:\\NetKVM\\2k22\\amd64 folder. Select the network drivers, and continue the installation. 5. Restart the installed virtual machine (VM) Once the installation is completed, the VM restarts Define a password for the Adminstrator when prompted and click on \"Finish\" button: Send the \"Ctrl+Alt+Delete\" key using Send Key Menu, this will unlock the windows and then prompt login for the Administrator - please login using the password you set on previous step: 6. Go to device manager and install all unrecognized devices Similarly as shown above repeat and install all missing drivers. 7. Enable Remote Desktop Protocol (RDP) login Explicitly enable RDP login and uncheck \"Require computers to use Network Level Authentication to connect\" option 8. Delete the recovery parition Delete the recovery parition which will allow expanding the Image as required running the following commands on Command Prompt (Run as Adminstrator) diskpart select disk 0 list partition select partition 3 delete partition override list partition and then extend C: drive to take up the remaining space using \"Disk Management\" . 9. Install any new Windows updates. (Optional) 10. Setup cloudbase-init to generate QCOW2 image Download and install stable version of cloudbase-init (A Windows project providing guest initialization features, similar to cloud-init) by browsing the Download Page on the web browser on virtual machine running Windows, you can escape registering and just click on \"No. just show me the downloads\" to navigate to the download page as shown below: During Installation, set Serial port for logging to COM1 as shown below: When the installation is done, in the Complete the Cloudbase-Init Setup Wizard window, select the Run Sysprep and Shutdown check boxes and click \"Finish\" as shown below: Wait for the machine to shutdown. 11. Where is the newly generated QCOW2 image? The Sysprep will generate QCOW2 image i.e. win2k22.qcow2 on /home/<YourUserName>/.local/share/libvirt/images/ 12. Create OpenStack image and push to NERC's image list You can copy/download this windows image to the folder where you configured your OpenStack CLI as described Here and upload to the NERC's OpenStack running the following OpenStack Image API command: openstack image create --disk-format qcow2 --file win2k22.qcow2 MS-Windows-2022 You can verify the uploaded image is available by running: openstack image list +--------------------------------------+---------------------+--------+ | ID | Name | Status | +--------------------------------------+---------------------+--------+ | 7da9f5d4-4836-4bv8-bc5e-xc07ac6d8171 | MS-Windows-2022 | active | | ... | ... | ... | +--------------------------------------+---------------------+--------+ 13. Launch an instance using newly uploaded MS-Windows-2022 image Login to the NERC's OpenStack and verify the uploaded MS-Windows-2022 is there also available on the NERC's OpenStack Images List for your project as shown below: Create a Volume using that Windows Image : Once successfully Volume is created, we can use the Volume to launch an instance as shown below: Add other information and setup a Security Group that allows RDP as shown below: Click on detail view of the Instance and then click on Console tab menu and click on \"Send CtrlAltDel\" button located on the top right side of the console as shown below: 14. How to have Remote Desktop login to your Windows instance Remote Desktop login should work with the Floating IP associated with the instance:","title":"Setting up custom images"},{"location":"openstack/advanced-openstack-topics/setting-up-your-own-images/how-to-build-windows-image/#virtual-machine-image-guide","text":"An OpenStack Compute cloud needs to have virtual machine images in order to launch an instance. A virtual machine image is a single file which contains a virtual disk that has a bootable operating system installed on it. Very Important The provided Windows Server 2022 R2 image is for evaluation only. This evaluation edition expires in 180 days . This is intended to evaluate if the product is right for you. This is on user discretion to update, extend, and handle licensing issues for future usages.","title":"Virtual Machine Image Guide"},{"location":"openstack/advanced-openstack-topics/setting-up-your-own-images/how-to-build-windows-image/#existing-microsoft-windows-image","text":"Cloudbase Solutions provides Microsoft Windows Server 2022 R2 Standard Evaluation for OpenStack . This includes the required support for hypervisor-specific drivers (Hyper-V / KVM). Also integrated are the guest initialization tools (Cloudbase-Init), security updates, proper performance, and security configurations as well as the final Sysprep.","title":"Existing Microsoft Windows Image"},{"location":"openstack/advanced-openstack-topics/setting-up-your-own-images/how-to-build-windows-image/#how-to-build-and-upload-your-custom-microsoft-windows-image","text":"Overall Process To create a new image, you will need the installation CD or DVD ISO file for the guest operating system. You will also need access to a virtualization tool. You can use KVM hypervisor for this. Or, if you have a GUI desktop virtualization tool (such as, virt-manager , VMware Fusion or VirtualBox), you can use that instead. Convert the file to QCOW2 (KVM, Xen) once you are done. You can customize and build the new image manually on your own system and then upload the image to the NERC's OpenStack Compute cloud. Please follow the following steps which describes how to obtain, create, and modify virtual machine images that are compatible with the NERC's OpenStack.","title":"How to Build and Upload your custom Microsoft Windows Image"},{"location":"openstack/advanced-openstack-topics/setting-up-your-own-images/how-to-build-windows-image/#1-prerequisite","text":"Follow these steps to prepare the installation a. Download a Windows Server 2022 installation ISO file. Evaluation images are available on the Microsoft website ( registration required ). b. Download the signed VirtIO drivers ISO file from the Fedora website . c. Install Virtual Machine Manager on your local Windows 10 machine using WSL: Enable WSL on your local Windows 10 subsystem for Linux: The steps given here are straightforward, however, before following them make sure on Windows 10, you have WSL enabled and have at least Ubuntu 20.04 or above LTS version running over it. If you don\u2019t know how to do that then see our tutorial on how to enable WSL and install Ubuntu over it . Download and install MobaXterm: MobaXterm is a free application that can be downloaded using this link . After downloading, install it like any other normal Windows software. Open MobaXterm and run WSL Linux: As you open this advanced terminal for Windows 10, WSL installed Ubuntu app will show on the left side panel of it. Double click on that to start the WSL session. Install Virt-Manager: sudo apt update sudo apt install virt-manager Run Virtual Machine Manager: Start the Virtual Machine Manager running this command on the opened terminal: virt-manager as shown below: This will open Virt-Manager as following: Connect QEMU/KVM user session on Virt-Manager:","title":"1. Prerequisite"},{"location":"openstack/advanced-openstack-topics/setting-up-your-own-images/how-to-build-windows-image/#2-create-a-virtual-machine","text":"Create a virtual machine with the storage set to a 15 GB qcow2 disk image using Virtual Machine Manager Please set 15 GB disk image size as shown below: Set the virtual machine name and also make sure \"Customize configuration before install\" is selected as shown below:","title":"2. Create a virtual machine"},{"location":"openstack/advanced-openstack-topics/setting-up-your-own-images/how-to-build-windows-image/#3-customize-the-virtual-machine","text":"Enable the VirtIO driver. By default, the Windows installer does not detect the disk. Click Add Hardware > select CDROM device and attach to downloaded virtio-win-* ISO file: Make sure the NIC is using the virtio Device model as shown below: Make sure to set proper order of Boot Options as shown below, so that CDROM with Windows ISO is set on the first and Apply the order change. After this please begin windows installation by clicking on \"Begin Installation\" button. Click \"Apply\" button.","title":"3. Customize the Virtual machine"},{"location":"openstack/advanced-openstack-topics/setting-up-your-own-images/how-to-build-windows-image/#4-continue-with-the-windows-installation","text":"You need to continue with the Windows installation process. When prompted you can choose \"Windows Server 2022 Standard Evaluation (Desktop Experinece)\" option as shown below: Load VirtIO SCSI drivers and network drivers by choosing an installation target when prompted. Click Load driver and browse the file system. Select the E:\\virtio-win-*\\viostor\\2k22\\amd64 folder. When converting an image file with Windows, ensure the virtio driver is installed. Otherwise, you will get a blue screen when launching the image due to lack of the virtio driver. The Windows installer displays a list of drivers to install. Select the VirtIO SCSI drivers. Click Load driver again and browse the file system, and select the E:\\NetKVM\\2k22\\amd64 folder. Select the network drivers, and continue the installation.","title":"4. Continue with the Windows installation"},{"location":"openstack/advanced-openstack-topics/setting-up-your-own-images/how-to-build-windows-image/#5-restart-the-installed-virtual-machine-vm","text":"Once the installation is completed, the VM restarts Define a password for the Adminstrator when prompted and click on \"Finish\" button: Send the \"Ctrl+Alt+Delete\" key using Send Key Menu, this will unlock the windows and then prompt login for the Administrator - please login using the password you set on previous step:","title":"5. Restart the installed virtual machine (VM)"},{"location":"openstack/advanced-openstack-topics/setting-up-your-own-images/how-to-build-windows-image/#6-go-to-device-manager-and-install-all-unrecognized-devices","text":"Similarly as shown above repeat and install all missing drivers.","title":"6. Go to device manager and install all unrecognized devices"},{"location":"openstack/advanced-openstack-topics/setting-up-your-own-images/how-to-build-windows-image/#7-enable-remote-desktop-protocol-rdp-login","text":"Explicitly enable RDP login and uncheck \"Require computers to use Network Level Authentication to connect\" option","title":"7. Enable Remote Desktop Protocol (RDP) login"},{"location":"openstack/advanced-openstack-topics/setting-up-your-own-images/how-to-build-windows-image/#8-delete-the-recovery-parition","text":"Delete the recovery parition which will allow expanding the Image as required running the following commands on Command Prompt (Run as Adminstrator) diskpart select disk 0 list partition select partition 3 delete partition override list partition and then extend C: drive to take up the remaining space using \"Disk Management\" .","title":"8. Delete the recovery parition"},{"location":"openstack/advanced-openstack-topics/setting-up-your-own-images/how-to-build-windows-image/#9-install-any-new-windows-updates-optional","text":"","title":"9. Install any new Windows updates. (Optional)"},{"location":"openstack/advanced-openstack-topics/setting-up-your-own-images/how-to-build-windows-image/#10-setup-cloudbase-init-to-generate-qcow2-image","text":"Download and install stable version of cloudbase-init (A Windows project providing guest initialization features, similar to cloud-init) by browsing the Download Page on the web browser on virtual machine running Windows, you can escape registering and just click on \"No. just show me the downloads\" to navigate to the download page as shown below: During Installation, set Serial port for logging to COM1 as shown below: When the installation is done, in the Complete the Cloudbase-Init Setup Wizard window, select the Run Sysprep and Shutdown check boxes and click \"Finish\" as shown below: Wait for the machine to shutdown.","title":"10. Setup cloudbase-init to generate QCOW2 image"},{"location":"openstack/advanced-openstack-topics/setting-up-your-own-images/how-to-build-windows-image/#11-where-is-the-newly-generated-qcow2-image","text":"The Sysprep will generate QCOW2 image i.e. win2k22.qcow2 on /home/<YourUserName>/.local/share/libvirt/images/","title":"11. Where is the newly generated QCOW2 image?"},{"location":"openstack/advanced-openstack-topics/setting-up-your-own-images/how-to-build-windows-image/#12-create-openstack-image-and-push-to-nercs-image-list","text":"You can copy/download this windows image to the folder where you configured your OpenStack CLI as described Here and upload to the NERC's OpenStack running the following OpenStack Image API command: openstack image create --disk-format qcow2 --file win2k22.qcow2 MS-Windows-2022 You can verify the uploaded image is available by running: openstack image list +--------------------------------------+---------------------+--------+ | ID | Name | Status | +--------------------------------------+---------------------+--------+ | 7da9f5d4-4836-4bv8-bc5e-xc07ac6d8171 | MS-Windows-2022 | active | | ... | ... | ... | +--------------------------------------+---------------------+--------+","title":"12. Create OpenStack image and push to NERC's image list"},{"location":"openstack/advanced-openstack-topics/setting-up-your-own-images/how-to-build-windows-image/#13-launch-an-instance-using-newly-uploaded-ms-windows-2022-image","text":"Login to the NERC's OpenStack and verify the uploaded MS-Windows-2022 is there also available on the NERC's OpenStack Images List for your project as shown below: Create a Volume using that Windows Image : Once successfully Volume is created, we can use the Volume to launch an instance as shown below: Add other information and setup a Security Group that allows RDP as shown below: Click on detail view of the Instance and then click on Console tab menu and click on \"Send CtrlAltDel\" button located on the top right side of the console as shown below:","title":"13. Launch an instance using newly uploaded MS-Windows-2022 image"},{"location":"openstack/advanced-openstack-topics/setting-up-your-own-images/how-to-build-windows-image/#14-how-to-have-remote-desktop-login-to-your-windows-instance","text":"Remote Desktop login should work with the Floating IP associated with the instance:","title":"14. How to have Remote Desktop login to your Windows instance"},{"location":"openstack/advanced-openstack-topics/terraform/terraform-on-NERC/","text":"Provisioning the NERC resources using Terraform Terraform is an open-source Infrastructure as Code (IaC) software tool that works with NERC and allows you to orchestrate, provision, and manage infrastructure resources quickly and easily. Terraform codifies cloud application programming interfaces (APIs) into human-readable, declarative configuration ( *.tf ) files. These files are used to manage underlying infrastructure rather than through NERC's web-based graphical interface - Horizon . Terraform allows you to build, change, and manage your infrastructure in a safe, consistent, and repeatable way by defining resource configurations that you can version, reuse, and share. Terraform\u2019s main job is to create, modify, and destroy compute instances, private networks and other NERC resources. Benefits of Terraform If you have multiple instances/ VMs you are managing for your work or research, it can be simpler and more reproducible if you are doing it with automation tool like Terraform . Installing Terraform To use Terraform you will need to install it from here . Basic Template to use Terraform on your NERC Project You can Git clone: git clone https://github.com/nerc-project/terraform-nerc.git and run our base template for terraform to provision some basic NERC's OpenStack resources using this terraform-nerc repo . Note The main branch of this git repo should be a good starting point in developing your own terraform code. Template to setup R Shiny server using Terraform on your NERC Project You can Git clone: git clone https://github.com/nerc-project/terraform-nerc-r-shiny.git and can run this template locally using terraform to provision R Shiny server on NERC's OpenStack resources using this terraform-nerc-r-shiny repo . Important Note Please make sure to review bash script file i.e. install-R-Shiny.sh located in this repo that is pointing as user-data-path variable in example.tfvars . This repo includes the script required to setup Shiny R server. You can use similar concept to any other project that needs custom user defined scripts while launching an instance. If you want to change and update this script you can just change this file and then run terraform plan and terraform apply command pointing this example.tfvars file. How Terraform Works Terraform reads configuration files and provides an execution plan of changes, which can be reviewed for safety and then applied and provisioned. Terraform reads all files with the extension .tf in your current directory. Resources can be in a single file, or organised across several different files. The basic Terraform deployment workflow is: i. Scope - Identify the infrastructure for your project. ii. Author - Write the configuration for your infrastructure in which you declare the elements of your infrastructure that you want to create. The format of the resource definition is straightforward and looks like this: resource type_of_resource \"resource name\" { attribute = \"attribue value\" ... } iii. Initialize - Install the plugins Terraform needs to manage the infrastructure. iv. Plan - Preview the changes Terraform will make to match your configuration. v. Apply - Make the planned changes. Running Terraform The Terraform deployment workflow on the NERC looks like this: Prerequisite You can download the \"NERC's OpenStack RC File\" with the credentials for your NERC project from the NERC's OpenStack dashboard . Then you need to source that RC file using: source *-openrc.sh . You can read here on how to do this. Setup SSH key pairs running ssh-keygen -t rsa -f username-keypair and then make sure the newly generated SSH key pairs exist on your ~/.ssh folder. Terraform Init The first command that should be run after writing a new Terraform configuration or cloning an existing one is terraform init . This command is used to initialize a working directory containing Terraform configuration files and install the plugins. Information You will need to run terraform init if you make any changes to providers. Terraform Plan terraform plan command creates an execution plan, which lets you preview the changes that Terraform plans to make to your infrastructure based on your configuration files. Terraform Apply When you use terraform apply without passing it a saved plan file, it incorporates the terraform plan command functionality and so the planning options are also available while running this command. Input Variables on the Command Line You can use the -var 'NAME=VALUE' command line option to specify values for input variables declared in your root module for e.g. terraform plan -var 'name=value' In most cases, it will be more convenient to set values for potentially many input variables declared in the root module of the configuration, using definitions from a \"tfvars\" file and use it using -var-file=FILENAME command for e.g. terraform plan -var-file=FILENAME Track your infrastructure and Collaborate Terraform keeps track of your real infrastructure in a state file, which acts as a source of truth for your environment. Terraform uses the state file to determine the changes to make to your infrastructure so that it will match your configuration. Terraform's state allows you to track resource changes throughout your deployments. You can securely share your state with your teammates, provide a stable environment for Terraform to run in, and prevent race conditions when multiple people make configuration changes at once. Some useful Terraform commands terraform init terraform fmt terraform validate terraform plan terraform apply terraform show terraform destroy terraform output","title":"Terraform on NERC"},{"location":"openstack/advanced-openstack-topics/terraform/terraform-on-NERC/#provisioning-the-nerc-resources-using-terraform","text":"Terraform is an open-source Infrastructure as Code (IaC) software tool that works with NERC and allows you to orchestrate, provision, and manage infrastructure resources quickly and easily. Terraform codifies cloud application programming interfaces (APIs) into human-readable, declarative configuration ( *.tf ) files. These files are used to manage underlying infrastructure rather than through NERC's web-based graphical interface - Horizon . Terraform allows you to build, change, and manage your infrastructure in a safe, consistent, and repeatable way by defining resource configurations that you can version, reuse, and share. Terraform\u2019s main job is to create, modify, and destroy compute instances, private networks and other NERC resources.","title":"Provisioning the NERC resources using Terraform"},{"location":"openstack/advanced-openstack-topics/terraform/terraform-on-NERC/#benefits-of-terraform","text":"If you have multiple instances/ VMs you are managing for your work or research, it can be simpler and more reproducible if you are doing it with automation tool like Terraform .","title":"Benefits of Terraform"},{"location":"openstack/advanced-openstack-topics/terraform/terraform-on-NERC/#installing-terraform","text":"To use Terraform you will need to install it from here .","title":"Installing Terraform"},{"location":"openstack/advanced-openstack-topics/terraform/terraform-on-NERC/#basic-template-to-use-terraform-on-your-nerc-project","text":"You can Git clone: git clone https://github.com/nerc-project/terraform-nerc.git and run our base template for terraform to provision some basic NERC's OpenStack resources using this terraform-nerc repo . Note The main branch of this git repo should be a good starting point in developing your own terraform code.","title":"Basic Template to use Terraform on your NERC Project"},{"location":"openstack/advanced-openstack-topics/terraform/terraform-on-NERC/#template-to-setup-r-shiny-server-using-terraform-on-your-nerc-project","text":"You can Git clone: git clone https://github.com/nerc-project/terraform-nerc-r-shiny.git and can run this template locally using terraform to provision R Shiny server on NERC's OpenStack resources using this terraform-nerc-r-shiny repo . Important Note Please make sure to review bash script file i.e. install-R-Shiny.sh located in this repo that is pointing as user-data-path variable in example.tfvars . This repo includes the script required to setup Shiny R server. You can use similar concept to any other project that needs custom user defined scripts while launching an instance. If you want to change and update this script you can just change this file and then run terraform plan and terraform apply command pointing this example.tfvars file.","title":"Template to setup R Shiny server using Terraform on your NERC Project"},{"location":"openstack/advanced-openstack-topics/terraform/terraform-on-NERC/#how-terraform-works","text":"Terraform reads configuration files and provides an execution plan of changes, which can be reviewed for safety and then applied and provisioned. Terraform reads all files with the extension .tf in your current directory. Resources can be in a single file, or organised across several different files. The basic Terraform deployment workflow is: i. Scope - Identify the infrastructure for your project. ii. Author - Write the configuration for your infrastructure in which you declare the elements of your infrastructure that you want to create. The format of the resource definition is straightforward and looks like this: resource type_of_resource \"resource name\" { attribute = \"attribue value\" ... } iii. Initialize - Install the plugins Terraform needs to manage the infrastructure. iv. Plan - Preview the changes Terraform will make to match your configuration. v. Apply - Make the planned changes.","title":"How Terraform Works"},{"location":"openstack/advanced-openstack-topics/terraform/terraform-on-NERC/#running-terraform","text":"The Terraform deployment workflow on the NERC looks like this:","title":"Running Terraform"},{"location":"openstack/advanced-openstack-topics/terraform/terraform-on-NERC/#prerequisite","text":"You can download the \"NERC's OpenStack RC File\" with the credentials for your NERC project from the NERC's OpenStack dashboard . Then you need to source that RC file using: source *-openrc.sh . You can read here on how to do this. Setup SSH key pairs running ssh-keygen -t rsa -f username-keypair and then make sure the newly generated SSH key pairs exist on your ~/.ssh folder.","title":"Prerequisite"},{"location":"openstack/advanced-openstack-topics/terraform/terraform-on-NERC/#terraform-init","text":"The first command that should be run after writing a new Terraform configuration or cloning an existing one is terraform init . This command is used to initialize a working directory containing Terraform configuration files and install the plugins. Information You will need to run terraform init if you make any changes to providers.","title":"Terraform Init"},{"location":"openstack/advanced-openstack-topics/terraform/terraform-on-NERC/#terraform-plan","text":"terraform plan command creates an execution plan, which lets you preview the changes that Terraform plans to make to your infrastructure based on your configuration files.","title":"Terraform Plan"},{"location":"openstack/advanced-openstack-topics/terraform/terraform-on-NERC/#terraform-apply","text":"When you use terraform apply without passing it a saved plan file, it incorporates the terraform plan command functionality and so the planning options are also available while running this command.","title":"Terraform Apply"},{"location":"openstack/advanced-openstack-topics/terraform/terraform-on-NERC/#input-variables-on-the-command-line","text":"You can use the -var 'NAME=VALUE' command line option to specify values for input variables declared in your root module for e.g. terraform plan -var 'name=value' In most cases, it will be more convenient to set values for potentially many input variables declared in the root module of the configuration, using definitions from a \"tfvars\" file and use it using -var-file=FILENAME command for e.g. terraform plan -var-file=FILENAME","title":"Input Variables on the Command Line"},{"location":"openstack/advanced-openstack-topics/terraform/terraform-on-NERC/#track-your-infrastructure-and-collaborate","text":"Terraform keeps track of your real infrastructure in a state file, which acts as a source of truth for your environment. Terraform uses the state file to determine the changes to make to your infrastructure so that it will match your configuration. Terraform's state allows you to track resource changes throughout your deployments. You can securely share your state with your teammates, provide a stable environment for Terraform to run in, and prevent race conditions when multiple people make configuration changes at once.","title":"Track your infrastructure and Collaborate"},{"location":"openstack/advanced-openstack-topics/terraform/terraform-on-NERC/#some-useful-terraform-commands","text":"terraform init terraform fmt terraform validate terraform plan terraform apply terraform show terraform destroy terraform output","title":"Some useful Terraform commands"},{"location":"openstack/create-and-connect-to-the-VM/assign-a-floating-IP/","text":"Assign a Floating IP When an instance is created in OpenStack, it is automatically assigned a fixed IP address in the network to which the instance is assigned. This IP address is permanently associated with the instance until the instance is terminated. However, in addition to the fixed IP address, a floating IP address can also be attached to an instance. Unlike fixed IP addresses, floating IP addresses can have their associations modified at any time, regardless of the state of the instances involved. Floating IPs are a limited resource, so your project will have a quota based on its needs. You should only assign public IPs to VMs that need them. This procedure details the reservation of a floating IP address from an existing pool of addresses and the association of that address with a specific instance. By attaching a Floating IP to your instance, you can ssh into your vm from your local machine. Make sure you are using key forwarding as described in Create a Key Pair . Allocate a Floating IP Navigate to Project -> Compute -> Instances Next to Instance Name -> Click Actions dropdown arrow (far right) -> Choose Associate Floating IP If you have some floating IPs already allocated to your project which are not yet associated with a VM, they will be available in the dropdown list on this screen. If you have no floating IPs allocated, or all your allocated IPs are in use already, the dropdown list will be empty. Click the + symbol to allocate an IP. You will see the following screen. Make sure 'provider' appears in the dropdown menu, and that you have not already met your quota of allocated IPs. In this example, the project has a quota of 50 floating IPs, but we have allocated 5 so far, so we can still allocate up to 45 IPs. Click \"Allocate IP\". You will get a green \"success\" popup in the top left that shows your public IP address. You will get a red error message instead if you attempt to exceed your project's floating IP quota. If you have not tried to exceed your quota, but you get a red error message anyway, please contact [TODO:contact_mail] for help. NOw click on \"Associate\" button. Then, a green \"success\" popup in the top left and you can see the floating IP is attached to your VM on the Instances page: Disassociate a Floating IP You may need to disassociate a Floating IP from an instance which no longer needs it, so you can assign it to one that does. Navigate to Project -> Compute -> Instances Find the instance you want to remove the IP from in the list. Click the red \"Disassociate Floating IP\" to the right. This IP will be disassociated from the instance, but it will still remain allocated to your project. Release a Floating IP You may discover that your project does not need all the floating IPs that are allocated to it. We can release a Floating IP while disassociating it just we need to check the \"Release Floating IP\" option as shown here: OR, Navigate to Project -> Network -> Floating IPs To release the floating IP address back into the floating IP pool, click the Release Floating IP option in the Actions column.","title":"Assign a Floating IP"},{"location":"openstack/create-and-connect-to-the-VM/assign-a-floating-IP/#assign-a-floating-ip","text":"When an instance is created in OpenStack, it is automatically assigned a fixed IP address in the network to which the instance is assigned. This IP address is permanently associated with the instance until the instance is terminated. However, in addition to the fixed IP address, a floating IP address can also be attached to an instance. Unlike fixed IP addresses, floating IP addresses can have their associations modified at any time, regardless of the state of the instances involved. Floating IPs are a limited resource, so your project will have a quota based on its needs. You should only assign public IPs to VMs that need them. This procedure details the reservation of a floating IP address from an existing pool of addresses and the association of that address with a specific instance. By attaching a Floating IP to your instance, you can ssh into your vm from your local machine. Make sure you are using key forwarding as described in Create a Key Pair .","title":"Assign a Floating IP"},{"location":"openstack/create-and-connect-to-the-VM/assign-a-floating-IP/#allocate-a-floating-ip","text":"Navigate to Project -> Compute -> Instances Next to Instance Name -> Click Actions dropdown arrow (far right) -> Choose Associate Floating IP If you have some floating IPs already allocated to your project which are not yet associated with a VM, they will be available in the dropdown list on this screen. If you have no floating IPs allocated, or all your allocated IPs are in use already, the dropdown list will be empty. Click the + symbol to allocate an IP. You will see the following screen. Make sure 'provider' appears in the dropdown menu, and that you have not already met your quota of allocated IPs. In this example, the project has a quota of 50 floating IPs, but we have allocated 5 so far, so we can still allocate up to 45 IPs. Click \"Allocate IP\". You will get a green \"success\" popup in the top left that shows your public IP address. You will get a red error message instead if you attempt to exceed your project's floating IP quota. If you have not tried to exceed your quota, but you get a red error message anyway, please contact [TODO:contact_mail] for help. NOw click on \"Associate\" button. Then, a green \"success\" popup in the top left and you can see the floating IP is attached to your VM on the Instances page:","title":"Allocate a Floating IP"},{"location":"openstack/create-and-connect-to-the-VM/assign-a-floating-IP/#disassociate-a-floating-ip","text":"You may need to disassociate a Floating IP from an instance which no longer needs it, so you can assign it to one that does. Navigate to Project -> Compute -> Instances Find the instance you want to remove the IP from in the list. Click the red \"Disassociate Floating IP\" to the right. This IP will be disassociated from the instance, but it will still remain allocated to your project.","title":"Disassociate a Floating IP"},{"location":"openstack/create-and-connect-to-the-VM/assign-a-floating-IP/#release-a-floating-ip","text":"You may discover that your project does not need all the floating IPs that are allocated to it. We can release a Floating IP while disassociating it just we need to check the \"Release Floating IP\" option as shown here: OR, Navigate to Project -> Network -> Floating IPs To release the floating IP address back into the floating IP pool, click the Release Floating IP option in the Actions column.","title":"Release a Floating IP"},{"location":"openstack/create-and-connect-to-the-VM/flavors/","text":"Nova flavors In NERC OpenStack, flavors define the compute, memory, and storage capacity of nova computing instances. In other words, a flavor is an available hardware configuration for a server. Currently, our setup supports and offers the following flavors NERC offers the following flavors based on our Infrastructure-as-a-Service (IaaS) - OpenStack offerings (Tiers of Service). 1. Standard Compute Tier The standard compute flavor \"cpu-a\" is provided from Lenovo SD530 (2x Intel 8268 2.9 GHz, 48 core, 384 GB memory) server. The base service unit (SU) is 1 vCPU, 2 GB memory at a rate of $0.016 / hr of wall time. Multiples of the cpu-a SU are available with 20 GB root disk, and the price scales accordingly: Flavor SUs Cost / hr cpu-a.1 1 $0.016 cpu-a.2 2 $0.032 cpu-a.4 4 $0.064 cpu-a.8 8 $0.128 cpu-a.16 16 $0.256 2. Memory Optimized Tier The memory optimized flavor \"mem-a\" is provided from the same servers at \"cpu-a\" but with 8 GB of memory per core. The base service unit (SU) is 1 vCPU, 8 GB memory at a rate of $0.030 / hr of wall time. Multiples of the mem-a SU are available with 20 GB root disk, and the price scales accordingly: Flavor SUs Cost / hr mem-a.1 1 $0.030 mem-a.2 2 $0.060 mem-a.4 4 $0.120 mem-a.8 8 $0.240 mem-a.16 16 $0.480 3. GPU Tier Information NERC also supports the most demanding workloads including Artificial Intelligence (AI), Machine Learning (ML) training and Deep Learning modeling, simulation, data analytics, data visualization, distributed databases, and more. For such demanding workloads, the NERC\u2019s GPU-based distributed computing flavor is recommended, which is integrated into a specialized hardware such as GPUs that produce unprecedented performance boosts for technical computing workloads. There are two flavors within the GPU tier, one featuring older NVidia K80s and the newer NVidia A100s technology . The \"gpu-k80\" flavor is provided from Supermicro (2x Intel E5-2620 v3, 24 core, 128GB memory, 2x NVidia K80s) servers. The base service unit is 25% of a whole server, so 1 SU provides 6 vCPU, 32 GB memory, 1 NVidia K80 at a rate of $0.190 / hr of wall time. Multiples of the gpu-k80 SU are available with 20 GB root disk. Flavor SUs Cost / hr gpu-k80.1 1 $0.190 gpu-k80.2 2 $0.380 gpu-k80.4 4 $0.760 The gpu-k80 flavor will be available soon. We are still working on setting up the hardware required to enable the cost-effective \"gpu-k80\" flavor. We will let you know once it is ready and available for your general use. The \"gpu-a100\" flavor is provided from Lenovo SR670 (2x Intel 8268 2.9 GHz, 48 core, 384 GB memory, 4x NVidia A100) servers. These latest GPUs deliver industry-leading high throughput and low latency networking. The base service unit is 25% of a whole server, so 1 SU provides 12 vCPU, 96 GB memory, 1 NVidia A100 at a rate of $0.537 / hr of wall time. Multiples of the gpu-a100 SU are available with 20 GB root disk. Flavor SUs Cost / hr gpu-a100.1 1 $0.537 gpu-a100.2 2 $1.074 gpu-a100.4 4 $2.148 Pro Tip Choose a flavor for your instance that suits your requirements, use-cases, and budget when launching a VM.","title":"Available NOVA Flavors"},{"location":"openstack/create-and-connect-to-the-VM/flavors/#nova-flavors","text":"In NERC OpenStack, flavors define the compute, memory, and storage capacity of nova computing instances. In other words, a flavor is an available hardware configuration for a server.","title":"Nova flavors"},{"location":"openstack/create-and-connect-to-the-VM/flavors/#currently-our-setup-supports-and-offers-the-following-flavors","text":"NERC offers the following flavors based on our Infrastructure-as-a-Service (IaaS) - OpenStack offerings (Tiers of Service).","title":"Currently, our setup supports and offers the following flavors"},{"location":"openstack/create-and-connect-to-the-VM/flavors/#1-standard-compute-tier","text":"The standard compute flavor \"cpu-a\" is provided from Lenovo SD530 (2x Intel 8268 2.9 GHz, 48 core, 384 GB memory) server. The base service unit (SU) is 1 vCPU, 2 GB memory at a rate of $0.016 / hr of wall time. Multiples of the cpu-a SU are available with 20 GB root disk, and the price scales accordingly: Flavor SUs Cost / hr cpu-a.1 1 $0.016 cpu-a.2 2 $0.032 cpu-a.4 4 $0.064 cpu-a.8 8 $0.128 cpu-a.16 16 $0.256","title":"1. Standard Compute Tier"},{"location":"openstack/create-and-connect-to-the-VM/flavors/#2-memory-optimized-tier","text":"The memory optimized flavor \"mem-a\" is provided from the same servers at \"cpu-a\" but with 8 GB of memory per core. The base service unit (SU) is 1 vCPU, 8 GB memory at a rate of $0.030 / hr of wall time. Multiples of the mem-a SU are available with 20 GB root disk, and the price scales accordingly: Flavor SUs Cost / hr mem-a.1 1 $0.030 mem-a.2 2 $0.060 mem-a.4 4 $0.120 mem-a.8 8 $0.240 mem-a.16 16 $0.480","title":"2. Memory Optimized Tier"},{"location":"openstack/create-and-connect-to-the-VM/flavors/#3-gpu-tier","text":"Information NERC also supports the most demanding workloads including Artificial Intelligence (AI), Machine Learning (ML) training and Deep Learning modeling, simulation, data analytics, data visualization, distributed databases, and more. For such demanding workloads, the NERC\u2019s GPU-based distributed computing flavor is recommended, which is integrated into a specialized hardware such as GPUs that produce unprecedented performance boosts for technical computing workloads. There are two flavors within the GPU tier, one featuring older NVidia K80s and the newer NVidia A100s technology . The \"gpu-k80\" flavor is provided from Supermicro (2x Intel E5-2620 v3, 24 core, 128GB memory, 2x NVidia K80s) servers. The base service unit is 25% of a whole server, so 1 SU provides 6 vCPU, 32 GB memory, 1 NVidia K80 at a rate of $0.190 / hr of wall time. Multiples of the gpu-k80 SU are available with 20 GB root disk. Flavor SUs Cost / hr gpu-k80.1 1 $0.190 gpu-k80.2 2 $0.380 gpu-k80.4 4 $0.760 The gpu-k80 flavor will be available soon. We are still working on setting up the hardware required to enable the cost-effective \"gpu-k80\" flavor. We will let you know once it is ready and available for your general use. The \"gpu-a100\" flavor is provided from Lenovo SR670 (2x Intel 8268 2.9 GHz, 48 core, 384 GB memory, 4x NVidia A100) servers. These latest GPUs deliver industry-leading high throughput and low latency networking. The base service unit is 25% of a whole server, so 1 SU provides 12 vCPU, 96 GB memory, 1 NVidia A100 at a rate of $0.537 / hr of wall time. Multiples of the gpu-a100 SU are available with 20 GB root disk. Flavor SUs Cost / hr gpu-a100.1 1 $0.537 gpu-a100.2 2 $1.074 gpu-a100.4 4 $2.148 Pro Tip Choose a flavor for your instance that suits your requirements, use-cases, and budget when launching a VM.","title":"3. GPU Tier"},{"location":"openstack/create-and-connect-to-the-VM/images/","text":"Images Image composed of a virtual collection of a kernel, operating system, and configuration. Glance Glance is the API-driven OpenStack image service that provides services and associated libraries to store, browse, register, distribute, and retrieve bootable disk images. It acts as a registry for virtual machine images, allowing users to copy server images for immediate storage. These images can be used as templates when setting up new instances. NERC Images List NERC provides a set of default images that can be used as source while launching an instance: Name centos-7-x86_64 debian-10-x86_64 fedora-36-x86_64 rocky-8-x86_64 ubuntu-18.04-x86_64 ubuntu-20.04-x86_64 ubuntu-22.04-x86_64 How to create and upload own custom images? Beside the above mentioned system provided images users can customize and upload their own images to the NERC, as demonstrated in this documentation . Please refer to this guide to understand more on how can you get other virtual machine images to the NERC's OpenStack platform.","title":"Available Images"},{"location":"openstack/create-and-connect-to-the-VM/images/#images","text":"Image composed of a virtual collection of a kernel, operating system, and configuration.","title":"Images"},{"location":"openstack/create-and-connect-to-the-VM/images/#glance","text":"Glance is the API-driven OpenStack image service that provides services and associated libraries to store, browse, register, distribute, and retrieve bootable disk images. It acts as a registry for virtual machine images, allowing users to copy server images for immediate storage. These images can be used as templates when setting up new instances.","title":"Glance"},{"location":"openstack/create-and-connect-to-the-VM/images/#nerc-images-list","text":"NERC provides a set of default images that can be used as source while launching an instance: Name centos-7-x86_64 debian-10-x86_64 fedora-36-x86_64 rocky-8-x86_64 ubuntu-18.04-x86_64 ubuntu-20.04-x86_64 ubuntu-22.04-x86_64","title":"NERC Images List"},{"location":"openstack/create-and-connect-to-the-VM/images/#how-to-create-and-upload-own-custom-images","text":"Beside the above mentioned system provided images users can customize and upload their own images to the NERC, as demonstrated in this documentation . Please refer to this guide to understand more on how can you get other virtual machine images to the NERC's OpenStack platform.","title":"How to create and upload own custom images?"},{"location":"openstack/create-and-connect-to-the-VM/launch-a-VM/","text":"Launch a VM Background To start a VM, we will need a base image. NERC has made several Public images available to users. Launch an Instance Navigate: Project -> Compute -> Images. Click Launch Instance next to the public image of your choice. In the example, we chose ubuntu-22.04-x86_64 , you may choose any available images. *Important: There are multiple tabs along the top of the the pop up window. Make sure you review all of them as per instructions before clicking on Launch! Otherwise, your launched VM may be inaccessible.* In the Launch Instance dialog box, specify the following values: Details tab Instance Name: Give your instance a name that assign a name to the virtual machine. Important Note The instance name you assign here becomes the initial host name of the server. If the name is longer than 63 characters, the Compute service truncates it automatically to ensure dnsmasq works correctly. Availability Zone: By default, this value is set to the availability zone given by the cloud provider i.e. nova . Count: To launch multiple instances, enter a value greater than 1. The default is 1. Source tab: Double check that in the dropdown \"Select Boot Source,\" \"Image\" is selected. Important Note To create an image that uses the boot volume sized according to the flavor ensure that \"No\" is selected under the \"Create New Volume\" section. When you deploy a non-ephemeral instance (i.e. Creating a new volume), and indicate \"Yes\" in \"Delete Volume on Instance delete\", then when you delete the instance, the volume is also removed. This is not desired while the data of its attached volumes need to persist even instance is deleted. But this incures the Volumes quotas so ideally you can select \"Yes\" only for those instances you will not be storing persistent data. More details about available bootable images can be found here . Customers can also upload their own custom images, as demonstrated in this documentation . Flavor tab: Specify the size of the instance to launch. Choose cpu-a.4 from the 'Flavor' tab by clicking on the \"+\" icon. Important Note In NERC OpenStack, flavors define the compute, memory, and storage capacity of nova computing instances. In other words, a flavor is an available hardware configuration for a server. Some of the flavors will not be available for your use as per your resource Quota limits and will be shown as below: More details about available flavors and corresponding quotas details can be found here . After choosing cpu-a.4 , you should see it moved up to \"Allocated\". Networks: tab: Make sure the Default Network that is created by default is moved up to \"Allocated\". If not, you can click on the \"+\" icon in \"Available\". Security Groups: tab: Make sure to add the security group where you enabled SSH. Key Pair: Add the key pair you created for your local machine/laptop to use with this VM. Important Note If you did not provide a key pair, security groups, or rules, users can access the instance only from inside the cloud through VNC. Even pinging the instance is not possible without an ICMP rule configured. Network Ports, Configuration, Server Groups, Schedular Hints, and Metadata: tab: Ignore these tabs for now. You are now ready to launch your VM - go ahead and click \"Launch Instance\"! This will initiate a instance on a compute node in the cloud. On a successful launch you would be redirected to Compute -> Instances tab and can see the VM spawning. Once your VM is successfully running you will see the Power State changes from \"No State\" to \"running\". Note You can also launch an instance from the \"Instances\" or \"Volumes\" category when you launch an instance from an instance or a volume respectively.","title":"Launch a VM"},{"location":"openstack/create-and-connect-to-the-VM/launch-a-VM/#launch-a-vm","text":"","title":"Launch a VM"},{"location":"openstack/create-and-connect-to-the-VM/launch-a-VM/#background","text":"To start a VM, we will need a base image. NERC has made several Public images available to users.","title":"Background"},{"location":"openstack/create-and-connect-to-the-VM/launch-a-VM/#launch-an-instance","text":"Navigate: Project -> Compute -> Images. Click Launch Instance next to the public image of your choice. In the example, we chose ubuntu-22.04-x86_64 , you may choose any available images. *Important: There are multiple tabs along the top of the the pop up window. Make sure you review all of them as per instructions before clicking on Launch! Otherwise, your launched VM may be inaccessible.* In the Launch Instance dialog box, specify the following values: Details tab Instance Name: Give your instance a name that assign a name to the virtual machine. Important Note The instance name you assign here becomes the initial host name of the server. If the name is longer than 63 characters, the Compute service truncates it automatically to ensure dnsmasq works correctly. Availability Zone: By default, this value is set to the availability zone given by the cloud provider i.e. nova . Count: To launch multiple instances, enter a value greater than 1. The default is 1. Source tab: Double check that in the dropdown \"Select Boot Source,\" \"Image\" is selected. Important Note To create an image that uses the boot volume sized according to the flavor ensure that \"No\" is selected under the \"Create New Volume\" section. When you deploy a non-ephemeral instance (i.e. Creating a new volume), and indicate \"Yes\" in \"Delete Volume on Instance delete\", then when you delete the instance, the volume is also removed. This is not desired while the data of its attached volumes need to persist even instance is deleted. But this incures the Volumes quotas so ideally you can select \"Yes\" only for those instances you will not be storing persistent data. More details about available bootable images can be found here . Customers can also upload their own custom images, as demonstrated in this documentation . Flavor tab: Specify the size of the instance to launch. Choose cpu-a.4 from the 'Flavor' tab by clicking on the \"+\" icon. Important Note In NERC OpenStack, flavors define the compute, memory, and storage capacity of nova computing instances. In other words, a flavor is an available hardware configuration for a server. Some of the flavors will not be available for your use as per your resource Quota limits and will be shown as below: More details about available flavors and corresponding quotas details can be found here . After choosing cpu-a.4 , you should see it moved up to \"Allocated\". Networks: tab: Make sure the Default Network that is created by default is moved up to \"Allocated\". If not, you can click on the \"+\" icon in \"Available\". Security Groups: tab: Make sure to add the security group where you enabled SSH. Key Pair: Add the key pair you created for your local machine/laptop to use with this VM. Important Note If you did not provide a key pair, security groups, or rules, users can access the instance only from inside the cloud through VNC. Even pinging the instance is not possible without an ICMP rule configured. Network Ports, Configuration, Server Groups, Schedular Hints, and Metadata: tab: Ignore these tabs for now. You are now ready to launch your VM - go ahead and click \"Launch Instance\"! This will initiate a instance on a compute node in the cloud. On a successful launch you would be redirected to Compute -> Instances tab and can see the VM spawning. Once your VM is successfully running you will see the Power State changes from \"No State\" to \"running\". Note You can also launch an instance from the \"Instances\" or \"Volumes\" category when you launch an instance from an instance or a volume respectively.","title":"Launch an Instance"},{"location":"openstack/create-and-connect-to-the-VM/ssh-to-cloud-VM/","text":"SSH to Cloud VM Shell , or SSH , is used to administering and managing Linux workloads. Before trying to access instances from the outside world, you need to make sure you have followed these steps: You followed the instruction in Create a Key Pair to set up a public ssh key. Your public ssh-key was selected (in the Access and Security tab) while launching the instance . Assign a Floating IP to the instance in order to access it from outside world. Make sure you have added rules in the Security Groups to allow ssh to the instance. Make a note of the floating IP you have associated to your instance. In our example, the IP is 199.94.60.66 . Default usernames for all the base images are: all Ubuntu images : ubuntu all CentOS images : centos all Rocky Linux images : centos all Fedora images : fedora all Debian images : debian all RHEL images : cloud-user Our example VM was launched with the ubuntu-22.04-x86_64 base image, the user we need is 'ubuntu'. Open a Terminal window and type: ssh ubuntu@199.94.60.66 Since you have never connected to this VM before, you will be asked if you are sure you want to connect. Type yes . Note If you haven't added your key to ssh-agent, you may need to specify the private key file, like this: ssh -i ~/.ssh/cloud.key ubuntu@199.94.60.66 Setting a password When the VMs are launched, a strong, randomly-generated password is created for the default user, and then discarded. Once you connect to your VM, you will want to set a password in case you ever need to log in via the console in the web dashboard. For example, if your network connections aren't working right. Since you are not using it to log in over SSH or to sudo, it doesn't really matter how hard it is to type, and we recommend using a randomly-generated password. Create a random password like this: ubuntu@test-vm:~$ cat /dev/urandom | base64 | dd count=14 bs=1 T1W16HCyfZf8V514+0 records in 14+0 records out 14 bytes copied, 0.00110367 s, 12.7 kB/s The 'count' parameter controls the number of characters. The first [count] characters of the output are your randomly generated output, followed immediately by \"[count]+0\", so in the above example the password is: T1W16HCyfZf8V5 . Set the password for ubuntu using the command: ubuntu@test-vm:~$ sudo passwd ubuntu New password: Retype new password: ... password updated successfully Store the password in a secure place. Don't send it over email, post it on your wall on a sticky note, etc. Adding other people's SSH keys to the instance You were able to log into using your own SSH key. Right now Openstack only permits one key to be added at launch, so you need to add your teammates keys manually. Get your teammates' public keys. If they used ssh-keygen to create their key, this will be in a file called .pub on their machine. If they created a key via the dashboard, or imported the key created with ssh-keygen, their public key is viewable from the Key Pairs tab. Click on the key pair name. The public key starts with 'ssh-rsa' and looks something like this: ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQDL6O5qNZHfgFwf4vnnib2XBub7ZU6khy6z6JQl3XRJg6I6gZ +Ss6tNjz0Xgax5My0bizORcka/TJ33S36XZfzUKGsZqyEl/ax1Xnl3MfE/rgq415wKljg4 +QvDznF0OFqXjDIgL938N8G4mq/ cKKtRSMdksAvNsAreO0W7GZi24G1giap4yuG4XghAXcYxDnOSzpyP2HgqgjsPdQue919IYvgH8shr +sPa48uC5sGU5PkTb0Pk/ef1Y5pLBQZYchyMakQvxjj7hHZaT/ Lw0wIvGpPQay84plkjR2IDNb51tiEy5x163YDtrrP7RM2LJwXm+1vI8MzYmFRrXiqUyznd test_user@demo Create a file called something like 'teammates.txt' and paste in your team's public keys, one per line. Hang onto this file to save yourself from having to do all the copy/pasting every time you launch a new VM. Copy the file to the vm: [you@your-laptop ~]$ scp teammates.txt ubuntu@199.94.60.66:~ If the copy works, you will see the output: teammates.txt 100% 0 0KB/s 00:00 Append the file's contents to authorized_keys: [cloud-user@test-vm ~] #cat teammates.txt >> ~/.ssh/authorized_keys Now your teammates should also be able to log in. Important Note Make sure to use >> instead of > to avoid overwriting your own key. Adding users to the instance You may decide that each teammate should have their own user on the VM instead of everyone logging in to the default user. Once you log into the VM, you can create another user like this. Note The 'sudo_group' is different for different OS - in CentOS and Red Hat, the group is called 'wheel', while in Ubuntu, the group is called 'sudo'. $ sudo su # useradd -m <username> # passwd <username> # usermod -aG <sudo_group> <username> <-- skip this step for users who # should not have root access # su username $ cd ~ $ mkdir .ssh $ chmod 700 .ssh $ cd .ssh $ vi authorized_keys <-- paste the public key for that user in this file $ chmod 600 authorized_keys","title":"SSH to Cloud VM"},{"location":"openstack/create-and-connect-to-the-VM/ssh-to-cloud-VM/#ssh-to-cloud-vm","text":"Shell , or SSH , is used to administering and managing Linux workloads. Before trying to access instances from the outside world, you need to make sure you have followed these steps: You followed the instruction in Create a Key Pair to set up a public ssh key. Your public ssh-key was selected (in the Access and Security tab) while launching the instance . Assign a Floating IP to the instance in order to access it from outside world. Make sure you have added rules in the Security Groups to allow ssh to the instance. Make a note of the floating IP you have associated to your instance. In our example, the IP is 199.94.60.66 . Default usernames for all the base images are: all Ubuntu images : ubuntu all CentOS images : centos all Rocky Linux images : centos all Fedora images : fedora all Debian images : debian all RHEL images : cloud-user Our example VM was launched with the ubuntu-22.04-x86_64 base image, the user we need is 'ubuntu'. Open a Terminal window and type: ssh ubuntu@199.94.60.66 Since you have never connected to this VM before, you will be asked if you are sure you want to connect. Type yes . Note If you haven't added your key to ssh-agent, you may need to specify the private key file, like this: ssh -i ~/.ssh/cloud.key ubuntu@199.94.60.66","title":"SSH to Cloud VM"},{"location":"openstack/create-and-connect-to-the-VM/ssh-to-cloud-VM/#setting-a-password","text":"When the VMs are launched, a strong, randomly-generated password is created for the default user, and then discarded. Once you connect to your VM, you will want to set a password in case you ever need to log in via the console in the web dashboard. For example, if your network connections aren't working right. Since you are not using it to log in over SSH or to sudo, it doesn't really matter how hard it is to type, and we recommend using a randomly-generated password. Create a random password like this: ubuntu@test-vm:~$ cat /dev/urandom | base64 | dd count=14 bs=1 T1W16HCyfZf8V514+0 records in 14+0 records out 14 bytes copied, 0.00110367 s, 12.7 kB/s The 'count' parameter controls the number of characters. The first [count] characters of the output are your randomly generated output, followed immediately by \"[count]+0\", so in the above example the password is: T1W16HCyfZf8V5 . Set the password for ubuntu using the command: ubuntu@test-vm:~$ sudo passwd ubuntu New password: Retype new password: ... password updated successfully Store the password in a secure place. Don't send it over email, post it on your wall on a sticky note, etc.","title":"Setting a password"},{"location":"openstack/create-and-connect-to-the-VM/ssh-to-cloud-VM/#adding-other-peoples-ssh-keys-to-the-instance","text":"You were able to log into using your own SSH key. Right now Openstack only permits one key to be added at launch, so you need to add your teammates keys manually. Get your teammates' public keys. If they used ssh-keygen to create their key, this will be in a file called .pub on their machine. If they created a key via the dashboard, or imported the key created with ssh-keygen, their public key is viewable from the Key Pairs tab. Click on the key pair name. The public key starts with 'ssh-rsa' and looks something like this: ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQDL6O5qNZHfgFwf4vnnib2XBub7ZU6khy6z6JQl3XRJg6I6gZ +Ss6tNjz0Xgax5My0bizORcka/TJ33S36XZfzUKGsZqyEl/ax1Xnl3MfE/rgq415wKljg4 +QvDznF0OFqXjDIgL938N8G4mq/ cKKtRSMdksAvNsAreO0W7GZi24G1giap4yuG4XghAXcYxDnOSzpyP2HgqgjsPdQue919IYvgH8shr +sPa48uC5sGU5PkTb0Pk/ef1Y5pLBQZYchyMakQvxjj7hHZaT/ Lw0wIvGpPQay84plkjR2IDNb51tiEy5x163YDtrrP7RM2LJwXm+1vI8MzYmFRrXiqUyznd test_user@demo Create a file called something like 'teammates.txt' and paste in your team's public keys, one per line. Hang onto this file to save yourself from having to do all the copy/pasting every time you launch a new VM. Copy the file to the vm: [you@your-laptop ~]$ scp teammates.txt ubuntu@199.94.60.66:~ If the copy works, you will see the output: teammates.txt 100% 0 0KB/s 00:00 Append the file's contents to authorized_keys: [cloud-user@test-vm ~] #cat teammates.txt >> ~/.ssh/authorized_keys Now your teammates should also be able to log in. Important Note Make sure to use >> instead of > to avoid overwriting your own key.","title":"Adding other people's SSH keys to the instance"},{"location":"openstack/create-and-connect-to-the-VM/ssh-to-cloud-VM/#adding-users-to-the-instance","text":"You may decide that each teammate should have their own user on the VM instead of everyone logging in to the default user. Once you log into the VM, you can create another user like this. Note The 'sudo_group' is different for different OS - in CentOS and Red Hat, the group is called 'wheel', while in Ubuntu, the group is called 'sudo'. $ sudo su # useradd -m <username> # passwd <username> # usermod -aG <sudo_group> <username> <-- skip this step for users who # should not have root access # su username $ cd ~ $ mkdir .ssh $ chmod 700 .ssh $ cd .ssh $ vi authorized_keys <-- paste the public key for that user in this file $ chmod 600 authorized_keys","title":"Adding users to the instance"},{"location":"openstack/create-and-connect-to-the-VM/bastion-host-based-ssh/","text":"Bastion Host A bastion host is a server that provides secure access to private networks over SSH from an external network, such as the Internet. We can leverage a bastion host to record all SSH sessions established with private network instances which enables auditing and can help us in efforts to comply with regulatory requirements. The following diagram illustrates the concept of using an SSH bastion host to provide access to Linux instances running inside OpenStack cloud network. In OpenStack, users can deploy instances in a private tenant network. In order to make these instances to be accessible externally via internet, the tenant must assign each instance a floating IP address i.e., an external public IP. Nevertheless, users may still want a way to deploy instances without having to assign a floating IP address for every instance. This is useful in the context of an OpenStack project as you don't necessarily want to reserve a floating IP for all your instances. This way you can isolate certain resources so that there is only a single point of access to them and conserve floating IP addresses so that you don't need as big of a quota. Leveraging an SSH bastion host allows this sort of configuration while still enabling SSH access to the private instances. Before trying to access instances from the outside world using SSH tunneling via Bastion Host, you need to make sure you have followed these steps: You followed the instruction in Create a Key Pair to set up a public ssh key. You can use the same key for both the bastion host and the remote instances, or different keys; you'll just need to ensure that the keys are loaded by ssh-agent appropriately so they can be used as needed. Please read this instruction on how to add ssh-agent and load your private key using ssh-add command to access the bastion host. Verify you have an SSH agent running. This should match whatever you built your cluster with. ssh-add -l If you need to add the key to your agent: ssh-add path/to/private/key Now you can SSH into the bastion host: ssh -A <user>@<bastion-floating-IP> Your public ssh-key was selected (in the Access and Security tab) while launching the instance . Add two Security Groups, one will be used by the Bastion host and another one will be used by any private instances. i. Bastion Host Security Group: Allow inbound SSH (optional ICMP) for this security group. Make sure you have added rules in the Security Groups to allow ssh to the bastion host. ii. Private Instances Security Group: You need to select \"Security Group\" in Remote dropdown option, and then select the \" Bastion Host Security Group \" under Security Group option as shown below: Assign a Floating IP to the Bastion host instance in order to access it from outside world. Make a note of the floating IP you have associated to your instance. While adding the Bastion host and private instance, please select appropriate Security Group as shown below: private1: bastion_host_demo: Finally, you'll want to configure the ProxyJump setting for the remote instances in your SSH configuration file (typically found in ~/.ssh/config ). In SSH configuration file, we can define multiple hosts by pet names, specify custom ports, hostnames, users, etc. For example, let's say that you had a remote instance named \" private1 \" and you wanted to run SSH connections through a bastion host called \" bastion \". The appropriate SSH configuration file might look something like this: Host bastion HostName 140.247.152.139 User ubuntu Host private1 Hostname 192.168.0.40 User ubuntu ProxyJump bastion ProxyJump makes it super simple to jump from one host to another totally transparently. OR, if you don't have keys loaded by ssh-add command starting ssh-agent on your local machine. you can load the private key using IdentityFile variable in SSH configuration file as shown below: Host private1 Hostname 192.168.0.40 User ubuntu IdentityFile ~/.ssh/cloud.key ProxyJump bastion Host bastion HostName 140.247.152.139 User ubuntu IdentityFile ~/.ssh/cloud.key With this configuration in place, when you type ssh private1 SSH will establish a connection to the bastion host and then through the bastion host connect to \" private1 \", using the agent added keys or specified private keys. In this sort of arrangement, SSH traffic to private servers that are not directly accessible via SSH is instead directed through a bastion host, which proxies the connection between the SSH client and the remote servers. The bastion host runs on an instance that is typically in a public subnet with attached floating public IP. Private instances are in a subnet that is not publicly accessible, and they are set up with a security group that allows SSH access from the security group attached to the underlying instance running the bastion host. The user won't see any of this; he or she will just see a shell for \" private1 \" appear. If you dig a bit further, though (try running who on the remote node), you'll see the connections are coming from the bastion host, not the original SSH client.","title":"Bastion Host based SSH"},{"location":"openstack/create-and-connect-to-the-VM/bastion-host-based-ssh/#bastion-host","text":"A bastion host is a server that provides secure access to private networks over SSH from an external network, such as the Internet. We can leverage a bastion host to record all SSH sessions established with private network instances which enables auditing and can help us in efforts to comply with regulatory requirements. The following diagram illustrates the concept of using an SSH bastion host to provide access to Linux instances running inside OpenStack cloud network. In OpenStack, users can deploy instances in a private tenant network. In order to make these instances to be accessible externally via internet, the tenant must assign each instance a floating IP address i.e., an external public IP. Nevertheless, users may still want a way to deploy instances without having to assign a floating IP address for every instance. This is useful in the context of an OpenStack project as you don't necessarily want to reserve a floating IP for all your instances. This way you can isolate certain resources so that there is only a single point of access to them and conserve floating IP addresses so that you don't need as big of a quota. Leveraging an SSH bastion host allows this sort of configuration while still enabling SSH access to the private instances. Before trying to access instances from the outside world using SSH tunneling via Bastion Host, you need to make sure you have followed these steps: You followed the instruction in Create a Key Pair to set up a public ssh key. You can use the same key for both the bastion host and the remote instances, or different keys; you'll just need to ensure that the keys are loaded by ssh-agent appropriately so they can be used as needed. Please read this instruction on how to add ssh-agent and load your private key using ssh-add command to access the bastion host. Verify you have an SSH agent running. This should match whatever you built your cluster with. ssh-add -l If you need to add the key to your agent: ssh-add path/to/private/key Now you can SSH into the bastion host: ssh -A <user>@<bastion-floating-IP> Your public ssh-key was selected (in the Access and Security tab) while launching the instance . Add two Security Groups, one will be used by the Bastion host and another one will be used by any private instances. i. Bastion Host Security Group: Allow inbound SSH (optional ICMP) for this security group. Make sure you have added rules in the Security Groups to allow ssh to the bastion host. ii. Private Instances Security Group: You need to select \"Security Group\" in Remote dropdown option, and then select the \" Bastion Host Security Group \" under Security Group option as shown below: Assign a Floating IP to the Bastion host instance in order to access it from outside world. Make a note of the floating IP you have associated to your instance. While adding the Bastion host and private instance, please select appropriate Security Group as shown below: private1: bastion_host_demo: Finally, you'll want to configure the ProxyJump setting for the remote instances in your SSH configuration file (typically found in ~/.ssh/config ). In SSH configuration file, we can define multiple hosts by pet names, specify custom ports, hostnames, users, etc. For example, let's say that you had a remote instance named \" private1 \" and you wanted to run SSH connections through a bastion host called \" bastion \". The appropriate SSH configuration file might look something like this: Host bastion HostName 140.247.152.139 User ubuntu Host private1 Hostname 192.168.0.40 User ubuntu ProxyJump bastion ProxyJump makes it super simple to jump from one host to another totally transparently. OR, if you don't have keys loaded by ssh-add command starting ssh-agent on your local machine. you can load the private key using IdentityFile variable in SSH configuration file as shown below: Host private1 Hostname 192.168.0.40 User ubuntu IdentityFile ~/.ssh/cloud.key ProxyJump bastion Host bastion HostName 140.247.152.139 User ubuntu IdentityFile ~/.ssh/cloud.key With this configuration in place, when you type ssh private1 SSH will establish a connection to the bastion host and then through the bastion host connect to \" private1 \", using the agent added keys or specified private keys. In this sort of arrangement, SSH traffic to private servers that are not directly accessible via SSH is instead directed through a bastion host, which proxies the connection between the SSH client and the remote servers. The bastion host runs on an instance that is typically in a public subnet with attached floating public IP. Private instances are in a subnet that is not publicly accessible, and they are set up with a security group that allows SSH access from the security group attached to the underlying instance running the bastion host. The user won't see any of this; he or she will just see a shell for \" private1 \" appear. If you dig a bit further, though (try running who on the remote node), you'll see the connections are coming from the bastion host, not the original SSH client.","title":"Bastion Host"},{"location":"openstack/create-and-connect-to-the-VM/openvpn/","text":"OpenVPN OpenVPN is a full-featured SSL VPN which implements OSI layer 2 or 3 secure network extension using the industry standard SSL/TLS protocol, supports flexible client authentication methods based on certificates, smart cards, and/ or username/password credentials, and allows user or group-specific access control policies using firewall rules applied to the VPN virtual interface. OpenVPN offers a scalable client/server mode, allowing multiple clients to connect to a single OpenVPN server process over a single TCP or UDP port. Installing OpenVPN Server You can read official documentation here . You can spin up a new instance with \" ubuntu-22.04-x86_64 \" or any available Ubuntu OS image, named \" openvpn_server \" on OpenStack, with \" default \" and \" ssh_only \" Security Groups attached to it. Also, attach a Floating IP to this instance so you can ssh into it from outside. Finally, you'll want to configure the setting for the remote instances in your SSH configuration file (typically found in ~/.ssh/config ). The SSH configuration file might include entry for your newly create OpenVPN server like this: Host openvpn HostName 199.94.60.66 User ubuntu IdentityFile ~/.ssh/cloud.key Then you can ssh into the OpenVPN Server running: ssh openvpn Also note that OpenVPN must be installed and run by a user who has administrative/root privileges. So, we need to run the command: sudo su We are using this repo to install OpenVPN server on this ubuntu server. For that, run the script and follow the assistant: sh wget https://git.io/vpn -O openvpn-install.sh && bash openvpn-install.sh You can press Enter for all default values. And, while entering a name for the first client you can give \" nerc \" as the client name, this will generate a new configuration file (.ovpn file) named as \" nerc.ovpn \". Based on your client's name it will name the config file as \" .ovpn \" Copy the generated config file from \" /root/nerc.ovpn \" to \" /home/ubuntu/ nerc.ovpn \" by running: cp /root/nerc.ovpn . Update the ownership of the config file to ubuntu user and ubuntu group by running the following command: chown ubuntu:ubuntu nerc.ovpn You can exit from the root and ssh session all together and then copy the configuration file to your local machine by running the following script on your local machine's terminal: scp openvpn:nerc.ovpn . To add a new client user Once it ends, you can run it again to add more users, remove some of them or even completely uninstall OpenVPN. For this, run the script and follow the assistant: wget https://git.io/vpn -O openvpn-install.sh && bash openvpn-install.sh Here, you are giving client name as \" mac_client \" and that will generate a new configuration file at \" /root/mac_client.ovpn \". You can repeat above steps: 4 to 6 to copy this new client's configuration file and share it to the new client. Important Note You need to contact your project administrator to get your own OpenVPN configuration file (file with .ovpn extension). Download it and Keep it in your local machine so in next steps we can use this configuration client profile file. A OpenVPN client or compatible software is needed to connect to the OpenVPN server. Please install one of these clients depending on your device. The client program must be configured with a client profile to connect to the OpenVPN server. Windows OpenVPN source code and Windows installers can be downloaded here . The OpenVPN executable should be installed on both server and client machines since the single executable provides both client and server functions. Please see the OpenVPN client setup guide for Windows . Mac OS X The client we recommend and support for Mac OS is Tunnelblick . To install Tunnelblick, download the dmg installer file from the Tunnelblick site , mount the dmg, and drag the Tunnelblick application to Applications. Please refer to this guide for more information. Linux OpenVPN is available through the package management system on most Linux distributions. On Debian/Ubuntu: sudo apt-get install openvpn On RedHat/CentOS: sudo yum install openvpn Then, to run OpenVPN using the client profile: Move the VPN client profile (configuration) file to /etc/openvpn/ : sudo mv nerc.ovpn /etc/openvpn/client.conf Restart the OpenVPN daemon (i.e., This will start OpenVPN connection and will automatically run on boot): sudo /etc/init.d/openvpn start OR, sudo systemctl enable --now openvpn@client sudo systemctl start openvpn@client Checking the status: systemctl status openvpn@client Alternatively, if you want to run OpenVPN manually each time, then run: sudo openvpn --config /etc/openvpn/client.ovpn OR, sudo openvpn --config nerc.ovpn","title":"About OpenVPN"},{"location":"openstack/create-and-connect-to-the-VM/openvpn/#openvpn","text":"OpenVPN is a full-featured SSL VPN which implements OSI layer 2 or 3 secure network extension using the industry standard SSL/TLS protocol, supports flexible client authentication methods based on certificates, smart cards, and/ or username/password credentials, and allows user or group-specific access control policies using firewall rules applied to the VPN virtual interface. OpenVPN offers a scalable client/server mode, allowing multiple clients to connect to a single OpenVPN server process over a single TCP or UDP port.","title":"OpenVPN"},{"location":"openstack/create-and-connect-to-the-VM/openvpn/#installing-openvpn-server","text":"You can read official documentation here . You can spin up a new instance with \" ubuntu-22.04-x86_64 \" or any available Ubuntu OS image, named \" openvpn_server \" on OpenStack, with \" default \" and \" ssh_only \" Security Groups attached to it. Also, attach a Floating IP to this instance so you can ssh into it from outside. Finally, you'll want to configure the setting for the remote instances in your SSH configuration file (typically found in ~/.ssh/config ). The SSH configuration file might include entry for your newly create OpenVPN server like this: Host openvpn HostName 199.94.60.66 User ubuntu IdentityFile ~/.ssh/cloud.key Then you can ssh into the OpenVPN Server running: ssh openvpn Also note that OpenVPN must be installed and run by a user who has administrative/root privileges. So, we need to run the command: sudo su We are using this repo to install OpenVPN server on this ubuntu server. For that, run the script and follow the assistant: sh wget https://git.io/vpn -O openvpn-install.sh && bash openvpn-install.sh You can press Enter for all default values. And, while entering a name for the first client you can give \" nerc \" as the client name, this will generate a new configuration file (.ovpn file) named as \" nerc.ovpn \". Based on your client's name it will name the config file as \" .ovpn \" Copy the generated config file from \" /root/nerc.ovpn \" to \" /home/ubuntu/ nerc.ovpn \" by running: cp /root/nerc.ovpn . Update the ownership of the config file to ubuntu user and ubuntu group by running the following command: chown ubuntu:ubuntu nerc.ovpn You can exit from the root and ssh session all together and then copy the configuration file to your local machine by running the following script on your local machine's terminal: scp openvpn:nerc.ovpn .","title":"Installing OpenVPN Server"},{"location":"openstack/create-and-connect-to-the-VM/openvpn/#to-add-a-new-client-user","text":"Once it ends, you can run it again to add more users, remove some of them or even completely uninstall OpenVPN. For this, run the script and follow the assistant: wget https://git.io/vpn -O openvpn-install.sh && bash openvpn-install.sh Here, you are giving client name as \" mac_client \" and that will generate a new configuration file at \" /root/mac_client.ovpn \". You can repeat above steps: 4 to 6 to copy this new client's configuration file and share it to the new client. Important Note You need to contact your project administrator to get your own OpenVPN configuration file (file with .ovpn extension). Download it and Keep it in your local machine so in next steps we can use this configuration client profile file. A OpenVPN client or compatible software is needed to connect to the OpenVPN server. Please install one of these clients depending on your device. The client program must be configured with a client profile to connect to the OpenVPN server.","title":"To add a new client user"},{"location":"openstack/create-and-connect-to-the-VM/openvpn/#windows","text":"OpenVPN source code and Windows installers can be downloaded here . The OpenVPN executable should be installed on both server and client machines since the single executable provides both client and server functions. Please see the OpenVPN client setup guide for Windows .","title":"Windows"},{"location":"openstack/create-and-connect-to-the-VM/openvpn/#mac-os-x","text":"The client we recommend and support for Mac OS is Tunnelblick . To install Tunnelblick, download the dmg installer file from the Tunnelblick site , mount the dmg, and drag the Tunnelblick application to Applications. Please refer to this guide for more information.","title":"Mac OS X"},{"location":"openstack/create-and-connect-to-the-VM/openvpn/#linux","text":"OpenVPN is available through the package management system on most Linux distributions. On Debian/Ubuntu: sudo apt-get install openvpn On RedHat/CentOS: sudo yum install openvpn Then, to run OpenVPN using the client profile: Move the VPN client profile (configuration) file to /etc/openvpn/ : sudo mv nerc.ovpn /etc/openvpn/client.conf Restart the OpenVPN daemon (i.e., This will start OpenVPN connection and will automatically run on boot): sudo /etc/init.d/openvpn start OR, sudo systemctl enable --now openvpn@client sudo systemctl start openvpn@client Checking the status: systemctl status openvpn@client Alternatively, if you want to run OpenVPN manually each time, then run: sudo openvpn --config /etc/openvpn/client.ovpn OR, sudo openvpn --config nerc.ovpn","title":"Linux"},{"location":"openstack/create-and-connect-to-the-VM/openvpn/openvpn_gui_for_windows/","text":"OpenVPN-GUI Official OpenVPN Windows installers include a Windows OpenVPN-GUI , which allows managing OpenVPN connections from a system tray applet. Find your client account credentials You need to contact your project administrator to get your own OpenVPN configuration file (file with .ovpn extension). Download it and Keep it in your local machine so in next steps we can use this configuration client profile file. Download and install OpenVPN-GUI Download the OpenVPN client installer: OpenVPN for Windows can be installed from the self-installing exe file on the OpenVPN download page . Also note that OpenVPN must be installed and run by a user who has administrative privileges (this restriction is imposed by Windows, not OpenVPN) Launch the installer and follow the prompts as directed. Clicking \" Customize \" button we can see settings and features of OpenVPN GUI client. Click \" Install Now \" to continue. Click \"Close\"button. For the newly installed OpenVPN GUI there will be no configuration profile for the client so it will show a pop up that alerts: Set up the VPN with OpenVPN GUI After you've run the Windows installer, OpenVPN is ready for use and will associate itself with files having the .ovpn extension. You can use the previously downloaded .ovpn file from your Downloads folder to setup the connection profiles. a. Either you can Right click on the OpenVPN configuration file (.ovpn) and select \" Start OpenVPN on this config file \": b. OR, you can use \"Import file\u2026\" menu to select the previously downloaded .ovpn file. Once, done it will show: c. OR, you can manually copy the config file to one of OpenVPN's configuration directories: C:\\Program Files\\OpenVPN\\config (global configs) C:\\Program Files\\OpenVPN\\config-auto (autostarted global configs) %USERPROFILE%\\OpenVPN\\config (per-user configs) Connect to a VPN server location For launching OpenVPN Connections you click on OpenVPN GUI (tray applet). OpenVPN GUI is used to launching VPN connections on demand. OpenVPN GUI is a system-tray applet, so an icon for the GUI will appear in the lower-right corner of the screen located at the taskbar notification area. Right click on the system tray icon, and if you have multiple configurations then a menu should appear showing the names of your OpenVPN configuration profiles and giving you the option to connect. If you have only one configuration then you can just click on \"Connect\" menu. When you are connected to OpenVPN server successfully, you will see popup message as shown below. That's it! You are now connected to a VPN. Once you are connected to the OpenVPN server, you can run commands like shown below in your terminal to connect to the private instances: ssh ubuntu@192.168. 0.40 -A -i cloud.key Disconnect VPN server To disconnect, right click on the system tray icon, in your status bar and select Disconnect from the menu.","title":"OpenVPN GUI for Windows"},{"location":"openstack/create-and-connect-to-the-VM/openvpn/openvpn_gui_for_windows/#openvpn-gui","text":"Official OpenVPN Windows installers include a Windows OpenVPN-GUI , which allows managing OpenVPN connections from a system tray applet.","title":"OpenVPN-GUI"},{"location":"openstack/create-and-connect-to-the-VM/openvpn/openvpn_gui_for_windows/#find-your-client-account-credentials","text":"You need to contact your project administrator to get your own OpenVPN configuration file (file with .ovpn extension). Download it and Keep it in your local machine so in next steps we can use this configuration client profile file.","title":"Find your client account credentials"},{"location":"openstack/create-and-connect-to-the-VM/openvpn/openvpn_gui_for_windows/#download-and-install-openvpn-gui","text":"Download the OpenVPN client installer: OpenVPN for Windows can be installed from the self-installing exe file on the OpenVPN download page . Also note that OpenVPN must be installed and run by a user who has administrative privileges (this restriction is imposed by Windows, not OpenVPN) Launch the installer and follow the prompts as directed. Clicking \" Customize \" button we can see settings and features of OpenVPN GUI client. Click \" Install Now \" to continue. Click \"Close\"button. For the newly installed OpenVPN GUI there will be no configuration profile for the client so it will show a pop up that alerts:","title":"Download and install OpenVPN-GUI"},{"location":"openstack/create-and-connect-to-the-VM/openvpn/openvpn_gui_for_windows/#set-up-the-vpn-with-openvpn-gui","text":"After you've run the Windows installer, OpenVPN is ready for use and will associate itself with files having the .ovpn extension. You can use the previously downloaded .ovpn file from your Downloads folder to setup the connection profiles. a. Either you can Right click on the OpenVPN configuration file (.ovpn) and select \" Start OpenVPN on this config file \": b. OR, you can use \"Import file\u2026\" menu to select the previously downloaded .ovpn file. Once, done it will show: c. OR, you can manually copy the config file to one of OpenVPN's configuration directories: C:\\Program Files\\OpenVPN\\config (global configs) C:\\Program Files\\OpenVPN\\config-auto (autostarted global configs) %USERPROFILE%\\OpenVPN\\config (per-user configs)","title":"Set up the VPN with OpenVPN GUI"},{"location":"openstack/create-and-connect-to-the-VM/openvpn/openvpn_gui_for_windows/#connect-to-a-vpn-server-location","text":"For launching OpenVPN Connections you click on OpenVPN GUI (tray applet). OpenVPN GUI is used to launching VPN connections on demand. OpenVPN GUI is a system-tray applet, so an icon for the GUI will appear in the lower-right corner of the screen located at the taskbar notification area. Right click on the system tray icon, and if you have multiple configurations then a menu should appear showing the names of your OpenVPN configuration profiles and giving you the option to connect. If you have only one configuration then you can just click on \"Connect\" menu. When you are connected to OpenVPN server successfully, you will see popup message as shown below. That's it! You are now connected to a VPN. Once you are connected to the OpenVPN server, you can run commands like shown below in your terminal to connect to the private instances: ssh ubuntu@192.168. 0.40 -A -i cloud.key","title":"Connect to a VPN server location"},{"location":"openstack/create-and-connect-to-the-VM/openvpn/openvpn_gui_for_windows/#disconnect-vpn-server","text":"To disconnect, right click on the system tray icon, in your status bar and select Disconnect from the menu.","title":"Disconnect VPN server"},{"location":"openstack/create-and-connect-to-the-VM/openvpn/tunnelblick_for_macos/","text":"Tunnelblick Tunnelblick is a free, open-source GUI (graphical user interface) for OpenVPN on macOS and OS X: More details can be found here . Access to a VPN server \u2014 your computer is one end of the tunnel and the VPN server is the other end. Find your client account credentials You need to contact your project administrator to get your own OpenVPN configuration file (file with .ovpn extension). Download it and Keep it in your local machine so in next steps we can use this configuration client profile file. Download and install Tunnelblick Download Tunnelblick , a free and user-friendly app for managing OpenVPN connections on macOS. Navigate to your Downloads folder and double-click the Tunnelblick installation file (.dmg installer file) you have just downloaded. In the window that opens, double-click on the Tunnelblick icon. A new dialogue box will pop up, asking you if you are sure you want to open the app. Click Open . You will be asked to enter your device password. Enter it and click OK : Select Allow or Don't Allow for your notification preference. Once the installation is complete, you will see a pop-up notification asking you if you want to launch Tunnelblick now. (An administrator username and password will be required to secure Tunnelblick). Click Launch . Alternatively, you can click on the Tunnelblick icon in the status bar and select VPN Details... : Set up the VPN with Tunnelblick A new dialogue box will appear. Click I have configuration files . Another notification will pop-up, instructing you how to import configuration files. Click OK . Drag and drop the previously downloaded .ovpn file from your Downloads folder to the Configurations tab in Tunnelblick. OR, You can just drag and drop the provided OpenVPN configuration file (file with .ovpn extension) directly to Tunnelblick icon in status bar at the top-right corner of your screen. A pop-up will appear, asking you if you want to install the configuration profile for your current user only or for all users on your Mac. Select your preferred option. If the VPN is intended for all accounts on your Mac, select All Users . If the VPN will only be used by your current account, select Only Me . You will be asked to enter your Mac password. Then the screen reads \" Tunnelblick successfully: installed one configuration \". You can see the configuration setting is loaded and installed successfully. Connect to a VPN server location To connect to a VPN server location, click the Tunnelblick icon in status bar at the top-right corner of your screen. From the drop down menu select the server and click Connect [name of the .ovpn configuration file] .. Alternatively, you can select \" VPN Details \" from the menu and then click the \" Connect \"button: This will show the connection log on the dialog: When you are connected to OpenVPN server successfully, you will see popup message as shown below. That's it! You are now connected to a VPN. Once you are connected to the OpenVPN server, you can run commands like shown below to connect to the private instances: sh ssh ubuntu@192.168.0.40 -A -i cloud.key Disconnect VPN server To disconnect, click on the Tunnelblick icon in your status bar and select Disconnect in the drop-down menu. While closing the log will be shown on popup as shown below:","title":"Tunnelblick for MacOS"},{"location":"openstack/create-and-connect-to-the-VM/openvpn/tunnelblick_for_macos/#tunnelblick","text":"Tunnelblick is a free, open-source GUI (graphical user interface) for OpenVPN on macOS and OS X: More details can be found here . Access to a VPN server \u2014 your computer is one end of the tunnel and the VPN server is the other end.","title":"Tunnelblick"},{"location":"openstack/create-and-connect-to-the-VM/openvpn/tunnelblick_for_macos/#find-your-client-account-credentials","text":"You need to contact your project administrator to get your own OpenVPN configuration file (file with .ovpn extension). Download it and Keep it in your local machine so in next steps we can use this configuration client profile file.","title":"Find your client account credentials"},{"location":"openstack/create-and-connect-to-the-VM/openvpn/tunnelblick_for_macos/#download-and-install-tunnelblick","text":"Download Tunnelblick , a free and user-friendly app for managing OpenVPN connections on macOS. Navigate to your Downloads folder and double-click the Tunnelblick installation file (.dmg installer file) you have just downloaded. In the window that opens, double-click on the Tunnelblick icon. A new dialogue box will pop up, asking you if you are sure you want to open the app. Click Open . You will be asked to enter your device password. Enter it and click OK : Select Allow or Don't Allow for your notification preference. Once the installation is complete, you will see a pop-up notification asking you if you want to launch Tunnelblick now. (An administrator username and password will be required to secure Tunnelblick). Click Launch . Alternatively, you can click on the Tunnelblick icon in the status bar and select VPN Details... :","title":"Download and install Tunnelblick"},{"location":"openstack/create-and-connect-to-the-VM/openvpn/tunnelblick_for_macos/#set-up-the-vpn-with-tunnelblick","text":"A new dialogue box will appear. Click I have configuration files . Another notification will pop-up, instructing you how to import configuration files. Click OK . Drag and drop the previously downloaded .ovpn file from your Downloads folder to the Configurations tab in Tunnelblick. OR, You can just drag and drop the provided OpenVPN configuration file (file with .ovpn extension) directly to Tunnelblick icon in status bar at the top-right corner of your screen. A pop-up will appear, asking you if you want to install the configuration profile for your current user only or for all users on your Mac. Select your preferred option. If the VPN is intended for all accounts on your Mac, select All Users . If the VPN will only be used by your current account, select Only Me . You will be asked to enter your Mac password. Then the screen reads \" Tunnelblick successfully: installed one configuration \". You can see the configuration setting is loaded and installed successfully.","title":"Set up the VPN with Tunnelblick"},{"location":"openstack/create-and-connect-to-the-VM/openvpn/tunnelblick_for_macos/#connect-to-a-vpn-server-location","text":"To connect to a VPN server location, click the Tunnelblick icon in status bar at the top-right corner of your screen. From the drop down menu select the server and click Connect [name of the .ovpn configuration file] .. Alternatively, you can select \" VPN Details \" from the menu and then click the \" Connect \"button: This will show the connection log on the dialog: When you are connected to OpenVPN server successfully, you will see popup message as shown below. That's it! You are now connected to a VPN. Once you are connected to the OpenVPN server, you can run commands like shown below to connect to the private instances: sh ssh ubuntu@192.168.0.40 -A -i cloud.key","title":"Connect to a VPN server location"},{"location":"openstack/create-and-connect-to-the-VM/openvpn/tunnelblick_for_macos/#disconnect-vpn-server","text":"To disconnect, click on the Tunnelblick icon in your status bar and select Disconnect in the drop-down menu. While closing the log will be shown on popup as shown below:","title":"Disconnect VPN server"},{"location":"openstack/create-and-connect-to-the-VM/sshuttle/","text":"sshuttle sshuttle is a lightweight SSH-encrypted VPN. This is a Python based script that allows you to tunnel connections through SSH in a far more efficient way then traditional ssh proxying. Installing sshuttle Server You can spin up a new instance with \" ubuntu-22.04-x86_64 \" or any available Ubuntu OS image, named \" sshuttle_server \" on OpenStack, with \" default \" and \" ssh_only \" Security Groups attached to it. Also, attach a Floating IP to this instance so you can ssh into it from outside. Finally, you'll want to configure the setting for the remote instances in your SSH configuration file (typically found in ~/.ssh/config ). The SSH configuration file might include entry for your newly create sshuttle server like this: Host sshuttle HostName 140.247.152.244 User ubuntu IdentityFile ~/.ssh/cloud.key Then you can ssh into the sshuttle Server running: ssh sshuttle Note Unlike other VPN servers, for sshuttle you don't need to install anything on the server side. As long as you have an SSH server (with python3 installed) you're good to go. To connect from a new client Install sshuttle Windows Currently there is no built in support for running sshuttle directly on Microsoft Windows. What you can do is to create a Linux VM with Vagrant (or simply Virtualbox if you like) and then try to connect via that VM. For more details read here Mac OS X Install using Homebrew : brew install sshuttle OR, via MacPorts sudo port selfupdate sudo port install sshuttle Linux sshuttle is available through the package management system on most Linux distributions. On Debian/Ubuntu: sudo apt-get install sshuttle On RedHat/CentOS: sudo yum install sshuttle It is also possible to install into a virtualenv as a non-root user . From PyPI: virtualenv -p python3 /tmp/sshuttle . /tmp/sshuttle/bin/activate pip install sshuttle Clone: virtualenv -p python3 /tmp/sshuttle . /tmp/sshuttle/bin/activate git clone [https://github.com/sshuttle/sshuttle.git](https://github.com/sshuttle/sshuttle.git) cd sshuttle ./setup.py install How to Connect Tunnel to all networks (0.0.0.0/0): sshuttle -r ubuntu @140.247.152.244 0.0.0.0/0 OR, shorthand: sudo sshuttle -r ubuntu@140.247.152.244 0/0 If you would also like your DNS queries to be proxied through the DNS server of the server, you are connected to: sshuttle --dns -r ubuntu@140.247.152.244 0/0","title":"sshuttle"},{"location":"openstack/create-and-connect-to-the-VM/sshuttle/#sshuttle","text":"sshuttle is a lightweight SSH-encrypted VPN. This is a Python based script that allows you to tunnel connections through SSH in a far more efficient way then traditional ssh proxying.","title":"sshuttle"},{"location":"openstack/create-and-connect-to-the-VM/sshuttle/#installing-sshuttle-server","text":"You can spin up a new instance with \" ubuntu-22.04-x86_64 \" or any available Ubuntu OS image, named \" sshuttle_server \" on OpenStack, with \" default \" and \" ssh_only \" Security Groups attached to it. Also, attach a Floating IP to this instance so you can ssh into it from outside. Finally, you'll want to configure the setting for the remote instances in your SSH configuration file (typically found in ~/.ssh/config ). The SSH configuration file might include entry for your newly create sshuttle server like this: Host sshuttle HostName 140.247.152.244 User ubuntu IdentityFile ~/.ssh/cloud.key Then you can ssh into the sshuttle Server running: ssh sshuttle Note Unlike other VPN servers, for sshuttle you don't need to install anything on the server side. As long as you have an SSH server (with python3 installed) you're good to go.","title":"Installing sshuttle Server"},{"location":"openstack/create-and-connect-to-the-VM/sshuttle/#to-connect-from-a-new-client","text":"","title":"To connect from a new client"},{"location":"openstack/create-and-connect-to-the-VM/sshuttle/#install-sshuttle","text":"","title":"Install sshuttle"},{"location":"openstack/create-and-connect-to-the-VM/sshuttle/#windows","text":"Currently there is no built in support for running sshuttle directly on Microsoft Windows. What you can do is to create a Linux VM with Vagrant (or simply Virtualbox if you like) and then try to connect via that VM. For more details read here","title":"Windows"},{"location":"openstack/create-and-connect-to-the-VM/sshuttle/#mac-os-x","text":"Install using Homebrew : brew install sshuttle OR, via MacPorts sudo port selfupdate sudo port install sshuttle","title":"Mac OS X"},{"location":"openstack/create-and-connect-to-the-VM/sshuttle/#linux","text":"sshuttle is available through the package management system on most Linux distributions. On Debian/Ubuntu: sudo apt-get install sshuttle On RedHat/CentOS: sudo yum install sshuttle It is also possible to install into a virtualenv as a non-root user . From PyPI: virtualenv -p python3 /tmp/sshuttle . /tmp/sshuttle/bin/activate pip install sshuttle Clone: virtualenv -p python3 /tmp/sshuttle . /tmp/sshuttle/bin/activate git clone [https://github.com/sshuttle/sshuttle.git](https://github.com/sshuttle/sshuttle.git) cd sshuttle ./setup.py install","title":"Linux"},{"location":"openstack/create-and-connect-to-the-VM/sshuttle/#how-to-connect","text":"Tunnel to all networks (0.0.0.0/0): sshuttle -r ubuntu @140.247.152.244 0.0.0.0/0 OR, shorthand: sudo sshuttle -r ubuntu@140.247.152.244 0/0 If you would also like your DNS queries to be proxied through the DNS server of the server, you are connected to: sshuttle --dns -r ubuntu@140.247.152.244 0/0","title":"How to Connect"},{"location":"openstack/create-and-connect-to-the-VM/wireguard/","text":"WireGuard WireGuard is an extremely simple yet fast and modern VPN that utilizes state-of-the-art cryptography. Here's what it will look like: Installing WireGuard Server You can spin up a new instance with \" ubuntu-22.04-x86_64 \" or any available Ubuntu OS image, named \" wireguard_server \" on OpenStack, with \" default \" and \" ssh_only \" Security Groups attached to it. Also, attach a Floating IP to this instance so you can ssh into it from outside. Finally, you'll want to configure the setting for the remote instances in your SSH configuration file (typically found in ~/.ssh/config ). The SSH configuration file might include entry for your newly create WireGuard server like this: Host wireguard HostName 140.247.152.188 User ubuntu IdentityFile ~/.ssh/cloud.key Then you can ssh into the WireGuard Server running: ssh wireguard Also note that WireGuard must be installed and run by a user who has administrative/root privileges. So, we need to run the command: sudo su We are using this repo to install WireGuard server on this ubuntu server. For that, run the script and follow the assistant: sh wget https://git.io/wireguard -O wireguard-install.sh && bash wireguard-install.sh You can press Enter for all default values. And, while entering a name for the first client you can give \" nerc \" as the client name, this will generate a new configuration file (.conf file) named as \" nerc.conf \". Based on your client's name it will name the config file as \" .conf \" Note: For each peers the client configuration files comply with the following template: Copy the generated config file from \" /root/nerc.conf \" to \" /home/ubuntu/nerc.conf \" by running: cp /root/nerc.conf . Update the ownership of the config file to ubuntu user and ubuntu group by running the following command: chown ubuntu:ubuntu nerc.conf You can exit from the root and ssh session all together and then copy the configuration file to your local machine by running the following script on your local machine's terminal: scp wireguard:nerc.conf . To add a new client user Once it ends, you can run it again to add more users, remove some of them or even completely uninstall WireGuard. For this, run the script and follow the assistant: wget https://git.io/wireguard -O wireguard-install.sh && bash wireguard-install.sh Here, you are giving client name as \" mac_client \" and that will generate a new configuration file at \" /root/mac_client.conf \". You can repeat above steps: 4 to 6 to copy this new client's configuration file and share it to the new client. Authentication Mechanism It would be kind of pointless to have our VPN server allow anyone to connect. This is where our public & private keys come into play. Each client's **public** key needs to be added to the SERVER'S configuration file The server's **public** key added to the CLIENT'S configuration file Useful commands To view server config: wg show or, wg To activateconfig: wg-quick up /path/to/file_name.config To deactivate config: wg-quick down /path/to/file_name.config Read more: https://git.zx2c4.com/wireguard-tools/about/src/man/wg.8 https://git.zx2c4.com/wireguard-tools/about/src/man/wg-quick.8 Important Note You need to contact your project administrator to get your own WireGUard configuration file (file with .conf extension). Download it and Keep it in your local machine so in next steps we can use this configuration client profile file. A WireGuard client or compatible software is needed to connect to the WireGuard VPN server. Please install one of these clients depending on your device. The client program must be configured with a client profile to connect to the WireGuard VPN server. Windows WireGuard client can be downloaded here . The WireGuard executable should be installed on client machines. After the installation, you should see the WireGuard icon in the lower-right corner of the screen located at the taskbar notification area. Set up the VPN with WireGuard GUI Next, we configure the VPN tunnel. This includes setting up the endpoints and exchanging the public keys. Open the WireGuard GUI and either click on Add Tunnel -> Import tunnel(s) from file\u2026 OR, click on \" Import tunnel(s) from file \" button located at the center. The software automatically loads the client configuration. Also, it creates a public key for this new tunnel and displays it on the screen. Either, Right Click on your tunnel name and select \" Edit selected tunnel\u2026 \" menu OR, click on \" Edit \" button at the lower left. Checking Block untunneled traffic (kill-switch) will make sure that all your traffic is being routed through this new VPN server. Test your connection On your Windows machine, press the \" Activate\" button. You should see a successful connection be made: After a few seconds, the status should change to Active. If the connection is routed through the VPN, it should show the IP address of the WireGuard server as the public address. If that's not the case, to troubleshoot please check the \" Log \" tab and verify and validate the client and server configuration. Clicking \" Deactivate \" button closes the VPN connection. Mac OS X I. Using HomeBrew This allows more than one Wireguard tunnel active at a time unlike the WireGuard GUI app. Install WireGuard CLI on macOS through brew: brew install wireguard-tools Copy the \" .conf \" file to \" /usr/local/etc/wireguard/ \" (or \" /etc/wireguard/ \"). You'll need to create the \" wireguard \" directory first. For your example, you will have your config file located at: \" /usr/local/etc /wireguard/mac_client.conf \" or, \" /etc/wireguard/mac_client.conf \" To activate the VPN: \"wg-quick up [ name of the conf file without including .conf extension ]\". For example, in your case, running wg-quick up mac_client - If the peer system is already configured and its interface is up, then the VPN connection should establish automatically, and you should be able to start routing traffic through the peer. Use wg-quick down mac_client to take the VPN connection down. II. Using WireGuard GUI App Download WireGuard Client from the macOS App Store You can find the official WireGuard Client app on the App Store here . Set up the VPN with WireGuard Next, we configure the VPN tunnel. This includes setting up the endpoints and exchanging the public keys. Open the WireGuard GUI by directly clicking WireGuard icon in status bar at the top-right corner of your screen. And then click on \" Import tunnel(s) from file \" menu to load your client config file. OR, Find and click the WireGUard GUI from your Launchpad and then either click on Add Tunnel -> Import tunnel(s) from file\u2026 or, just click on \" Import tunnel(s) from file \" button located at the center. Browse to the configuration file: The software automatically loads the client configuration. Also, it creates a public key for this new tunnel and displays it on the screen. If you would like your computer to automatically connect to the WireGuard VPN server as soon as either (or both) Ethernet or Wi-Fi network adapter becomes active, check the relevant ' On-Demand ' checkboxes for \" Ethernet \" and \" Wi-Fi \". Checking Exclude private IPs will generate a list of networks which excludes the server IP address and add them to the AllowedIPs list. This setting allows you to pass all your traffic through your Wireguard VPN EXCLUDING private address ranges like 10.0.0.0/8 , 172.16.0.0/12 , and 192.168.0.0/16 . Test your connection On your Windows machine, press the \" Activate \" button. You should see a successful connection be made: After a few seconds, the status should change to Active. Clicking \" Deactivate \" button from the GUI's interface or directly clicking \" Deactivate \" menu from the WireGuard icon in status bar at the top-right corner of your screen closes the VPN connection. Linux WireGuard is available through the package management system on most Linux distributions. On Debian/Ubuntu: sudo apt update sudo apt-get install wireguard resolvconf -y On RedHat/CentOS: sudo yum install wireguard Then, to run WireGuard using the client profile: Move the VPN client profile (configuration) file to /etc/wireguard/ : sudo mv nerc.conf /etc/wireguard/client.conf Restart the WireGuard daemon (i.e., This will start WireGuard connection and will automatically run on boot): sudo /etc/init.d/wireguard start OR, sudo systemctl enable --now wg-quick@client sudo systemctl start wg-quick@client OR, wg-quick up /etc/wireguard/client.conf Checking the status: systemctl status wg-quick@client Alternatively, if you want to run WireGuard manually each time, then run: sudo wireguard --config /etc/wireguard/client.conf OR, sudo wireguard --config nerc.conf","title":"WireGuard"},{"location":"openstack/create-and-connect-to-the-VM/wireguard/#wireguard","text":"WireGuard is an extremely simple yet fast and modern VPN that utilizes state-of-the-art cryptography. Here's what it will look like:","title":"WireGuard"},{"location":"openstack/create-and-connect-to-the-VM/wireguard/#installing-wireguard-server","text":"You can spin up a new instance with \" ubuntu-22.04-x86_64 \" or any available Ubuntu OS image, named \" wireguard_server \" on OpenStack, with \" default \" and \" ssh_only \" Security Groups attached to it. Also, attach a Floating IP to this instance so you can ssh into it from outside. Finally, you'll want to configure the setting for the remote instances in your SSH configuration file (typically found in ~/.ssh/config ). The SSH configuration file might include entry for your newly create WireGuard server like this: Host wireguard HostName 140.247.152.188 User ubuntu IdentityFile ~/.ssh/cloud.key Then you can ssh into the WireGuard Server running: ssh wireguard Also note that WireGuard must be installed and run by a user who has administrative/root privileges. So, we need to run the command: sudo su We are using this repo to install WireGuard server on this ubuntu server. For that, run the script and follow the assistant: sh wget https://git.io/wireguard -O wireguard-install.sh && bash wireguard-install.sh You can press Enter for all default values. And, while entering a name for the first client you can give \" nerc \" as the client name, this will generate a new configuration file (.conf file) named as \" nerc.conf \". Based on your client's name it will name the config file as \" .conf \" Note: For each peers the client configuration files comply with the following template: Copy the generated config file from \" /root/nerc.conf \" to \" /home/ubuntu/nerc.conf \" by running: cp /root/nerc.conf . Update the ownership of the config file to ubuntu user and ubuntu group by running the following command: chown ubuntu:ubuntu nerc.conf You can exit from the root and ssh session all together and then copy the configuration file to your local machine by running the following script on your local machine's terminal: scp wireguard:nerc.conf .","title":"Installing WireGuard Server"},{"location":"openstack/create-and-connect-to-the-VM/wireguard/#to-add-a-new-client-user","text":"Once it ends, you can run it again to add more users, remove some of them or even completely uninstall WireGuard. For this, run the script and follow the assistant: wget https://git.io/wireguard -O wireguard-install.sh && bash wireguard-install.sh Here, you are giving client name as \" mac_client \" and that will generate a new configuration file at \" /root/mac_client.conf \". You can repeat above steps: 4 to 6 to copy this new client's configuration file and share it to the new client.","title":"To add a new client user"},{"location":"openstack/create-and-connect-to-the-VM/wireguard/#authentication-mechanism","text":"It would be kind of pointless to have our VPN server allow anyone to connect. This is where our public & private keys come into play. Each client's **public** key needs to be added to the SERVER'S configuration file The server's **public** key added to the CLIENT'S configuration file","title":"Authentication Mechanism"},{"location":"openstack/create-and-connect-to-the-VM/wireguard/#useful-commands","text":"To view server config: wg show or, wg To activateconfig: wg-quick up /path/to/file_name.config To deactivate config: wg-quick down /path/to/file_name.config Read more: https://git.zx2c4.com/wireguard-tools/about/src/man/wg.8 https://git.zx2c4.com/wireguard-tools/about/src/man/wg-quick.8 Important Note You need to contact your project administrator to get your own WireGUard configuration file (file with .conf extension). Download it and Keep it in your local machine so in next steps we can use this configuration client profile file. A WireGuard client or compatible software is needed to connect to the WireGuard VPN server. Please install one of these clients depending on your device. The client program must be configured with a client profile to connect to the WireGuard VPN server.","title":"Useful commands"},{"location":"openstack/create-and-connect-to-the-VM/wireguard/#windows","text":"WireGuard client can be downloaded here . The WireGuard executable should be installed on client machines. After the installation, you should see the WireGuard icon in the lower-right corner of the screen located at the taskbar notification area.","title":"Windows"},{"location":"openstack/create-and-connect-to-the-VM/wireguard/#set-up-the-vpn-with-wireguard-gui","text":"Next, we configure the VPN tunnel. This includes setting up the endpoints and exchanging the public keys. Open the WireGuard GUI and either click on Add Tunnel -> Import tunnel(s) from file\u2026 OR, click on \" Import tunnel(s) from file \" button located at the center. The software automatically loads the client configuration. Also, it creates a public key for this new tunnel and displays it on the screen. Either, Right Click on your tunnel name and select \" Edit selected tunnel\u2026 \" menu OR, click on \" Edit \" button at the lower left. Checking Block untunneled traffic (kill-switch) will make sure that all your traffic is being routed through this new VPN server.","title":"Set up the VPN with WireGuard GUI"},{"location":"openstack/create-and-connect-to-the-VM/wireguard/#test-your-connection","text":"On your Windows machine, press the \" Activate\" button. You should see a successful connection be made: After a few seconds, the status should change to Active. If the connection is routed through the VPN, it should show the IP address of the WireGuard server as the public address. If that's not the case, to troubleshoot please check the \" Log \" tab and verify and validate the client and server configuration. Clicking \" Deactivate \" button closes the VPN connection.","title":"Test your connection"},{"location":"openstack/create-and-connect-to-the-VM/wireguard/#mac-os-x","text":"","title":"Mac OS X"},{"location":"openstack/create-and-connect-to-the-VM/wireguard/#i-using-homebrew","text":"This allows more than one Wireguard tunnel active at a time unlike the WireGuard GUI app. Install WireGuard CLI on macOS through brew: brew install wireguard-tools Copy the \" .conf \" file to \" /usr/local/etc/wireguard/ \" (or \" /etc/wireguard/ \"). You'll need to create the \" wireguard \" directory first. For your example, you will have your config file located at: \" /usr/local/etc /wireguard/mac_client.conf \" or, \" /etc/wireguard/mac_client.conf \" To activate the VPN: \"wg-quick up [ name of the conf file without including .conf extension ]\". For example, in your case, running wg-quick up mac_client - If the peer system is already configured and its interface is up, then the VPN connection should establish automatically, and you should be able to start routing traffic through the peer. Use wg-quick down mac_client to take the VPN connection down.","title":"I. Using HomeBrew"},{"location":"openstack/create-and-connect-to-the-VM/wireguard/#ii-using-wireguard-gui-app","text":"Download WireGuard Client from the macOS App Store You can find the official WireGuard Client app on the App Store here . Set up the VPN with WireGuard Next, we configure the VPN tunnel. This includes setting up the endpoints and exchanging the public keys. Open the WireGuard GUI by directly clicking WireGuard icon in status bar at the top-right corner of your screen. And then click on \" Import tunnel(s) from file \" menu to load your client config file. OR, Find and click the WireGUard GUI from your Launchpad and then either click on Add Tunnel -> Import tunnel(s) from file\u2026 or, just click on \" Import tunnel(s) from file \" button located at the center. Browse to the configuration file: The software automatically loads the client configuration. Also, it creates a public key for this new tunnel and displays it on the screen. If you would like your computer to automatically connect to the WireGuard VPN server as soon as either (or both) Ethernet or Wi-Fi network adapter becomes active, check the relevant ' On-Demand ' checkboxes for \" Ethernet \" and \" Wi-Fi \". Checking Exclude private IPs will generate a list of networks which excludes the server IP address and add them to the AllowedIPs list. This setting allows you to pass all your traffic through your Wireguard VPN EXCLUDING private address ranges like 10.0.0.0/8 , 172.16.0.0/12 , and 192.168.0.0/16 . Test your connection On your Windows machine, press the \" Activate \" button. You should see a successful connection be made: After a few seconds, the status should change to Active. Clicking \" Deactivate \" button from the GUI's interface or directly clicking \" Deactivate \" menu from the WireGuard icon in status bar at the top-right corner of your screen closes the VPN connection.","title":"II. Using WireGuard GUI App"},{"location":"openstack/create-and-connect-to-the-VM/wireguard/#linux","text":"WireGuard is available through the package management system on most Linux distributions. On Debian/Ubuntu: sudo apt update sudo apt-get install wireguard resolvconf -y On RedHat/CentOS: sudo yum install wireguard Then, to run WireGuard using the client profile: Move the VPN client profile (configuration) file to /etc/wireguard/ : sudo mv nerc.conf /etc/wireguard/client.conf Restart the WireGuard daemon (i.e., This will start WireGuard connection and will automatically run on boot): sudo /etc/init.d/wireguard start OR, sudo systemctl enable --now wg-quick@client sudo systemctl start wg-quick@client OR, wg-quick up /etc/wireguard/client.conf Checking the status: systemctl status wg-quick@client Alternatively, if you want to run WireGuard manually each time, then run: sudo wireguard --config /etc/wireguard/client.conf OR, sudo wireguard --config nerc.conf","title":"Linux"},{"location":"openstack/logging-in/access-the-openstack-dashboard/","text":"Access the OpenStack Dashboard The OpenStack Dashboard which is a web-based graphical interface, code named Horizon, is located at https://stack.nerc.mghpcc.org . The NERC Authentication supports CILogon using Keycloak for gateway authentication and authorization that provides federated login via your institution accounts and it is the recommended authentication method. Make sure you are selecting \"OpenID Connect\" (which is selected by default) as shown here: Next, you will redirected to CILogon welcome page as shown below: MGHPCC Shared Services (MSS) Keycloak will request approval of access to the following information from the user: Your CILogon user identifier Your name Your email address Your username and affiliation from your identity provider which are required in order to allow access your account to NERC's OpenStack dashboard. From the \"Selected Identity Provider\" dropdown option, please select your institution's name. If you would like to remember your selected institution name for future logins please check the \"Remember this selection\" checkbox this will bypass the CILogon welcome page on subsequent visits and proceed directly to the selected insitution's identity provider(IdP). Click \"Log On\". This will redirect to your respective institutional login page where you need to enter your institutional credentials. Important Note The NERC does not see or have access to your institutional account credentials and neither store them rather it just point to your selected insitution's identity provider and redirects back once authenticated. Once you successfully authenticated you should see an overview of the resources like Compute (instances, VCPUs, RAM, etc.), Volume and Network. You can also see usage summary for provided date range.","title":"Access the OpenStack Dashboard"},{"location":"openstack/logging-in/access-the-openstack-dashboard/#access-the-openstack-dashboard","text":"The OpenStack Dashboard which is a web-based graphical interface, code named Horizon, is located at https://stack.nerc.mghpcc.org . The NERC Authentication supports CILogon using Keycloak for gateway authentication and authorization that provides federated login via your institution accounts and it is the recommended authentication method. Make sure you are selecting \"OpenID Connect\" (which is selected by default) as shown here: Next, you will redirected to CILogon welcome page as shown below: MGHPCC Shared Services (MSS) Keycloak will request approval of access to the following information from the user: Your CILogon user identifier Your name Your email address Your username and affiliation from your identity provider which are required in order to allow access your account to NERC's OpenStack dashboard. From the \"Selected Identity Provider\" dropdown option, please select your institution's name. If you would like to remember your selected institution name for future logins please check the \"Remember this selection\" checkbox this will bypass the CILogon welcome page on subsequent visits and proceed directly to the selected insitution's identity provider(IdP). Click \"Log On\". This will redirect to your respective institutional login page where you need to enter your institutional credentials. Important Note The NERC does not see or have access to your institutional account credentials and neither store them rather it just point to your selected insitution's identity provider and redirects back once authenticated. Once you successfully authenticated you should see an overview of the resources like Compute (instances, VCPUs, RAM, etc.), Volume and Network. You can also see usage summary for provided date range.","title":"Access the OpenStack Dashboard"},{"location":"openstack/logging-in/dashboard-overview/","text":"Dashboard Overview When you logged-in, you will redirected to the Compute panel which is under the Project tab. In the top bar, you can see the two small tabs: \"Project\" and \"Identity\". Beneath that you can see six panels in larger print: \"Project\", \"Compute\", \"Volumes\", \"Network\", \"Orchestration\", and \"Object Store\". Project Panel Navigate: Project -> Project API Access: View API endpoints. Compute Panel Navigate: Project -> Compute Overview: View reports for the project. Instances: View, launch, create a snapshot from, stop, pause, or reboot instances, or connect to them through VNC. Images: View images and instance snapshots created by project users, plus any images that are publicly available. Create, edit, and delete images, and launch instances from images and snapshots. Key Pairs: View, create, edit, import, and delete key pairs. Server Groups: View, create, edit, and delete server groups. Volume Panel Navigate: Project -> Volume Volumes: View, create, edit, delete volumes, and accept volume trnasfer. Backups: View, create, edit, and delete backups. Snapshots: View, create, edit, and delete volume snapshots. Groups: View, create, edit, and delete groups. Group Snapshots: View, create, edit, and delete group snapshots. Network Panel Navigate: Project -> Network Network Topology: View the network topology. Networks: Create and manage public and private networks. Routers: Create and manage routers. Security Groups: View, create, edit, and delete security groups and security group rules.. Load Balancers: View, create, edit, and delete load balancers. Floating IPs: Allocate an IP address to or release it from a project. Trunks: View, create, edit, and delete trunk. Orchestration Panel Navigate: Project->Orchestration Stacks: Use the REST API to orchestrate multiple composite cloud applications. Resource Types: view various resources types and their details. Template Versions: view different heat templates. Template Generator: GUI to generate and save template using drag and drop resources. Object Store Panel Navigate: Project->Object Store Containers: Create and manage containers and objects. In future you would use this tab to create Swift object storage for your projects on a need basis.","title":"Dashboard Overview"},{"location":"openstack/logging-in/dashboard-overview/#dashboard-overview","text":"When you logged-in, you will redirected to the Compute panel which is under the Project tab. In the top bar, you can see the two small tabs: \"Project\" and \"Identity\". Beneath that you can see six panels in larger print: \"Project\", \"Compute\", \"Volumes\", \"Network\", \"Orchestration\", and \"Object Store\".","title":"Dashboard Overview"},{"location":"openstack/logging-in/dashboard-overview/#project-panel","text":"Navigate: Project -> Project API Access: View API endpoints.","title":"Project Panel"},{"location":"openstack/logging-in/dashboard-overview/#compute-panel","text":"Navigate: Project -> Compute Overview: View reports for the project. Instances: View, launch, create a snapshot from, stop, pause, or reboot instances, or connect to them through VNC. Images: View images and instance snapshots created by project users, plus any images that are publicly available. Create, edit, and delete images, and launch instances from images and snapshots. Key Pairs: View, create, edit, import, and delete key pairs. Server Groups: View, create, edit, and delete server groups.","title":"Compute Panel"},{"location":"openstack/logging-in/dashboard-overview/#volume-panel","text":"Navigate: Project -> Volume Volumes: View, create, edit, delete volumes, and accept volume trnasfer. Backups: View, create, edit, and delete backups. Snapshots: View, create, edit, and delete volume snapshots. Groups: View, create, edit, and delete groups. Group Snapshots: View, create, edit, and delete group snapshots.","title":"Volume Panel"},{"location":"openstack/logging-in/dashboard-overview/#network-panel","text":"Navigate: Project -> Network Network Topology: View the network topology. Networks: Create and manage public and private networks. Routers: Create and manage routers. Security Groups: View, create, edit, and delete security groups and security group rules.. Load Balancers: View, create, edit, and delete load balancers. Floating IPs: Allocate an IP address to or release it from a project. Trunks: View, create, edit, and delete trunk.","title":"Network Panel"},{"location":"openstack/logging-in/dashboard-overview/#orchestration-panel","text":"Navigate: Project->Orchestration Stacks: Use the REST API to orchestrate multiple composite cloud applications. Resource Types: view various resources types and their details. Template Versions: view different heat templates. Template Generator: GUI to generate and save template using drag and drop resources.","title":"Orchestration Panel"},{"location":"openstack/logging-in/dashboard-overview/#object-store-panel","text":"Navigate: Project->Object Store Containers: Create and manage containers and objects. In future you would use this tab to create Swift object storage for your projects on a need basis.","title":"Object Store Panel"},{"location":"other-tools/","text":"Kubernetes Kubernetes Overview K8s Flavors Comparision i. Kubernetes Development environment Minikube Minishift Kind MicroK8s K3s 5.a. K3s with High Availibility(HA) setup 5.b. Multi-master HA K3s cluster using k3sup 5.c. Single-Node K3s Cluster using k3d 5.d. Multi-master K3s cluster setup using k3d k0s Red Hat Code Ready Containers(CRC) ii. Kubernetes Production environment Kubeadm 1.a. Bootstrapping cluster with kubeadm 1.b. Creating a HA cluster with kubeadm Kubespray CI/ CD Tools CI/CD Overview Setup CI/CD on NERC GitHub to Jenkins Pipeline","title":"Other Useful Tools"},{"location":"other-tools/#kubernetes","text":"Kubernetes Overview K8s Flavors Comparision","title":"Kubernetes"},{"location":"other-tools/#i-kubernetes-development-environment","text":"Minikube Minishift Kind MicroK8s K3s 5.a. K3s with High Availibility(HA) setup 5.b. Multi-master HA K3s cluster using k3sup 5.c. Single-Node K3s Cluster using k3d 5.d. Multi-master K3s cluster setup using k3d k0s Red Hat Code Ready Containers(CRC)","title":"i. Kubernetes Development environment"},{"location":"other-tools/#ii-kubernetes-production-environment","text":"Kubeadm 1.a. Bootstrapping cluster with kubeadm 1.b. Creating a HA cluster with kubeadm Kubespray","title":"ii. Kubernetes Production environment"},{"location":"other-tools/#ci-cd-tools","text":"CI/CD Overview Setup CI/CD on NERC GitHub to Jenkins Pipeline","title":"CI/ CD Tools"},{"location":"other-tools/CI-CD/CI-CD-pipeline/","text":"What is Continuous Integration/Continuous Delivery (CI/CD) Pipeline? A Continuous Integration/Continuous Delivery (CI/CD) pipeline involves a series of steps that is performed in order to deliver a new version of application. CI/CD pipelines are a practice focused on improving software delivery using automation. Components of a CI/CD pipeline The steps that form a CI/CD pipeline are distinct subsets of tasks that are grouped into a pipeline stage. Typical pipeline stages include: Build - The stage where the application is compiled. Test - The stage where code is tested. Automation here can save both time and effort. Release - The stage where the application is delivered to the central repository. Deploy - In this stage code is deployed to production environment. Validation and compliance - The steps to validate a build are determined by the needs of your organization. Image security scanning, security scanning and code analysis of applications ensure the quality of images and written application's code. Figure: CI/CD Pipeline Stages","title":"CI/CD Overview"},{"location":"other-tools/CI-CD/CI-CD-pipeline/#what-is-continuous-integrationcontinuous-delivery-cicd-pipeline","text":"A Continuous Integration/Continuous Delivery (CI/CD) pipeline involves a series of steps that is performed in order to deliver a new version of application. CI/CD pipelines are a practice focused on improving software delivery using automation.","title":"What is Continuous Integration/Continuous Delivery (CI/CD) Pipeline?"},{"location":"other-tools/CI-CD/CI-CD-pipeline/#components-of-a-cicd-pipeline","text":"The steps that form a CI/CD pipeline are distinct subsets of tasks that are grouped into a pipeline stage. Typical pipeline stages include: Build - The stage where the application is compiled. Test - The stage where code is tested. Automation here can save both time and effort. Release - The stage where the application is delivered to the central repository. Deploy - In this stage code is deployed to production environment. Validation and compliance - The steps to validate a build are determined by the needs of your organization. Image security scanning, security scanning and code analysis of applications ensure the quality of images and written application's code. Figure: CI/CD Pipeline Stages","title":"Components of a CI/CD pipeline"},{"location":"other-tools/CI-CD/integrate-your-GitHub-repository/","text":"How to Integrate Your GitHub Repository to Your Jenkins Project This explains how to add a GitHub Webhook in your Jenkins Pipeline that saves your time and keeps your project updated all the time. Prerequisite You need to have setup CI/CD Pipelines on NERC's OpenStack by following this document . What is a webhook? A webhook is an HTTP callback, an HTTP POST that occurs when something happens through a simple event-notification via HTTP POST. Github provides its own webhooks options for such tasks. Configuring GitHub Let's see how to configure and add a webhook in GitHub: Go to your GitHub project repository. Click on \"Settings\" . in the right corner as shown below: Click on \"Webhooks\" and then \"Click \"Add webhooks.\" In the \"Payload URL\" field paste your Jenkins environment URL. At the end of this URL add /github-webhook/ using http://<Worker_Node_Floating_IP>:8080/github-webhook/ i.e. http://199.94.60.4:8080/github-webhook/ . Select \"Content type\" as \"application/json\" and leave the \"Secret\" field empty. In the page \"Which events would you like to trigger this webhook?\" select the option \"Let me select individual events.\" Then, check \"Pull Requests\" and \"Pushes\". At the end of this option, make sure that the \"Active\" option is checked and then click on \"Add webhook\" button. We're done with the configuration on GitHub's side! Now let's config on Jenkins side to use this webhook. That's it! in this way we can add a webhook to our job and ensure that everytime you commits your changes to your Github repo, GitHub will trigger your new Jenkins job. As we already had setup \"Github hook tirgger for GITScm polling\" for our Jenkins pipeline setup previously .","title":"GitHub to Jenkins Pipeline"},{"location":"other-tools/CI-CD/integrate-your-GitHub-repository/#how-to-integrate-your-github-repository-to-your-jenkins-project","text":"This explains how to add a GitHub Webhook in your Jenkins Pipeline that saves your time and keeps your project updated all the time. Prerequisite You need to have setup CI/CD Pipelines on NERC's OpenStack by following this document .","title":"How to Integrate Your GitHub Repository to Your Jenkins Project"},{"location":"other-tools/CI-CD/integrate-your-GitHub-repository/#what-is-a-webhook","text":"A webhook is an HTTP callback, an HTTP POST that occurs when something happens through a simple event-notification via HTTP POST. Github provides its own webhooks options for such tasks.","title":"What is a webhook?"},{"location":"other-tools/CI-CD/integrate-your-GitHub-repository/#configuring-github","text":"Let's see how to configure and add a webhook in GitHub: Go to your GitHub project repository. Click on \"Settings\" . in the right corner as shown below: Click on \"Webhooks\" and then \"Click \"Add webhooks.\" In the \"Payload URL\" field paste your Jenkins environment URL. At the end of this URL add /github-webhook/ using http://<Worker_Node_Floating_IP>:8080/github-webhook/ i.e. http://199.94.60.4:8080/github-webhook/ . Select \"Content type\" as \"application/json\" and leave the \"Secret\" field empty. In the page \"Which events would you like to trigger this webhook?\" select the option \"Let me select individual events.\" Then, check \"Pull Requests\" and \"Pushes\". At the end of this option, make sure that the \"Active\" option is checked and then click on \"Add webhook\" button. We're done with the configuration on GitHub's side! Now let's config on Jenkins side to use this webhook. That's it! in this way we can add a webhook to our job and ensure that everytime you commits your changes to your Github repo, GitHub will trigger your new Jenkins job. As we already had setup \"Github hook tirgger for GITScm polling\" for our Jenkins pipeline setup previously .","title":"Configuring GitHub"},{"location":"other-tools/CI-CD/setup-CI-CD-pipeline/","text":"How to setup CI/CD Pipelines on NERC's OpenStack This document will walk you through how to setup a minimal \"CI/CD Pipeline To Deploy To Kubernetes Cluster Using a CI/CD tool called Jenkins\" on your NERC's OpenStack environment. Jenkins uses the Kubernetes control plane on K8s Cluster to run pipeline tasks that enable DevOps to spend more time coding and testing and less time troubleshooting. Prerequisite You need Kubernetes cluster running in your OpenStack environment. To setup your K8s cluster please Read this . Figure: CI/CD Pipeline To Deploy To Kubernetes Cluster Using Jenkins on NERC Please follow the following steps: Install a Jenkins Server on one of your Worker Node on your K8s Cluster. This will act as both Jenkins master and slave in this case. But you can just setup a Jenkins Slave on one of your K8s worker node and the master on an external OpenStack Instance too. Setup a Security Rules with the following rules and attach it to the worker node where you will install the Jenkins Server. After SSH'ing to your worker node where you want to install the Jenkins server, To install a Jenkins server using Docker run the following command: docker run -u 0 --privileged --name jenkins -it -d -p 8080:8080 -p 50000:50000 \\ -v /var/run/docker.sock:/var/run/docker.sock \\ -v $(which docker):/usr/bin/docker \\ -v $(which kubectl):/usr/bin/kubectl \\ -v /home/jenkins_home:/var/jenkins_home \\ jenkins/jenkins:latest Once successfully docker run, browse to http://<Worker_Node_Floating_IP>:8080 this will show you where to get the initial Administrator password to get started i.e. /var/jenkins_home/secrets/initialAdminPassword as shown below: The /var/jenkins_home in Jenkins docker container is a mounted volume to the host's /home/jenkins_home so you can just browse to /home/jenkins_home/secrets/initialAdminPassword on your ssh'ed host machine to copy the same content from /var/jenkins_home/secrets/initialAdminPassword . OR , you can run docker ps on worker node where you run the Jenkins server. Note the Name of the docker container and then run: docker logs -f <enkins_docker_container_name> . This will show the initial Administrator password on the terminal which you can copy and paste on the web GUI on the browser. Initial Admin Password The initial admin password can be found between the rows of asterisks. Once you login to the Jenkins Web UI by entering the admin password shown on CLI terminal, click on the \"Install suggested plugins\" button as shown below: Continue by selecting 'Continue as admin' and then click the 'Save and Finish' button. And then, Jenkins is ready to use as showing below: Jenkins has a wide range of plugin options. From your Jenkins dashboard navigate to \"Manage Jenkins > Manage Plugins\" as shown below: Select the \"Available\" tab and then locate Docker pipeline by searching and then click \"Install without restart\" button as shown below: Also, install the Kubernetes CLI plugin that allows you to configure kubectl commands on Jenkinsfile to interact with Kubernetes clusters as shown below: Create a global credential for your Docker Hub Registry by providing the username and password that will be used by the Jenkins pipelines: Click on the \"Manage Jenkins\" menu and then click on the \"Manage Credentials\" link as shown below: Click on Jenkins Store as shown below: The credentials can be added by clicking the 'Add Credentials' button in the left pane. First, add the 'DockerHub' credentials as 'Username and password' with the ID dockerhublogin . a. Select the Kind \"Username and password\" from the dropdown options. b. Provide your Docker Hub Registry's username and password. c. Give its ID and short description. ID is very important is that will need to be specify as used on your Jenkinsfile i.e. dockerhublogin . Config the 'Kubeconfig' credentials as 'Secret file' that holds Kubeconfig file from K8s master i.e. located at /etc/kubernetes/admin.conf with the ID 'kubernetes' a. Click on the \"Add Credentials\" button in the left pane. b. Select the Kind \"Secret file\" from the dropdown options. c. On File section choose the config file that contains the EXACT content from your K8s master's kubeconfig file located at: /etc/kubernetes/admin.conf d. Give a ID and description that you will need to use on your Jenkinsfile i.e. kubernetes . Once both credentials are successfully added the following credentials are shown: Write a Jenkins Pipeline Script file named as \u2018 Jenkinsfile \u2019 as following: pipeline { environment { dockerimagename = \"milstein/nodeapp:${env.BUILD_NUMBER}\" dockerImage = \"\" } agent any stages { stage('Checkout Source') { steps { git branch: 'main', url: 'https://github.com/nerc-project/nodeapp.git' } } stage('Build image') { steps{ script { dockerImage = docker.build dockerimagename } } } stage('Pushing Image') { environment { registryCredential = 'dockerhublogin' } steps{ script { docker.withRegistry('https://registry.hub.docker.com', registryCredential){ dockerImage.push() } } } } stage('Docker Remove Image') { steps { sh \"docker rmi -f ${dockerimagename}\" sh \"docker rmi -f registry.hub.docker.com/${dockerimagename}\" } } stage('Deploying App to Kubernetes') { steps { sh \"sed -i 's/nodeapp:latest/nodeapp:${env.BUILD_NUMBER}/g' deploymentservice.yml\" withKubeConfig([credentialsId: 'kubernetes']) { sh 'kubectl apply -f deploymentservice.yml' } } } } } Very Important Information Above added global credentials such as dockerhublogin is the ID given during the credential saving steps mentioned above. Similarly, kubernetes is the ID given for Kubeconfig credential file. This Jenkinsfile is also located at the root of our demo application git repo from where we can reference on our Jenkins pipeline steps. For example, in this case we are using: https://github.com/nerc-project/nodeapp as our git repo; where our Node.js application resides. Other way to Generate Pipeline Jenkinsfile You can generate your custom Jenkinsfile by clicking on \"Pipeline Syntax\" link shown when you create a new Pipeline when clicking the \"New Item\" menu link. Once you review the provided Jenkinsfile and understand the stages, you can now create a pipeline to trigger it on your newly setup Jenkins server: a. Click on the \"New Item\" link. b. Select the \"Pipeline\" link. c. Give name to your Pipeline i.e. \u201c jenkins-k8s-pipeline \u201d d. Select \"Build Triggers\" tab and then select Github hook tirgger for GITScm polling as shown below: e. Select \"Pipeline\" tab and then select the \"Pipeline script from SCM\" from the dropdown options. Then you need to specify the Git as SCM and also \"Repository URL\" for your public git repo and also specify your branch and Jenkinsfile's name as shown below: OR , You can copy/paste the content of your Jenkinsfile on the given textbox. Please make sure you are selecting the \"Pipeline script\" from the dropdown options. f. Click on \"Save\" button. Finally, click on the \"Build Now\" menu link on right side navigation that will triggers the Pipeline process i.e. Build docker image, Push Image to your Docker Hub Registry and Pull the image from Docker Registry, Remove local Docker images and then Deploy to K8s Cluster as shown below: You can see the deployment to your K8s Cluster is successful then you can browse the output using http://<Worker_Node_Floating_IP>:<NodePort> as shown below: You can see the Console Output logs of this pipeline process by clicking the icon before the id of the started Pipeline on the right bottom corner. The pipeline stages after successful completion looks like below: We will continue on next documentation on how to setup GitHub Webhook in your Jenkins Pipeline so that Jenkins will trigger the build when a devops commits code to your GitHub repository's specific branch.","title":"Setup CI/CD on NERC"},{"location":"other-tools/CI-CD/setup-CI-CD-pipeline/#how-to-setup-cicd-pipelines-on-nercs-openstack","text":"This document will walk you through how to setup a minimal \"CI/CD Pipeline To Deploy To Kubernetes Cluster Using a CI/CD tool called Jenkins\" on your NERC's OpenStack environment. Jenkins uses the Kubernetes control plane on K8s Cluster to run pipeline tasks that enable DevOps to spend more time coding and testing and less time troubleshooting. Prerequisite You need Kubernetes cluster running in your OpenStack environment. To setup your K8s cluster please Read this . Figure: CI/CD Pipeline To Deploy To Kubernetes Cluster Using Jenkins on NERC Please follow the following steps: Install a Jenkins Server on one of your Worker Node on your K8s Cluster. This will act as both Jenkins master and slave in this case. But you can just setup a Jenkins Slave on one of your K8s worker node and the master on an external OpenStack Instance too. Setup a Security Rules with the following rules and attach it to the worker node where you will install the Jenkins Server. After SSH'ing to your worker node where you want to install the Jenkins server, To install a Jenkins server using Docker run the following command: docker run -u 0 --privileged --name jenkins -it -d -p 8080:8080 -p 50000:50000 \\ -v /var/run/docker.sock:/var/run/docker.sock \\ -v $(which docker):/usr/bin/docker \\ -v $(which kubectl):/usr/bin/kubectl \\ -v /home/jenkins_home:/var/jenkins_home \\ jenkins/jenkins:latest Once successfully docker run, browse to http://<Worker_Node_Floating_IP>:8080 this will show you where to get the initial Administrator password to get started i.e. /var/jenkins_home/secrets/initialAdminPassword as shown below: The /var/jenkins_home in Jenkins docker container is a mounted volume to the host's /home/jenkins_home so you can just browse to /home/jenkins_home/secrets/initialAdminPassword on your ssh'ed host machine to copy the same content from /var/jenkins_home/secrets/initialAdminPassword . OR , you can run docker ps on worker node where you run the Jenkins server. Note the Name of the docker container and then run: docker logs -f <enkins_docker_container_name> . This will show the initial Administrator password on the terminal which you can copy and paste on the web GUI on the browser. Initial Admin Password The initial admin password can be found between the rows of asterisks. Once you login to the Jenkins Web UI by entering the admin password shown on CLI terminal, click on the \"Install suggested plugins\" button as shown below: Continue by selecting 'Continue as admin' and then click the 'Save and Finish' button. And then, Jenkins is ready to use as showing below: Jenkins has a wide range of plugin options. From your Jenkins dashboard navigate to \"Manage Jenkins > Manage Plugins\" as shown below: Select the \"Available\" tab and then locate Docker pipeline by searching and then click \"Install without restart\" button as shown below: Also, install the Kubernetes CLI plugin that allows you to configure kubectl commands on Jenkinsfile to interact with Kubernetes clusters as shown below: Create a global credential for your Docker Hub Registry by providing the username and password that will be used by the Jenkins pipelines: Click on the \"Manage Jenkins\" menu and then click on the \"Manage Credentials\" link as shown below: Click on Jenkins Store as shown below: The credentials can be added by clicking the 'Add Credentials' button in the left pane. First, add the 'DockerHub' credentials as 'Username and password' with the ID dockerhublogin . a. Select the Kind \"Username and password\" from the dropdown options. b. Provide your Docker Hub Registry's username and password. c. Give its ID and short description. ID is very important is that will need to be specify as used on your Jenkinsfile i.e. dockerhublogin . Config the 'Kubeconfig' credentials as 'Secret file' that holds Kubeconfig file from K8s master i.e. located at /etc/kubernetes/admin.conf with the ID 'kubernetes' a. Click on the \"Add Credentials\" button in the left pane. b. Select the Kind \"Secret file\" from the dropdown options. c. On File section choose the config file that contains the EXACT content from your K8s master's kubeconfig file located at: /etc/kubernetes/admin.conf d. Give a ID and description that you will need to use on your Jenkinsfile i.e. kubernetes . Once both credentials are successfully added the following credentials are shown: Write a Jenkins Pipeline Script file named as \u2018 Jenkinsfile \u2019 as following: pipeline { environment { dockerimagename = \"milstein/nodeapp:${env.BUILD_NUMBER}\" dockerImage = \"\" } agent any stages { stage('Checkout Source') { steps { git branch: 'main', url: 'https://github.com/nerc-project/nodeapp.git' } } stage('Build image') { steps{ script { dockerImage = docker.build dockerimagename } } } stage('Pushing Image') { environment { registryCredential = 'dockerhublogin' } steps{ script { docker.withRegistry('https://registry.hub.docker.com', registryCredential){ dockerImage.push() } } } } stage('Docker Remove Image') { steps { sh \"docker rmi -f ${dockerimagename}\" sh \"docker rmi -f registry.hub.docker.com/${dockerimagename}\" } } stage('Deploying App to Kubernetes') { steps { sh \"sed -i 's/nodeapp:latest/nodeapp:${env.BUILD_NUMBER}/g' deploymentservice.yml\" withKubeConfig([credentialsId: 'kubernetes']) { sh 'kubectl apply -f deploymentservice.yml' } } } } } Very Important Information Above added global credentials such as dockerhublogin is the ID given during the credential saving steps mentioned above. Similarly, kubernetes is the ID given for Kubeconfig credential file. This Jenkinsfile is also located at the root of our demo application git repo from where we can reference on our Jenkins pipeline steps. For example, in this case we are using: https://github.com/nerc-project/nodeapp as our git repo; where our Node.js application resides. Other way to Generate Pipeline Jenkinsfile You can generate your custom Jenkinsfile by clicking on \"Pipeline Syntax\" link shown when you create a new Pipeline when clicking the \"New Item\" menu link. Once you review the provided Jenkinsfile and understand the stages, you can now create a pipeline to trigger it on your newly setup Jenkins server: a. Click on the \"New Item\" link. b. Select the \"Pipeline\" link. c. Give name to your Pipeline i.e. \u201c jenkins-k8s-pipeline \u201d d. Select \"Build Triggers\" tab and then select Github hook tirgger for GITScm polling as shown below: e. Select \"Pipeline\" tab and then select the \"Pipeline script from SCM\" from the dropdown options. Then you need to specify the Git as SCM and also \"Repository URL\" for your public git repo and also specify your branch and Jenkinsfile's name as shown below: OR , You can copy/paste the content of your Jenkinsfile on the given textbox. Please make sure you are selecting the \"Pipeline script\" from the dropdown options. f. Click on \"Save\" button. Finally, click on the \"Build Now\" menu link on right side navigation that will triggers the Pipeline process i.e. Build docker image, Push Image to your Docker Hub Registry and Pull the image from Docker Registry, Remove local Docker images and then Deploy to K8s Cluster as shown below: You can see the deployment to your K8s Cluster is successful then you can browse the output using http://<Worker_Node_Floating_IP>:<NodePort> as shown below: You can see the Console Output logs of this pipeline process by clicking the icon before the id of the started Pipeline on the right bottom corner. The pipeline stages after successful completion looks like below: We will continue on next documentation on how to setup GitHub Webhook in your Jenkins Pipeline so that Jenkins will trigger the build when a devops commits code to your GitHub repository's specific branch.","title":"How to setup CI/CD Pipelines on NERC's OpenStack"},{"location":"other-tools/kubernetes/comparisons/","text":"Comparison Kubespray vs Kubeadm Kubeadm provides domain Knowledge of Kubernetes clusters' life cycle management, including self-hosted layouts, dynamic discovery services and so on. Had it belonged to the new operators world , it may have been named a \"Kubernetes cluster operator\". Kubespray however, does generic configuration management tasks from the \"OS operators\" ansible world, plus some initial K8s clustering (with networking plugins included) and control plane bootstrapping. Kubespray has started using kubeadm internally for cluster creation since v2.3 in order to consume life cycle management domain knowledge from it and offload generic OS configuration things from it, which hopefully benefits both sides.","title":"K8s Flavors Comparision"},{"location":"other-tools/kubernetes/comparisons/#comparison","text":"","title":"Comparison"},{"location":"other-tools/kubernetes/comparisons/#kubespray-vs-kubeadm","text":"Kubeadm provides domain Knowledge of Kubernetes clusters' life cycle management, including self-hosted layouts, dynamic discovery services and so on. Had it belonged to the new operators world , it may have been named a \"Kubernetes cluster operator\". Kubespray however, does generic configuration management tasks from the \"OS operators\" ansible world, plus some initial K8s clustering (with networking plugins included) and control plane bootstrapping. Kubespray has started using kubeadm internally for cluster creation since v2.3 in order to consume life cycle management domain knowledge from it and offload generic OS configuration things from it, which hopefully benefits both sides.","title":"Kubespray vs Kubeadm"},{"location":"other-tools/kubernetes/crc/","text":"CRC - Red Hat Code Ready Containers Red Hat CodeReady Containers allows you to spin up a small Red Hat OpenShift cluster on your local PC, without the need for a server, a cloud, or a team of operations people. For developers who want to get started immediately with cloud-native development, containers, and Kubernetes (as well as OpenShift), it's a simple and slick tool. It runs on macOS, Linux, and all versions of Windows 10. Minimum system requirements for CRC CodeReady Containers requires the following system resources: 4 virtual CPUs (vCPUs) 8 GB of memory 35 GB of storage space Pre-requisite We will need 1 VM to create a single node kubernetes cluster using crc . We are using following setting for this purpose: 1 Linux machine, fedora-36-x86_64, cpu-a.16 flavor with 16vCPUs, 32GB RAM, 20GB storage - also assign Floating IP to this VM. Prepare host for CRC Run the below command on the CRC's VM: SSH into crc machine Important Note Please run the following commands not using root user. On newly built VM download CRC using your redhat login, from: https://console.redhat.com/openshift/create/local To save transfer hassle you can, curl CRC bundle directly to the VM, using the url from a \"Download CodeReady Containers\" button in redhat console. curl \\ https://developers.redhat.com/content-gateway/rest/mirror/pub/openshift-v4/\\ clients/crc/latest/crc-linux-amd64.tar.xz \\ --output crc-linux-amd64.tar.xz -L then click \"Copy pull secret\" button from the same console page and save it to a file somewhere (for example ~fedora/pull-secret ) Setup crc binary to be accessable tar -xvf crc-linux-amd64.tar.xz mkdir -p ~/bin mv crc-linux-1.34.0-amd64/crc ~/bin/ export PATH=$PATH:$HOME/bin echo 'export PATH=$PATH:$HOME/bin' >> ~/.bashrc Note This CRC version crc-linux-1.34.0-amd64 may be different when you are installing! Please update it as your are running above command. Install and configure CRC Run the install (you can choose to answer N for collecing data by RedHat when prompted on terminal). Also, CRC has trouble starting with default resources allocated to it, that is why additional steps need to be taken allocationg addtional memory to it. Additional cores added as well, but this may not be as critical to it's performance. crc setup Paste crc secret copied during previous prep step when prompted for \"? Please enter the pull secret\" by crc start terminal. crc start Make a note of user login info displayed once install is finished. Output would look like below: Started the OpenShift cluster. The server is accessible via web console at: https://console-openshift-console.apps-crc.testing Log in as administrator: Username: kubeadmin Password: ... #pragma: allowlist secret Log in as user: Username: developer Password: ... #pragma: allowlist secret Use the 'oc' command line interface: $ eval $(crc oc-env) $ oc login -u developer https://api.crc.testing:6443 Note We can set memory and cpu for the CRC instance using: crc config set memory 24576 and crc config set cpus 12 Using CRC CLI Setup your environment eval $(crc oc-env) To look up CRC login credentials you can run. This will provide the oc login commands with password info for both admin and developer users. crc console --credentials To login as a regular user, run 'oc login -u developer -p developer https://api.crc.testing:6443'. To login as an admin, run 'oc login -u kubeadmin -p MTNAK-YHvuU-FIuSt-qgAxd https://api.crc.testing:6443' Using CRC web interface Install and configure HAPROXY first Switch as root: sudo su Install the package sudo dnf install haproxy policycoreutils-python-utils Update configuration cd /etc/haproxy sudo cp haproxy.cfg haproxy.cfg.orig Clean the content of /etc/haproxy/haproxy.cfg : sudo echo > /etc/haproxy/haproxy.cfg Replace /etc/haproxy/haproxy.cfg with cat <<EOF | sudo tee /etc/haproxy/haproxy.cfg global defaults log global mode http timeout connect 0 timeout client 0 timeout server 0 frontend apps bind SERVER_IP:80 bind SERVER_IP:443 option tcplog mode tcp default_backend apps backend apps mode tcp balance roundrobin option ssl-hello-chk server webserver1 CRC_IP:443 check frontend api bind SERVER_IP:6443 option tcplog mode tcp default_backend api backend api mode tcp balance roundrobin option ssl-hello-chk server webserver1 CRC_IP:6443 check EOF Switch out from root to fedora user and then continue following steps: Plugin your servers and crc ip addresses # this may be different depending on your setup export SERVER_IP=$(hostname --ip-address |cut -d\\ -f3) export CRC_IP=$(crc ip) sudo sed -i \"s/SERVER_IP/$SERVER_IP/g\" /etc/haproxy/haproxy.cfg sudo sed -i \"s/CRC_IP/$CRC_IP/g\" /etc/haproxy/haproxy.cfg sudo semanage port -a -t http_port_t -p tcp 6443 Start haproxy sudo systemctl start haproxy sudo systemctl status haproxy Ensure haproxy is in running status. Configure your local workstation to resolve CRC addresses Add security groups for your CRC instance to open ports 80, 443 and 6443. In our example setup, the Security Group Rules that are attached to a new Security Rule to the CRC instance has entries like this: Configure your local workstations host lookup to resolve names associated with CRC to the Public IP i.e. Floating IP address of your openstack VM instance. This can be done in several ways: 1) RH document [2] describes a dnsmasq configuration. 2) A simpler path for Linux and Mac users is just to create an entry in your /etc/hosts file or for Windows users find it at C:\\Windows\\System32\\Drivers\\etc\\hosts . Associate your CRC servers public ip retrieved from Horizon with hostnames: api.crc.testing canary-openshift-ingress-canary.apps-crc.testing console-openshift-console.apps-crc.testing default-route-openshift-image-registry.apps-crc.testing downloads-openshift-console.apps-crc.testing oauth-openshift.apps-crc.testing apps-crc.testing For example, using #2 by adding an entry in your /etc/hosts file in your local machine. Your local machine's /etc/hosts may have an entry looks like this: <Your CRC Instance's Floating IP> api.crc.testing canary-openshift-ingress-canary.apps-crc.testing console-openshift-console.apps-crc.testing default-route-openshift-image-registry.apps-crc.testing downloads-openshift-console.apps-crc.testing oauth-openshift.apps-crc.testing apps-crc.testing Use what's appropriate to your environment, ask for help if unsure. Point your browser https://console-openshift-console.apps-crc.testing and log in using crednetials provided by the output of crc start or crc console --credentials command. References [1] Getting Started CRC [2] Accessing CRC on a remote server","title":"CRC - Red Hat Code Ready Containers"},{"location":"other-tools/kubernetes/crc/#crc-red-hat-code-ready-containers","text":"Red Hat CodeReady Containers allows you to spin up a small Red Hat OpenShift cluster on your local PC, without the need for a server, a cloud, or a team of operations people. For developers who want to get started immediately with cloud-native development, containers, and Kubernetes (as well as OpenShift), it's a simple and slick tool. It runs on macOS, Linux, and all versions of Windows 10.","title":"CRC - Red Hat Code Ready Containers"},{"location":"other-tools/kubernetes/crc/#minimum-system-requirements-for-crc","text":"CodeReady Containers requires the following system resources: 4 virtual CPUs (vCPUs) 8 GB of memory 35 GB of storage space","title":"Minimum system requirements for CRC"},{"location":"other-tools/kubernetes/crc/#pre-requisite","text":"We will need 1 VM to create a single node kubernetes cluster using crc . We are using following setting for this purpose: 1 Linux machine, fedora-36-x86_64, cpu-a.16 flavor with 16vCPUs, 32GB RAM, 20GB storage - also assign Floating IP to this VM.","title":"Pre-requisite"},{"location":"other-tools/kubernetes/crc/#prepare-host-for-crc","text":"Run the below command on the CRC's VM: SSH into crc machine Important Note Please run the following commands not using root user. On newly built VM download CRC using your redhat login, from: https://console.redhat.com/openshift/create/local To save transfer hassle you can, curl CRC bundle directly to the VM, using the url from a \"Download CodeReady Containers\" button in redhat console. curl \\ https://developers.redhat.com/content-gateway/rest/mirror/pub/openshift-v4/\\ clients/crc/latest/crc-linux-amd64.tar.xz \\ --output crc-linux-amd64.tar.xz -L then click \"Copy pull secret\" button from the same console page and save it to a file somewhere (for example ~fedora/pull-secret ) Setup crc binary to be accessable tar -xvf crc-linux-amd64.tar.xz mkdir -p ~/bin mv crc-linux-1.34.0-amd64/crc ~/bin/ export PATH=$PATH:$HOME/bin echo 'export PATH=$PATH:$HOME/bin' >> ~/.bashrc Note This CRC version crc-linux-1.34.0-amd64 may be different when you are installing! Please update it as your are running above command.","title":"Prepare host for CRC"},{"location":"other-tools/kubernetes/crc/#install-and-configure-crc","text":"Run the install (you can choose to answer N for collecing data by RedHat when prompted on terminal). Also, CRC has trouble starting with default resources allocated to it, that is why additional steps need to be taken allocationg addtional memory to it. Additional cores added as well, but this may not be as critical to it's performance. crc setup Paste crc secret copied during previous prep step when prompted for \"? Please enter the pull secret\" by crc start terminal. crc start Make a note of user login info displayed once install is finished. Output would look like below: Started the OpenShift cluster. The server is accessible via web console at: https://console-openshift-console.apps-crc.testing Log in as administrator: Username: kubeadmin Password: ... #pragma: allowlist secret Log in as user: Username: developer Password: ... #pragma: allowlist secret Use the 'oc' command line interface: $ eval $(crc oc-env) $ oc login -u developer https://api.crc.testing:6443 Note We can set memory and cpu for the CRC instance using: crc config set memory 24576 and crc config set cpus 12","title":"Install and configure CRC"},{"location":"other-tools/kubernetes/crc/#using-crc-cli","text":"Setup your environment eval $(crc oc-env) To look up CRC login credentials you can run. This will provide the oc login commands with password info for both admin and developer users. crc console --credentials To login as a regular user, run 'oc login -u developer -p developer https://api.crc.testing:6443'. To login as an admin, run 'oc login -u kubeadmin -p MTNAK-YHvuU-FIuSt-qgAxd https://api.crc.testing:6443'","title":"Using CRC CLI"},{"location":"other-tools/kubernetes/crc/#using-crc-web-interface","text":"","title":"Using CRC web interface"},{"location":"other-tools/kubernetes/crc/#install-and-configure-haproxy-first","text":"Switch as root: sudo su Install the package sudo dnf install haproxy policycoreutils-python-utils Update configuration cd /etc/haproxy sudo cp haproxy.cfg haproxy.cfg.orig Clean the content of /etc/haproxy/haproxy.cfg : sudo echo > /etc/haproxy/haproxy.cfg Replace /etc/haproxy/haproxy.cfg with cat <<EOF | sudo tee /etc/haproxy/haproxy.cfg global defaults log global mode http timeout connect 0 timeout client 0 timeout server 0 frontend apps bind SERVER_IP:80 bind SERVER_IP:443 option tcplog mode tcp default_backend apps backend apps mode tcp balance roundrobin option ssl-hello-chk server webserver1 CRC_IP:443 check frontend api bind SERVER_IP:6443 option tcplog mode tcp default_backend api backend api mode tcp balance roundrobin option ssl-hello-chk server webserver1 CRC_IP:6443 check EOF Switch out from root to fedora user and then continue following steps: Plugin your servers and crc ip addresses # this may be different depending on your setup export SERVER_IP=$(hostname --ip-address |cut -d\\ -f3) export CRC_IP=$(crc ip) sudo sed -i \"s/SERVER_IP/$SERVER_IP/g\" /etc/haproxy/haproxy.cfg sudo sed -i \"s/CRC_IP/$CRC_IP/g\" /etc/haproxy/haproxy.cfg sudo semanage port -a -t http_port_t -p tcp 6443 Start haproxy sudo systemctl start haproxy sudo systemctl status haproxy Ensure haproxy is in running status.","title":"Install and configure HAPROXY first"},{"location":"other-tools/kubernetes/crc/#configure-your-local-workstation-to-resolve-crc-addresses","text":"Add security groups for your CRC instance to open ports 80, 443 and 6443. In our example setup, the Security Group Rules that are attached to a new Security Rule to the CRC instance has entries like this: Configure your local workstations host lookup to resolve names associated with CRC to the Public IP i.e. Floating IP address of your openstack VM instance. This can be done in several ways: 1) RH document [2] describes a dnsmasq configuration. 2) A simpler path for Linux and Mac users is just to create an entry in your /etc/hosts file or for Windows users find it at C:\\Windows\\System32\\Drivers\\etc\\hosts . Associate your CRC servers public ip retrieved from Horizon with hostnames: api.crc.testing canary-openshift-ingress-canary.apps-crc.testing console-openshift-console.apps-crc.testing default-route-openshift-image-registry.apps-crc.testing downloads-openshift-console.apps-crc.testing oauth-openshift.apps-crc.testing apps-crc.testing For example, using #2 by adding an entry in your /etc/hosts file in your local machine. Your local machine's /etc/hosts may have an entry looks like this: <Your CRC Instance's Floating IP> api.crc.testing canary-openshift-ingress-canary.apps-crc.testing console-openshift-console.apps-crc.testing default-route-openshift-image-registry.apps-crc.testing downloads-openshift-console.apps-crc.testing oauth-openshift.apps-crc.testing apps-crc.testing Use what's appropriate to your environment, ask for help if unsure. Point your browser https://console-openshift-console.apps-crc.testing and log in using crednetials provided by the output of crc start or crc console --credentials command.","title":"Configure your local workstation to resolve CRC addresses"},{"location":"other-tools/kubernetes/crc/#references","text":"[1] Getting Started CRC [2] Accessing CRC on a remote server","title":"References"},{"location":"other-tools/kubernetes/k0s/","text":"k0s Key Features Available as a single static binary Offers a self-hosted, isolated control plane Supports a variety of storage backends, including etcd, SQLite, MySQL (or any compatible), and PostgreSQL. Offers an Elastic control plane Vanilla upstream Kubernetes Supports custom container runtimes (containerd is the default) Supports custom Container Network Interface (CNI) plugins (calico is the default) Supports x86_64 and arm64 Pre-requisite We will need 1 VM to create a single node kubernetes cluster using k0s . We are using following setting for this purpose: 1 Linux machine, ubuntu-22.04-x86_64 or your choice of Ubuntu OS image, cpu-a.2 flavor with 2vCPU, 4GB RAM, 20GB - also assign Floating IP to this VM. setup Unique hostname to the machine using the following command: echo \"<node_internal_IP> <host_name>\" >> /etc/hosts hostnamectl set-hostname <host_name> For example, echo \"192.168.0.252 k0s\" >> /etc/hosts hostnamectl set-hostname k0s Install k0s on Ubuntu Run the below command on the Ubuntu VM: SSH into k0s machine Switch to root user: sudo su Update the repositories and packages: apt-get update && apt-get upgrade -y Download k0s: curl -sSLf https://get.k0s.sh | sudo sh Install k0s as a service: k0s install controller --single INFO[2021-10-12 01:45:52] no config file given, using defaults INFO[2021-10-12 01:45:52] creating user: etcd INFO[2021-10-12 01:46:00] creating user: kube-apiserver INFO[2021-10-12 01:46:00] creating user: konnectivity-server INFO[2021-10-12 01:46:00] creating user: kube-scheduler INFO[2021-10-12 01:46:01] Installing k0s service Start k0s as a service: k0s start Check service, logs and k0s status: k0s status Version: v1.22.2+k0s.1 Process ID: 16625 Role: controller Workloads: true Access your cluster using kubectl : k0s kubectl get nodes NAME STATUS ROLES AGE VERSION k0s Ready <none> 8m3s v1.22.2+k0s alias kubectl='k0s kubectl' kubectl get nodes -o wide kubectl get all NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/kubernetes ClusterIP 10.96.0.1 <none> 443/TCP 38s Uninstall k0s Stop the service: sudo k0s stop Execute the k0s reset command - cleans up the installed system service, data directories, containers, mounts and network namespaces. sudo k0s reset Reboot the system","title":"k0s"},{"location":"other-tools/kubernetes/k0s/#k0s","text":"","title":"k0s"},{"location":"other-tools/kubernetes/k0s/#key-features","text":"Available as a single static binary Offers a self-hosted, isolated control plane Supports a variety of storage backends, including etcd, SQLite, MySQL (or any compatible), and PostgreSQL. Offers an Elastic control plane Vanilla upstream Kubernetes Supports custom container runtimes (containerd is the default) Supports custom Container Network Interface (CNI) plugins (calico is the default) Supports x86_64 and arm64","title":"Key Features"},{"location":"other-tools/kubernetes/k0s/#pre-requisite","text":"We will need 1 VM to create a single node kubernetes cluster using k0s . We are using following setting for this purpose: 1 Linux machine, ubuntu-22.04-x86_64 or your choice of Ubuntu OS image, cpu-a.2 flavor with 2vCPU, 4GB RAM, 20GB - also assign Floating IP to this VM. setup Unique hostname to the machine using the following command: echo \"<node_internal_IP> <host_name>\" >> /etc/hosts hostnamectl set-hostname <host_name> For example, echo \"192.168.0.252 k0s\" >> /etc/hosts hostnamectl set-hostname k0s","title":"Pre-requisite"},{"location":"other-tools/kubernetes/k0s/#install-k0s-on-ubuntu","text":"Run the below command on the Ubuntu VM: SSH into k0s machine Switch to root user: sudo su Update the repositories and packages: apt-get update && apt-get upgrade -y Download k0s: curl -sSLf https://get.k0s.sh | sudo sh Install k0s as a service: k0s install controller --single INFO[2021-10-12 01:45:52] no config file given, using defaults INFO[2021-10-12 01:45:52] creating user: etcd INFO[2021-10-12 01:46:00] creating user: kube-apiserver INFO[2021-10-12 01:46:00] creating user: konnectivity-server INFO[2021-10-12 01:46:00] creating user: kube-scheduler INFO[2021-10-12 01:46:01] Installing k0s service Start k0s as a service: k0s start Check service, logs and k0s status: k0s status Version: v1.22.2+k0s.1 Process ID: 16625 Role: controller Workloads: true Access your cluster using kubectl : k0s kubectl get nodes NAME STATUS ROLES AGE VERSION k0s Ready <none> 8m3s v1.22.2+k0s alias kubectl='k0s kubectl' kubectl get nodes -o wide kubectl get all NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/kubernetes ClusterIP 10.96.0.1 <none> 443/TCP 38s","title":"Install k0s on Ubuntu"},{"location":"other-tools/kubernetes/k0s/#uninstall-k0s","text":"Stop the service: sudo k0s stop Execute the k0s reset command - cleans up the installed system service, data directories, containers, mounts and network namespaces. sudo k0s reset Reboot the system","title":"Uninstall k0s"},{"location":"other-tools/kubernetes/kind/","text":"Kind Pre-requisite We will need 1 VM to create a single node kubernetes cluster using kind . We are using following setting for this purpose: 1 Linux machine, centos-7-x86_64, cpu-a.2 flavor with 2vCPU, 4GB RAM, 20GB storage - also assign Floating IP to this VM. setup Unique hostname to the machine using the following command: echo \"<node_internal_IP> <host_name>\" >> /etc/hosts hostnamectl set-hostname <host_name> For example, echo \"192.168.0.167 kind\" >> /etc/hosts hostnamectl set-hostname kind Install docker on CentOS7 Run the below command on the CentOS7 VM: SSH into kind machine Switch to root user: sudo su Execute the below command to initialize the cluster: yum -y install epel-release; yum -y install docker; systemctl enable --now docker; systemctl status docker docker version Install kubectl on CentOS7 curl -LO \"https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl\" sudo install -o root -g root -m 0755 kubectl /usr/bin/kubectl chmod +x /usr/bin/kubectl Test to ensure the version you installed is up-to-date: kubectl version --client Install kind curl -Lo ./kind https://kind.sigs.k8s.io/dl/v0.11.1/kind-linux-amd64 chmod +x ./kind mv ./kind /usr/bin which kind /usr/bin/kind kind version kind v0.11.1 go1.16.4 linux/amd64 To communicate with cluster, just give the cluster name as a context in kubectl: kind create cluster --name k8s-kind-cluster1 Creating cluster \"k8s-kind-cluster1\" ... \u2713 Ensuring node image (kindest/node:v1.21.1) \ud83d\uddbc \u2713 Preparing nodes \ud83d\udce6 \u2713 Writing configuration \ud83d\udcdc \u2713 Starting control-plane \ud83d\udd79\ufe0f \u2713 Installing CNI \ud83d\udd0c \u2713 Installing StorageClass \ud83d\udcbe Set kubectl context to \"kind-k8s-kind-cluster1\" You can now use your cluster with: kubectl cluster-info --context kind-k8s-kind-cluster1 Thanks for using kind! \ud83d\ude0a Get the cluster details: kubectl cluster-info --context kind-k8s-kind-cluster1 Kubernetes control plane is running at https://127.0.0.1:38646 CoreDNS is running at https://127.0.0.1:38646/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy To further debug and diagnose cluster problems, use 'kubectl cluster-info dump'. kubectl get all NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/kubernetes ClusterIP 10.96.0.1 <none> 443/TCP 5m25s kubectl get nodes NAME STATUS ROLES AGE VERSION k8s-kind-cluster1-control-plane Ready control-plane,master 5m26s v1.21.11 Deleting a Cluster If you created a cluster with kind create cluster then deleting is equally simple: kind delete cluster","title":"Kind"},{"location":"other-tools/kubernetes/kind/#kind","text":"","title":"Kind"},{"location":"other-tools/kubernetes/kind/#pre-requisite","text":"We will need 1 VM to create a single node kubernetes cluster using kind . We are using following setting for this purpose: 1 Linux machine, centos-7-x86_64, cpu-a.2 flavor with 2vCPU, 4GB RAM, 20GB storage - also assign Floating IP to this VM. setup Unique hostname to the machine using the following command: echo \"<node_internal_IP> <host_name>\" >> /etc/hosts hostnamectl set-hostname <host_name> For example, echo \"192.168.0.167 kind\" >> /etc/hosts hostnamectl set-hostname kind","title":"Pre-requisite"},{"location":"other-tools/kubernetes/kind/#install-docker-on-centos7","text":"Run the below command on the CentOS7 VM: SSH into kind machine Switch to root user: sudo su Execute the below command to initialize the cluster: yum -y install epel-release; yum -y install docker; systemctl enable --now docker; systemctl status docker docker version","title":"Install docker on CentOS7"},{"location":"other-tools/kubernetes/kind/#install-kubectl-on-centos7","text":"curl -LO \"https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl\" sudo install -o root -g root -m 0755 kubectl /usr/bin/kubectl chmod +x /usr/bin/kubectl Test to ensure the version you installed is up-to-date: kubectl version --client","title":"Install kubectl on CentOS7"},{"location":"other-tools/kubernetes/kind/#install-kind","text":"curl -Lo ./kind https://kind.sigs.k8s.io/dl/v0.11.1/kind-linux-amd64 chmod +x ./kind mv ./kind /usr/bin which kind /usr/bin/kind kind version kind v0.11.1 go1.16.4 linux/amd64 To communicate with cluster, just give the cluster name as a context in kubectl: kind create cluster --name k8s-kind-cluster1 Creating cluster \"k8s-kind-cluster1\" ... \u2713 Ensuring node image (kindest/node:v1.21.1) \ud83d\uddbc \u2713 Preparing nodes \ud83d\udce6 \u2713 Writing configuration \ud83d\udcdc \u2713 Starting control-plane \ud83d\udd79\ufe0f \u2713 Installing CNI \ud83d\udd0c \u2713 Installing StorageClass \ud83d\udcbe Set kubectl context to \"kind-k8s-kind-cluster1\" You can now use your cluster with: kubectl cluster-info --context kind-k8s-kind-cluster1 Thanks for using kind! \ud83d\ude0a Get the cluster details: kubectl cluster-info --context kind-k8s-kind-cluster1 Kubernetes control plane is running at https://127.0.0.1:38646 CoreDNS is running at https://127.0.0.1:38646/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy To further debug and diagnose cluster problems, use 'kubectl cluster-info dump'. kubectl get all NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/kubernetes ClusterIP 10.96.0.1 <none> 443/TCP 5m25s kubectl get nodes NAME STATUS ROLES AGE VERSION k8s-kind-cluster1-control-plane Ready control-plane,master 5m26s v1.21.11","title":"Install kind"},{"location":"other-tools/kubernetes/kind/#deleting-a-cluster","text":"If you created a cluster with kind create cluster then deleting is equally simple: kind delete cluster","title":"Deleting a Cluster"},{"location":"other-tools/kubernetes/kubernetes/","text":"Kubernetes Overview Kubernetes , commonly known as K8s is an open sourced container orchestration tool for managing containerized cloud-native workloads and services in computing, networking, and storage infrastructure. K8s can help to deploy and manage containerized applications like platforms as a service(PaaS), batch processing workers, and microservices in the cloud at scale. It reduces cloud computing costs while simplifying the operation of resilient and scalable applications. While it is possible to install and manage Kubernetes on infrastructure that you manage, it is a time-consuming and complicated process. To make provisioning and deploying clusters much easier, we have listed a number of popular platforms and tools to setup your K8s on your NERC's OpenStack Project space. Kubernetes Components & Architecture A Kubernetes cluster consists of a set of worker machines, called nodes, that run containerized applications. Every cluster has at least one worker node. The worker node(s) host the Pods that are the components of the application workload. The control plane or master manages the worker nodes and the Pods in the cluster. In production environments, the control plane usually runs across multiple computers and a cluster usually runs multiple nodes, providing fault-tolerance, redundancy, and high availability. Here's the diagram of a Kubernetes cluster with all the components tied together. Kubernetes Basics workflow Create a Kubernetes cluster Deploy an app Explore your app Expose your app publicly Scale up your app Update your app Development environment Minikube is a local Kubernetes cluster that focuses on making Kubernetes development and learning simple. Kubernetes may be started with just a single command if you have a Docker (or similarly comparable) container or a Virtual Machine environment. For more read this . Minishift is a tool for running OKD locally by launching a single-node OKD cluster within a virtual machine. Minishift is an open-source project forked from Minikube . Minishift is a tool that helps you run OpenShift locally by running a single-node OpenShift cluster inside a VM. For more read this . Kind is a tool for running local Kubernetes clusters utilizing Docker container \"nodes\". It was built for Kubernetes testing, but it may also be used for local development and continuous integration. For more read this . MicroK8s is the smallest, fastest, and most conformant Kubernetes that tracks upstream releases and simplifies clustering. MicroK8s is ideal for prototyping, testing, and offline development. For more read this . K3s is a single <40MB binary, certified Kubernetes distribution developed by Rancher Labs and now a CNCF sandbox project that fully implements the Kubernetes API and is less than 40MB in size. To do so, they got rid of a lot of additional drivers that didn't need to be in the core and could easily be replaced with add-ons. For more read this . To setup a Multi-master HA K3s cluster using k3sup(pronounced ketchup ) read this . To setup a Single-Node K3s Cluster using k3d read this and if you would like to setup Multi-master K3s cluster setup using k3d read this . k0s is an all-inclusive Kubernetes distribution, configured with all of the features needed to build a Kubernetes cluster simply by copying and running an executable file on each target host. For more read this . CRC - Red Hat Code Ready Containers is a great way to experience the most recent version of OpenShift locally (CRC). CRC instals a minimum OpenShift 4.x cluster on your local machine, allowing you to create and test in a controlled environment. CRC is primarily designed for use on the workstations of programmers. For more read this . Production environment If your Kubernetes cluster has to run critical workloads, it must be configured to be resilient and higly available(HA) production-ready Kubernetes cluster. To setup production-quality cluster, you can use the following deployment tools. Kubeadm performs the actions necessary to get a minimum viable, secure cluster up and running in a user friendly way. Bootstrapping cluster with kubeadm read this and if you would like to setup Multi-master cluster setup using Kubeadm read this . Kubespray helps to install a Kubernetes cluster on NERC OpenStack. Kubespray is a composition of Ansible playbooks, inventory, provisioning tools, and domain knowledge for generic OS/Kubernetes clusters configuration management tasks. Installing Kubernetes with Kubespray read this . To choose a tool which best fits your use case, read this comparison .","title":"Kubernetes Overview"},{"location":"other-tools/kubernetes/kubernetes/#kubernetes-overview","text":"Kubernetes , commonly known as K8s is an open sourced container orchestration tool for managing containerized cloud-native workloads and services in computing, networking, and storage infrastructure. K8s can help to deploy and manage containerized applications like platforms as a service(PaaS), batch processing workers, and microservices in the cloud at scale. It reduces cloud computing costs while simplifying the operation of resilient and scalable applications. While it is possible to install and manage Kubernetes on infrastructure that you manage, it is a time-consuming and complicated process. To make provisioning and deploying clusters much easier, we have listed a number of popular platforms and tools to setup your K8s on your NERC's OpenStack Project space.","title":"Kubernetes Overview"},{"location":"other-tools/kubernetes/kubernetes/#kubernetes-components-architecture","text":"A Kubernetes cluster consists of a set of worker machines, called nodes, that run containerized applications. Every cluster has at least one worker node. The worker node(s) host the Pods that are the components of the application workload. The control plane or master manages the worker nodes and the Pods in the cluster. In production environments, the control plane usually runs across multiple computers and a cluster usually runs multiple nodes, providing fault-tolerance, redundancy, and high availability. Here's the diagram of a Kubernetes cluster with all the components tied together.","title":"Kubernetes Components &amp; Architecture"},{"location":"other-tools/kubernetes/kubernetes/#kubernetes-basics-workflow","text":"Create a Kubernetes cluster Deploy an app Explore your app Expose your app publicly Scale up your app Update your app","title":"Kubernetes Basics workflow"},{"location":"other-tools/kubernetes/kubernetes/#development-environment","text":"Minikube is a local Kubernetes cluster that focuses on making Kubernetes development and learning simple. Kubernetes may be started with just a single command if you have a Docker (or similarly comparable) container or a Virtual Machine environment. For more read this . Minishift is a tool for running OKD locally by launching a single-node OKD cluster within a virtual machine. Minishift is an open-source project forked from Minikube . Minishift is a tool that helps you run OpenShift locally by running a single-node OpenShift cluster inside a VM. For more read this . Kind is a tool for running local Kubernetes clusters utilizing Docker container \"nodes\". It was built for Kubernetes testing, but it may also be used for local development and continuous integration. For more read this . MicroK8s is the smallest, fastest, and most conformant Kubernetes that tracks upstream releases and simplifies clustering. MicroK8s is ideal for prototyping, testing, and offline development. For more read this . K3s is a single <40MB binary, certified Kubernetes distribution developed by Rancher Labs and now a CNCF sandbox project that fully implements the Kubernetes API and is less than 40MB in size. To do so, they got rid of a lot of additional drivers that didn't need to be in the core and could easily be replaced with add-ons. For more read this . To setup a Multi-master HA K3s cluster using k3sup(pronounced ketchup ) read this . To setup a Single-Node K3s Cluster using k3d read this and if you would like to setup Multi-master K3s cluster setup using k3d read this . k0s is an all-inclusive Kubernetes distribution, configured with all of the features needed to build a Kubernetes cluster simply by copying and running an executable file on each target host. For more read this . CRC - Red Hat Code Ready Containers is a great way to experience the most recent version of OpenShift locally (CRC). CRC instals a minimum OpenShift 4.x cluster on your local machine, allowing you to create and test in a controlled environment. CRC is primarily designed for use on the workstations of programmers. For more read this .","title":"Development environment"},{"location":"other-tools/kubernetes/kubernetes/#production-environment","text":"If your Kubernetes cluster has to run critical workloads, it must be configured to be resilient and higly available(HA) production-ready Kubernetes cluster. To setup production-quality cluster, you can use the following deployment tools. Kubeadm performs the actions necessary to get a minimum viable, secure cluster up and running in a user friendly way. Bootstrapping cluster with kubeadm read this and if you would like to setup Multi-master cluster setup using Kubeadm read this . Kubespray helps to install a Kubernetes cluster on NERC OpenStack. Kubespray is a composition of Ansible playbooks, inventory, provisioning tools, and domain knowledge for generic OS/Kubernetes clusters configuration management tasks. Installing Kubernetes with Kubespray read this . To choose a tool which best fits your use case, read this comparison .","title":"Production environment"},{"location":"other-tools/kubernetes/kubespray/","text":"Kubespray Pre-requisite We will need 1 control-plane(master) and 1 worker node to create a single control-plane kubernetes cluster using Kubespray . We are using following setting for this purpose: 1 Linux machine for Ansible master, ubuntu-22.04-x86_64 or your choice of Ubuntu OS image, cpu-a.2 flavor with 2vCPU, 4GB RAM, 20GB. 1 Linux machine for master, ubuntu-22.04-x86_64 or your choice of Ubuntu OS image, cpu-a.2 flavor with 2vCPU, 4GB RAM, 20GB - also assign Floating IP to the master node. 1 Linux machines for worker, ubuntu-22.04-x86_64 or your choice of Ubuntu OS image, cpu-a.1 flavor with 1vCPU, 2GB RAM, 20GB storage. ssh access to all machines: Read more here on how to setup SSH to your remote VMs. To allow SSH from Ansible master to all other nodes : Read more here Generate SSH key for Ansible master node using: ssh-keygen -t rsa Generating public/private rsa key pair. Enter file in which to save the key (/root/.ssh/id_rsa): Enter passphrase (empty for no passphrase): Enter same passphrase again: Your identification has been saved in /root/.ssh/id_rsa Your public key has been saved in /root/.ssh/id_rsa.pub The key fingerprint is: SHA256:OMsKP7EmhT400AJA/KN1smKt6eTaa3QFQUiepmj8dxroot@ansible-master The key's randomart image is: +---[RSA 3072]----+ |=o.oo. | |.o... | |..= . | |=o.= ... | |o=+.=.o SE | |.+*o+. o. . | |.=== +o. . | |o+=o=.. | |++o=o. | +----[SHA256]-----+ Copy and append the content of SSH public key i.e. ~/.ssh/id_rsa.pub to other nodes's ~/.ssh/authorized_keys file. This will allow ssh <other_nodes_internal_ip> from the Ansible master node's terminal. Create 2 security groups with appropriate ports and protocols : i. To be used by the master nodes: ii. To be used by the worker nodes: - setup Unique hostname to each machine using the following command: echo \"<node_internal_IP> <host_name>\" >> /etc/hosts hostnamectl set-hostname <host_name> For example, echo \"192.168.0.224 ansible_master\" >> /etc/hosts hostnamectl set-hostname ansible_master In this step, you will update packages and disable swap on the all 3 nodes: 1 Ansible Master Node - ansible_master 1 Kubernetes Master Node - kubspray_master 1 Kubernetes Worker Node - kubspray_worker1 The below steps will be performed on all the above mentioned nodes: SSH into all the 3 machines Switch as root: sudo su Update the repositories and packages: apt-get update && apt-get upgrade -y Turn off swap swapoff -a sed -i '/ swap / s/^/#/' /etc/fstab Configure Kubespray on ansible_master node using Ansible Playbook Run the below command on the master node i.e. master that you want to setup as control plane. SSH into ansible_master machine Switch to root user: sudo su Execute the below command to initialize the cluster: Install Python3 and upgrade pip to pip3: apt install python3-pip -y pip3 install --upgrade pip python3 -V && pip3 -V pip -V Clone the Kubespray git repository: git clone https://github.com/kubernetes-sigs/kubespray.git cd kubespray Install dependencies from requirements.txt : pip install -r requirements.txt Copy inventory/sample as inventory/mycluster cp -rfp inventory/sample inventory/mycluster Update Ansible inventory file with inventory builder This step is little trivial because we need to update hosts.yml with the nodes IP. Now we are going to declare a variable \"IPS\" for storing the IP address of other K8s nodes .i.e. kubspray_master(192.168.0.130), kubspray_worker1(192.168.0.32) declare -a IPS=(192.168.0.130 192.168.0.32) CONFIG_FILE=inventory/mycluster/hosts.yml python3 \\ contrib/inventory_builder/inventory.py ${IPS[@]} DEBUG: Adding group all DEBUG: Adding group kube_control_plane DEBUG: Adding group kube_node DEBUG: Adding group etcd DEBUG: Adding group k8s_cluster DEBUG: Adding group calico_rr DEBUG: adding host node1 to group all DEBUG: adding host node2 to group all DEBUG: adding host node1 to group etcd DEBUG: adding host node1 to group kube_control_plane DEBUG: adding host node2 to group kube_control_plane DEBUG: adding host node1 to group kube_node DEBUG: adding host node2 to group kube_node After running the above commands do verify the hosts.yml and its content: cat inventory/mycluster/hosts.yml The content of the hosts.yml file should looks like: all: hosts: node1: ansible_host: 192.168.0.130 ip: 192.168.0.130 access_ip: 192.168.0.130 node2: ansible_host: 192.168.0.32 ip: 192.168.0.32 access_ip: 192.168.0.32 children: kube_control_plane: hosts: node1: node2: kube_node: hosts: node1: node2: etcd: hosts: node1: k8s_cluster: children: kube_control_plane: kube_node: calico_rr: hosts: {} Review and change parameters under inventory/mycluster/group_vars cat inventory/mycluster/group_vars/all/all.yml cat inventory/mycluster/group_vars/k8s_cluster/k8s-cluster.yml It can be useful to set the following two variables to true in inventory/mycluster/group_vars/k8s_cluster/k8s-cluster.yml : kubeconfig_localhost (to make a copy of kubeconfig on the host that runs Ansible in { inventory_dir }/artifacts ) and kubectl_localhost (to download kubectl onto the host that runs Ansible in { bin_dir } ). Very Important As Ubuntu 20 kvm kernel doesn't have dummy module we need to modify the following two variables in inventory/mycluster/group_vars/k8s_cluster/k8s-cluster.yml : enable_nodelocaldns: false and kube_proxy_mode: iptables which will Disable nodelocal dns cache and Kube-proxy proxyMode to iptables respectively. Deploy Kubespray with Ansible Playbook - run the playbook as root user. The option --become is required, as for example writing SSL keys in /etc/ , installing packages and interacting with various systemd daemons. Without --become the playbook will fail to run! ansible-playbook -i inventory/mycluster/hosts.yml --become --become-user=root cluster.yml Note Running ansible playbook takes little time because it depends on the network bandwidth also. Install kubectl on Kubernetes master node .i.e. kubspray_master Install kubectl binary snap install kubectl --classic This outputs: kubectl 1.22.2 from Canonical\u2713 installed Now verify the kubectl version: kubectl version -o yaml Validate all cluster components and nodes are visible on all nodes Verify the cluster kubectl get nodes NAME STATUS ROLES AGE VERSION node1 Ready control-plane,master 6m7s v1.23.3 node2 Ready control-plane,master 5m32s v1.23.3 Deploy A Hello Minikube Application Use the kubectl create command to create a Deployment that manages a Pod. The Pod runs a Container based on the provided Docker image. kubectl create deployment hello-minikube --image=k8s.gcr.io/echoserver:1.4 kubectl expose deployment hello-minikube --type=LoadBalancer --port=8080 service/hello-minikube exposed View the deployments information: kubectl get deployments NAME READY UP-TO-DATE AVAILABLE AGE hello-minikube 1/1 1 1 50s View the port information: kubectl get svc hello-minikube NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE hello-minikube LoadBalancer 10.233.35.126 <pending> 8080:30723/TCP 40s Expose the service locally kubectl port-forward svc/hello-minikube 30723:8080 Forwarding from [::1]:30723 -> 8080 Forwarding from 127.0.0.1:30723 -> 8080 Handling connection for 30723 Handling connection for 30723 Go to browser, visit http://<Master-Floating-IP>:8080 i.e. http://140.247.152.235:8080/ to check the hello minikube default page. Clean up Now you can clean up the app resources you created in your cluster: kubectl delete service my-nginx kubectl delete deployment my-nginx kubectl delete service hello-minikube kubectl delete deployment hello-minikube","title":"Kubespray"},{"location":"other-tools/kubernetes/kubespray/#kubespray","text":"","title":"Kubespray"},{"location":"other-tools/kubernetes/kubespray/#pre-requisite","text":"We will need 1 control-plane(master) and 1 worker node to create a single control-plane kubernetes cluster using Kubespray . We are using following setting for this purpose: 1 Linux machine for Ansible master, ubuntu-22.04-x86_64 or your choice of Ubuntu OS image, cpu-a.2 flavor with 2vCPU, 4GB RAM, 20GB. 1 Linux machine for master, ubuntu-22.04-x86_64 or your choice of Ubuntu OS image, cpu-a.2 flavor with 2vCPU, 4GB RAM, 20GB - also assign Floating IP to the master node. 1 Linux machines for worker, ubuntu-22.04-x86_64 or your choice of Ubuntu OS image, cpu-a.1 flavor with 1vCPU, 2GB RAM, 20GB storage. ssh access to all machines: Read more here on how to setup SSH to your remote VMs. To allow SSH from Ansible master to all other nodes : Read more here Generate SSH key for Ansible master node using: ssh-keygen -t rsa Generating public/private rsa key pair. Enter file in which to save the key (/root/.ssh/id_rsa): Enter passphrase (empty for no passphrase): Enter same passphrase again: Your identification has been saved in /root/.ssh/id_rsa Your public key has been saved in /root/.ssh/id_rsa.pub The key fingerprint is: SHA256:OMsKP7EmhT400AJA/KN1smKt6eTaa3QFQUiepmj8dxroot@ansible-master The key's randomart image is: +---[RSA 3072]----+ |=o.oo. | |.o... | |..= . | |=o.= ... | |o=+.=.o SE | |.+*o+. o. . | |.=== +o. . | |o+=o=.. | |++o=o. | +----[SHA256]-----+ Copy and append the content of SSH public key i.e. ~/.ssh/id_rsa.pub to other nodes's ~/.ssh/authorized_keys file. This will allow ssh <other_nodes_internal_ip> from the Ansible master node's terminal. Create 2 security groups with appropriate ports and protocols : i. To be used by the master nodes: ii. To be used by the worker nodes: - setup Unique hostname to each machine using the following command: echo \"<node_internal_IP> <host_name>\" >> /etc/hosts hostnamectl set-hostname <host_name> For example, echo \"192.168.0.224 ansible_master\" >> /etc/hosts hostnamectl set-hostname ansible_master In this step, you will update packages and disable swap on the all 3 nodes: 1 Ansible Master Node - ansible_master 1 Kubernetes Master Node - kubspray_master 1 Kubernetes Worker Node - kubspray_worker1 The below steps will be performed on all the above mentioned nodes: SSH into all the 3 machines Switch as root: sudo su Update the repositories and packages: apt-get update && apt-get upgrade -y Turn off swap swapoff -a sed -i '/ swap / s/^/#/' /etc/fstab","title":"Pre-requisite"},{"location":"other-tools/kubernetes/kubespray/#configure-kubespray-on-ansible_master-node-using-ansible-playbook","text":"Run the below command on the master node i.e. master that you want to setup as control plane. SSH into ansible_master machine Switch to root user: sudo su Execute the below command to initialize the cluster: Install Python3 and upgrade pip to pip3: apt install python3-pip -y pip3 install --upgrade pip python3 -V && pip3 -V pip -V Clone the Kubespray git repository: git clone https://github.com/kubernetes-sigs/kubespray.git cd kubespray Install dependencies from requirements.txt : pip install -r requirements.txt Copy inventory/sample as inventory/mycluster cp -rfp inventory/sample inventory/mycluster Update Ansible inventory file with inventory builder This step is little trivial because we need to update hosts.yml with the nodes IP. Now we are going to declare a variable \"IPS\" for storing the IP address of other K8s nodes .i.e. kubspray_master(192.168.0.130), kubspray_worker1(192.168.0.32) declare -a IPS=(192.168.0.130 192.168.0.32) CONFIG_FILE=inventory/mycluster/hosts.yml python3 \\ contrib/inventory_builder/inventory.py ${IPS[@]} DEBUG: Adding group all DEBUG: Adding group kube_control_plane DEBUG: Adding group kube_node DEBUG: Adding group etcd DEBUG: Adding group k8s_cluster DEBUG: Adding group calico_rr DEBUG: adding host node1 to group all DEBUG: adding host node2 to group all DEBUG: adding host node1 to group etcd DEBUG: adding host node1 to group kube_control_plane DEBUG: adding host node2 to group kube_control_plane DEBUG: adding host node1 to group kube_node DEBUG: adding host node2 to group kube_node After running the above commands do verify the hosts.yml and its content: cat inventory/mycluster/hosts.yml The content of the hosts.yml file should looks like: all: hosts: node1: ansible_host: 192.168.0.130 ip: 192.168.0.130 access_ip: 192.168.0.130 node2: ansible_host: 192.168.0.32 ip: 192.168.0.32 access_ip: 192.168.0.32 children: kube_control_plane: hosts: node1: node2: kube_node: hosts: node1: node2: etcd: hosts: node1: k8s_cluster: children: kube_control_plane: kube_node: calico_rr: hosts: {} Review and change parameters under inventory/mycluster/group_vars cat inventory/mycluster/group_vars/all/all.yml cat inventory/mycluster/group_vars/k8s_cluster/k8s-cluster.yml It can be useful to set the following two variables to true in inventory/mycluster/group_vars/k8s_cluster/k8s-cluster.yml : kubeconfig_localhost (to make a copy of kubeconfig on the host that runs Ansible in { inventory_dir }/artifacts ) and kubectl_localhost (to download kubectl onto the host that runs Ansible in { bin_dir } ). Very Important As Ubuntu 20 kvm kernel doesn't have dummy module we need to modify the following two variables in inventory/mycluster/group_vars/k8s_cluster/k8s-cluster.yml : enable_nodelocaldns: false and kube_proxy_mode: iptables which will Disable nodelocal dns cache and Kube-proxy proxyMode to iptables respectively. Deploy Kubespray with Ansible Playbook - run the playbook as root user. The option --become is required, as for example writing SSL keys in /etc/ , installing packages and interacting with various systemd daemons. Without --become the playbook will fail to run! ansible-playbook -i inventory/mycluster/hosts.yml --become --become-user=root cluster.yml Note Running ansible playbook takes little time because it depends on the network bandwidth also.","title":"Configure Kubespray on ansible_master node using Ansible Playbook"},{"location":"other-tools/kubernetes/kubespray/#install-kubectl-on-kubernetes-master-node-ie-kubspray_master","text":"Install kubectl binary snap install kubectl --classic This outputs: kubectl 1.22.2 from Canonical\u2713 installed Now verify the kubectl version: kubectl version -o yaml","title":"Install kubectl on Kubernetes master node .i.e. kubspray_master"},{"location":"other-tools/kubernetes/kubespray/#validate-all-cluster-components-and-nodes-are-visible-on-all-nodes","text":"Verify the cluster kubectl get nodes NAME STATUS ROLES AGE VERSION node1 Ready control-plane,master 6m7s v1.23.3 node2 Ready control-plane,master 5m32s v1.23.3","title":"Validate all cluster components and nodes are visible on all nodes"},{"location":"other-tools/kubernetes/kubespray/#deploy-a-hello-minikube-application","text":"Use the kubectl create command to create a Deployment that manages a Pod. The Pod runs a Container based on the provided Docker image. kubectl create deployment hello-minikube --image=k8s.gcr.io/echoserver:1.4 kubectl expose deployment hello-minikube --type=LoadBalancer --port=8080 service/hello-minikube exposed View the deployments information: kubectl get deployments NAME READY UP-TO-DATE AVAILABLE AGE hello-minikube 1/1 1 1 50s View the port information: kubectl get svc hello-minikube NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE hello-minikube LoadBalancer 10.233.35.126 <pending> 8080:30723/TCP 40s Expose the service locally kubectl port-forward svc/hello-minikube 30723:8080 Forwarding from [::1]:30723 -> 8080 Forwarding from 127.0.0.1:30723 -> 8080 Handling connection for 30723 Handling connection for 30723 Go to browser, visit http://<Master-Floating-IP>:8080 i.e. http://140.247.152.235:8080/ to check the hello minikube default page.","title":"Deploy A Hello Minikube Application"},{"location":"other-tools/kubernetes/kubespray/#clean-up","text":"Now you can clean up the app resources you created in your cluster: kubectl delete service my-nginx kubectl delete deployment my-nginx kubectl delete service hello-minikube kubectl delete deployment hello-minikube","title":"Clean up"},{"location":"other-tools/kubernetes/microk8s/","text":"Microk8s Pre-requisite We will need 1 VM to create a single node kubernetes cluster using microk8s . We are using following setting for this purpose: 1 Linux machine, ubuntu-22.04-x86_64 or your choice of Ubuntu OS image, cpu-a.2 flavor with 2vCPU, 4GB RAM, 20GB storage - also assign Floating IP to this VM. setup Unique hostname to the machine using the following command: echo \"<node_internal_IP> <host_name>\" >> /etc/hosts hostnamectl set-hostname <host_name> For example, echo \"192.168.0.62 microk8s\" >> /etc/hosts hostnamectl set-hostname microk8s Install MicroK8s on Ubuntu Run the below command on the Ubuntu VM: SSH into microk8s machine Switch to root user: sudo su Update the repositories and packages: apt-get update && apt-get upgrade -y Install MicroK8s: sudo snap install microk8s --classic Check the status while Kubernetes starts microk8s status --wait-ready Turn on the services you want: microk8s enable dns dashboard Try microk8s enable --help for a list of available services and optional features. microk8s disable <name> turns off a service. For example other useful services are: microk8s enable registry istio storage Start using Kubernetes microk8s kubectl get all --all-namespaces If you mainly use MicroK8s you can make our kubectl the default one on your command-line with alias mkctl=\"microk8s kubectl\" . Since it is a standard upstream kubectl, you can also drive other Kubernetes clusters with it by pointing to the respective kubeconfig file via the --kubeconfig argument. Access the Kubernetes dashboard UI: As we see above the kubernetes-dashboard service in the kube-system namespace has a ClusterIP of 10.152.183.73 and listens on TCP port 443. The ClusterIP is randomly assigned, so if you follow these steps on your host, make sure you check the IP adress you got. Note Another way to access the default token to be used for the dashboard access can be retrieved with: Keep running the kubernetes-dashboad on Proxy to access it via web browser: microk8s dashboard-proxy Checking if Dashboard is running. Dashboard will be available at https://127.0.0.1:10443 Use the following token to login: eyJhbGc.... Important This tells us the IP address of the Dashboard and the port. The values assigned to your Dashboard will differ. Please note the displayed PORT and also the TOKEN that are required to access the kubernetes-dashboard. Make sure, the exposed PORT is opened in Security Groups for the instance following this guide . This will show the token to login to the Dashbord shown on the url with NodePort. You\u2019ll need to wait a few minutes before the dashboard becomes available. If you open a web browser on the same desktop you deployed Microk8s and point it to https://<Floating-IP>:<PORT> (where PORT is the PORT assigned to the Dashboard noted while running the above command), you\u2019ll need to accept the risk (because the Dashboard uses a self-signed certificate). And, we can enter the previously noted TOKEN to access the kubernetes-dashboard. Once entered correct TOKEN the kubernetes-dashboard is accessed and looks like below: Information Start and stop Kubernetes: Kubernetes is a collection of system services that talk to each other all the time. If you don\u2019t need them running in the background then you will save battery by stopping them. microk8s start and microk8s stop will those tasks for you. To Reset the infrastructure to a clean state: microk8s reset Deploy a Container using the Kubernetes-Dashboard Click on the + button in the top left corner of the main window. On the resulting page, click Create from form and then fill out the necessary information as shown below: You should immediately be directed to a page that lists your new deployment as shown below: Go back to the terminal window and issue the command: microk8s kubectl get svc tns -n kube-system NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE tns LoadBalancer 10.152.183.90 <pending> 8080:30012/TCP 14m Go to browser, visit http://<Floating-IP>:<NodePort> i.e. http://128.31.26.4:30012/ to check the nginx default page. Deploy A Sample Nginx Application Create an alias: alias mkctl=\"microk8s kubectl\" Create a deployment, in this case Nginx : mkctl create deployment --image nginx my-nginx To access the deployment we will need to expose it: mkctl expose deployment my-nginx --port=80 --type=NodePort mkctl get svc my-nginx NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE my-nginx NodePort 10.152.183.41 <none> 80:31225/TCP 35h Go to browser, visit http://<Floating-IP>:<NodePort> i.e. http://128.31.26.4:31225/ to check the nginx default page. Deploy Another Application You can start by creating a microbot deployment with two pods via the kubectl cli: mkctl create deployment microbot --image=dontrebootme/microbot:v1 mkctl scale deployment microbot --replicas=2 To expose the deployment to NodePort, you need to create a service: mkctl expose deployment microbot --type=NodePort --port=80 --name=microbot-service View the port information: mkctl get svc microbot-service NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE microbot-service NodePort 10.152.183.8 <none> 80:31442/TCP 35h Go to browser, visit http://<Floating-IP>:<NodePort> i.e. http://128.31.26.4:31442/ to check the microbot default page.","title":"MicroK8s"},{"location":"other-tools/kubernetes/microk8s/#microk8s","text":"","title":"Microk8s"},{"location":"other-tools/kubernetes/microk8s/#pre-requisite","text":"We will need 1 VM to create a single node kubernetes cluster using microk8s . We are using following setting for this purpose: 1 Linux machine, ubuntu-22.04-x86_64 or your choice of Ubuntu OS image, cpu-a.2 flavor with 2vCPU, 4GB RAM, 20GB storage - also assign Floating IP to this VM. setup Unique hostname to the machine using the following command: echo \"<node_internal_IP> <host_name>\" >> /etc/hosts hostnamectl set-hostname <host_name> For example, echo \"192.168.0.62 microk8s\" >> /etc/hosts hostnamectl set-hostname microk8s","title":"Pre-requisite"},{"location":"other-tools/kubernetes/microk8s/#install-microk8s-on-ubuntu","text":"Run the below command on the Ubuntu VM: SSH into microk8s machine Switch to root user: sudo su Update the repositories and packages: apt-get update && apt-get upgrade -y Install MicroK8s: sudo snap install microk8s --classic Check the status while Kubernetes starts microk8s status --wait-ready Turn on the services you want: microk8s enable dns dashboard Try microk8s enable --help for a list of available services and optional features. microk8s disable <name> turns off a service. For example other useful services are: microk8s enable registry istio storage Start using Kubernetes microk8s kubectl get all --all-namespaces If you mainly use MicroK8s you can make our kubectl the default one on your command-line with alias mkctl=\"microk8s kubectl\" . Since it is a standard upstream kubectl, you can also drive other Kubernetes clusters with it by pointing to the respective kubeconfig file via the --kubeconfig argument. Access the Kubernetes dashboard UI: As we see above the kubernetes-dashboard service in the kube-system namespace has a ClusterIP of 10.152.183.73 and listens on TCP port 443. The ClusterIP is randomly assigned, so if you follow these steps on your host, make sure you check the IP adress you got. Note Another way to access the default token to be used for the dashboard access can be retrieved with: Keep running the kubernetes-dashboad on Proxy to access it via web browser: microk8s dashboard-proxy Checking if Dashboard is running. Dashboard will be available at https://127.0.0.1:10443 Use the following token to login: eyJhbGc.... Important This tells us the IP address of the Dashboard and the port. The values assigned to your Dashboard will differ. Please note the displayed PORT and also the TOKEN that are required to access the kubernetes-dashboard. Make sure, the exposed PORT is opened in Security Groups for the instance following this guide . This will show the token to login to the Dashbord shown on the url with NodePort. You\u2019ll need to wait a few minutes before the dashboard becomes available. If you open a web browser on the same desktop you deployed Microk8s and point it to https://<Floating-IP>:<PORT> (where PORT is the PORT assigned to the Dashboard noted while running the above command), you\u2019ll need to accept the risk (because the Dashboard uses a self-signed certificate). And, we can enter the previously noted TOKEN to access the kubernetes-dashboard. Once entered correct TOKEN the kubernetes-dashboard is accessed and looks like below: Information Start and stop Kubernetes: Kubernetes is a collection of system services that talk to each other all the time. If you don\u2019t need them running in the background then you will save battery by stopping them. microk8s start and microk8s stop will those tasks for you. To Reset the infrastructure to a clean state: microk8s reset","title":"Install MicroK8s on Ubuntu"},{"location":"other-tools/kubernetes/microk8s/#deploy-a-container-using-the-kubernetes-dashboard","text":"Click on the + button in the top left corner of the main window. On the resulting page, click Create from form and then fill out the necessary information as shown below: You should immediately be directed to a page that lists your new deployment as shown below: Go back to the terminal window and issue the command: microk8s kubectl get svc tns -n kube-system NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE tns LoadBalancer 10.152.183.90 <pending> 8080:30012/TCP 14m Go to browser, visit http://<Floating-IP>:<NodePort> i.e. http://128.31.26.4:30012/ to check the nginx default page.","title":"Deploy a Container using the Kubernetes-Dashboard"},{"location":"other-tools/kubernetes/microk8s/#deploy-a-sample-nginx-application","text":"Create an alias: alias mkctl=\"microk8s kubectl\" Create a deployment, in this case Nginx : mkctl create deployment --image nginx my-nginx To access the deployment we will need to expose it: mkctl expose deployment my-nginx --port=80 --type=NodePort mkctl get svc my-nginx NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE my-nginx NodePort 10.152.183.41 <none> 80:31225/TCP 35h Go to browser, visit http://<Floating-IP>:<NodePort> i.e. http://128.31.26.4:31225/ to check the nginx default page.","title":"Deploy A Sample Nginx Application"},{"location":"other-tools/kubernetes/microk8s/#deploy-another-application","text":"You can start by creating a microbot deployment with two pods via the kubectl cli: mkctl create deployment microbot --image=dontrebootme/microbot:v1 mkctl scale deployment microbot --replicas=2 To expose the deployment to NodePort, you need to create a service: mkctl expose deployment microbot --type=NodePort --port=80 --name=microbot-service View the port information: mkctl get svc microbot-service NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE microbot-service NodePort 10.152.183.8 <none> 80:31442/TCP 35h Go to browser, visit http://<Floating-IP>:<NodePort> i.e. http://128.31.26.4:31442/ to check the microbot default page.","title":"Deploy Another Application"},{"location":"other-tools/kubernetes/minikube/","text":"Minikube Minimum system requirements for minikube 2 GB RAM or more 2 CPU / vCPUs or more 20 GB free hard disk space or more Docker / Virtual Machine Manager \u2013 KVM & VirtualBox. Docker, Hyperkit, Hyper-V, KVM, Parallels, Podman, VirtualBox, or VMWare are examples of container or virtual machine managers. Pre-requisite We will need 1 VM to create a single node kubernetes cluster using minikube . We are using following setting for this purpose: 1 Linux machine for master, ubuntu-22.04-x86_64 or your choice of Ubuntu OS image, cpu-a.2 flavor with 2vCPU, 4GB RAM, 20GB storage - also assign Floating IP to this VM. setup Unique hostname to the machine using the following command: echo \"<node_internal_IP> <host_name>\" >> /etc/hosts hostnamectl set-hostname <host_name> For example, echo \"192.168.0.62 minikube\" >> /etc/hosts hostnamectl set-hostname minikube Install Minikube on Ubuntu Run the below command on the Ubuntu VM: SSH into minikube machine Switch to root user: sudo su Update the repositories and packages: apt-get update && apt-get upgrade -y Install curl , wget , and apt-transport-https apt-get update && apt-get install -y curl wget apt-transport-https Install Docker Install container runtime - docker sudo apt-get install docker.io -y Configure the Docker daemon, in particular to use systemd for the management of the container\u2019s cgroups cat <<EOF | sudo tee /etc/docker/daemon.json { \"exec-opts\": [\"native.cgroupdriver=systemd\"] } EOF systemctl enable --now docker usermod -aG docker ubuntu systemctl daemon-reload systemctl restart docker OR, you can install VirtualBox Hypervisor as runtime: sudo apt install virtualbox virtualbox-ext-pack -y Install kubectl Install kubectl binary \u2022 kubectl : the command line util to talk to your cluster. snap install kubectl --classic This outputs: kubectl 1.22.2 from Canonical\u2713 installed Now verify the kubectl version: kubectl version -o yaml Installing minikube Install minikube curl -LO https://storage.googleapis.com/minikube/releases/latest/minikube_latest_amd64.deb sudo dpkg -i minikube_latest_amd64.deb OR, install minikube using wget : wget https://storage.googleapis.com/minikube/releases/latest/minikube-linux-amd64 cp minikube-linux-amd64 /usr/bin/minikube chmod +x /usr/bin/minikube Verify the Minikube installation: minikube version minikube version: v1.23.2 commit: 0a0ad764652082477c00d51d2475284b5d39ceed Install conntrack: Kubernetes 1.22.2 requires conntrack to be installed in root's path: apt-get install -y conntrack Start minikube: As we are already stated in the beginning that we would be using docker as base for minikue, so start the minikube with the docker driver, minikube start --driver=none Note To check the internal IP, run the minikube ip command. By default, Minikube uses the driver most relevant to the host OS. To use a different driver, set the --driver flag in minikube start . For example, to use Docker instead of others or none, run minikube start --driver=docker . To persistent configuration so that you to run minikube start without explicitly passing i.e. in global scope the --vm-driver docker flag each time, run: minikube config set vm-driver docker . In case you want to start minikube with customize resources and want installer to automatically select the driver then you can run following command, minikube start --addons=ingress --cpus=2 --cni=flannel --install-addons=true --kubernetes-version=stable --memory=6g Output would like below: Perfect, above confirms that minikube cluster has been configured and started successfully. Run below minikube command to check status: minikube status minikube type: Control Plane host: Running kubelet: Running apiserver: Running kubeconfig: Configured Run following kubectl command to verify the cluster info and node status: kubectl cluster-info Kubernetes control plane is running at https://192.168.0.62:8443 CoreDNS is running at https://192.168.0.62:8443/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy To further debug and diagnose cluster problems, use 'kubectl cluster-info dump'. kubectl get nodes NAME STATUS ROLES AGE VERSION minikube Ready control-plane,master 5m v1.22.2 To see the kubectl configuration use the command: kubectl config view The output looks like: Get minikube addon details: minikube addons list The output will display like below: If you wish to enable any addons run the below minikube command, minikube addons enable <addon-name> Enable minikube dashboard addon: minikube dashboard \ud83d\udd0c Enabling dashboard ... \u25aa Using image kubernetesui/metrics-scraper:v1.0.7 \u25aa Using image kubernetesui/dashboard:v2.3.1 \ud83e\udd14 Verifying dashboard health ... \ud83d\ude80 Launching proxy ... \ud83e\udd14 Verifying proxy health ... http://127.0.0.1:40783/api/v1/namespaces/kubernetes-dashboard/services/http:kubernetes-dashboard:/proxy/ To view minikube dashboard url: minikube dashboard --url \ud83e\udd14 Verifying dashboard health ... \ud83d\ude80 Launching proxy ... \ud83e\udd14 Verifying proxy health ... http://127.0.0.1:42669/api/v1/namespaces/kubernetes-dashboard/services/http:kubernetes-dashboard:/proxy/ Expose Dashboard on NodePort instead of ClusterIP : -- Check the current port for kubernetes-dashboard : kubectl get services -n kubernetes-dashboard The output looks like below: kubectl edit service kubernetes-dashboard -n kubernetes-dashboard -- Replace type: \"ClusterIP\" to \"NodePort\": -- After saving the file: Test again: kubectl get services -n kubernetes-dashboard Now the output should look like below: So, now you can browser the K8s Dashboard, visit http://<Floating-IP>:<NodePort> i.e. http://140.247.152.235:31881 to view the Dashboard. Deploy A Sample Nginx Application Create a deployment, in this case Nginx : A Kubernetes Pod is a group of one or more Containers, tied together for the purposes of administration and networking. The Pod in this tutorial has only one Container. A Kubernetes Deployment checks on the health of your Pod and restarts the Pod's Container if it terminates. Deployments are the recommended way to manage the creation and scaling of Pods. Let's check if the Kubernetes cluster is up and running: kubectl get all --all-namespaces kubectl get po -A kubectl get nodes kubectl create deployment --image nginx my-nginx To access the deployment we will need to expose it: kubectl expose deployment my-nginx --port=80 --type=NodePort To check which NodePort is opened and running the Nginx run: kubectl get svc The output will show: OR, minikube service list |----------------------|---------------------------|--------------|-------------| | NAMESPACE | NAME | TARGET PORT | URL | |----------------------|---------------------------|--------------|-------------| | default | kubernetes | No node port | | default | my-nginx | 80 | http:.:31081| | kube-system | kube-dns | No node port | | kubernetes-dashboard | dashboard-metrics-scraper | No node port | | kubernetes-dashboard | kubernetes-dashboard | 80 | http:.:31929| |----------------------|---------------------------|--------------|-------------| OR, kubectl get svc my-nginx minikube service my-nginx --url Once the deployment is up, you should be able to access the Nginx home page on the allocated NodePort from the node's Floating IP. Go to browser, visit http://<Floating-IP>:<NodePort> i.e. http://140.247.152.235:31081/ to check the nginx default page. For your example, Deploy A Hello Minikube Application Use the kubectl create command to create a Deployment that manages a Pod. The Pod runs a Container based on the provided Docker image. kubectl create deployment hello-minikube --image=k8s.gcr.io/echoserver:1.4 kubectl expose deployment hello-minikube --type=NodePort --port=8080 View the port information: kubectl get svc hello-minikube minikube service hello-minikube --url Go to browser, visit http://<Floating-IP>:<NodePort> i.e. http://140.247.152.235:31293/ to check the hello minikube default page. For your example, Clean up Now you can clean up the app resources you created in your cluster: kubectl delete service my-nginx kubectl delete deployment my-nginx kubectl delete service hello-minikube kubectl delete deployment hello-minikube Managing Minikube Cluster To stop the minikube, run minikube stop To delete the single node cluster: minikube delete To Start the minikube, run minikube start In case you want to start the minikube with higher resource like 8 GB RM and 4 CPU then execute following commands one after the another. minikube config set cpus 4 minikube config set memory 8192 minikube delete minikube start","title":"Minikube"},{"location":"other-tools/kubernetes/minikube/#minikube","text":"","title":"Minikube"},{"location":"other-tools/kubernetes/minikube/#minimum-system-requirements-for-minikube","text":"2 GB RAM or more 2 CPU / vCPUs or more 20 GB free hard disk space or more Docker / Virtual Machine Manager \u2013 KVM & VirtualBox. Docker, Hyperkit, Hyper-V, KVM, Parallels, Podman, VirtualBox, or VMWare are examples of container or virtual machine managers.","title":"Minimum system requirements for minikube"},{"location":"other-tools/kubernetes/minikube/#pre-requisite","text":"We will need 1 VM to create a single node kubernetes cluster using minikube . We are using following setting for this purpose: 1 Linux machine for master, ubuntu-22.04-x86_64 or your choice of Ubuntu OS image, cpu-a.2 flavor with 2vCPU, 4GB RAM, 20GB storage - also assign Floating IP to this VM. setup Unique hostname to the machine using the following command: echo \"<node_internal_IP> <host_name>\" >> /etc/hosts hostnamectl set-hostname <host_name> For example, echo \"192.168.0.62 minikube\" >> /etc/hosts hostnamectl set-hostname minikube","title":"Pre-requisite"},{"location":"other-tools/kubernetes/minikube/#install-minikube-on-ubuntu","text":"Run the below command on the Ubuntu VM: SSH into minikube machine Switch to root user: sudo su Update the repositories and packages: apt-get update && apt-get upgrade -y Install curl , wget , and apt-transport-https apt-get update && apt-get install -y curl wget apt-transport-https","title":"Install Minikube on Ubuntu"},{"location":"other-tools/kubernetes/minikube/#install-docker","text":"Install container runtime - docker sudo apt-get install docker.io -y Configure the Docker daemon, in particular to use systemd for the management of the container\u2019s cgroups cat <<EOF | sudo tee /etc/docker/daemon.json { \"exec-opts\": [\"native.cgroupdriver=systemd\"] } EOF systemctl enable --now docker usermod -aG docker ubuntu systemctl daemon-reload systemctl restart docker OR, you can install VirtualBox Hypervisor as runtime: sudo apt install virtualbox virtualbox-ext-pack -y","title":"Install Docker"},{"location":"other-tools/kubernetes/minikube/#install-kubectl","text":"Install kubectl binary \u2022 kubectl : the command line util to talk to your cluster. snap install kubectl --classic This outputs: kubectl 1.22.2 from Canonical\u2713 installed Now verify the kubectl version: kubectl version -o yaml","title":"Install kubectl"},{"location":"other-tools/kubernetes/minikube/#installing-minikube","text":"Install minikube curl -LO https://storage.googleapis.com/minikube/releases/latest/minikube_latest_amd64.deb sudo dpkg -i minikube_latest_amd64.deb OR, install minikube using wget : wget https://storage.googleapis.com/minikube/releases/latest/minikube-linux-amd64 cp minikube-linux-amd64 /usr/bin/minikube chmod +x /usr/bin/minikube Verify the Minikube installation: minikube version minikube version: v1.23.2 commit: 0a0ad764652082477c00d51d2475284b5d39ceed Install conntrack: Kubernetes 1.22.2 requires conntrack to be installed in root's path: apt-get install -y conntrack Start minikube: As we are already stated in the beginning that we would be using docker as base for minikue, so start the minikube with the docker driver, minikube start --driver=none Note To check the internal IP, run the minikube ip command. By default, Minikube uses the driver most relevant to the host OS. To use a different driver, set the --driver flag in minikube start . For example, to use Docker instead of others or none, run minikube start --driver=docker . To persistent configuration so that you to run minikube start without explicitly passing i.e. in global scope the --vm-driver docker flag each time, run: minikube config set vm-driver docker . In case you want to start minikube with customize resources and want installer to automatically select the driver then you can run following command, minikube start --addons=ingress --cpus=2 --cni=flannel --install-addons=true --kubernetes-version=stable --memory=6g Output would like below: Perfect, above confirms that minikube cluster has been configured and started successfully. Run below minikube command to check status: minikube status minikube type: Control Plane host: Running kubelet: Running apiserver: Running kubeconfig: Configured Run following kubectl command to verify the cluster info and node status: kubectl cluster-info Kubernetes control plane is running at https://192.168.0.62:8443 CoreDNS is running at https://192.168.0.62:8443/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy To further debug and diagnose cluster problems, use 'kubectl cluster-info dump'. kubectl get nodes NAME STATUS ROLES AGE VERSION minikube Ready control-plane,master 5m v1.22.2 To see the kubectl configuration use the command: kubectl config view The output looks like: Get minikube addon details: minikube addons list The output will display like below: If you wish to enable any addons run the below minikube command, minikube addons enable <addon-name> Enable minikube dashboard addon: minikube dashboard \ud83d\udd0c Enabling dashboard ... \u25aa Using image kubernetesui/metrics-scraper:v1.0.7 \u25aa Using image kubernetesui/dashboard:v2.3.1 \ud83e\udd14 Verifying dashboard health ... \ud83d\ude80 Launching proxy ... \ud83e\udd14 Verifying proxy health ... http://127.0.0.1:40783/api/v1/namespaces/kubernetes-dashboard/services/http:kubernetes-dashboard:/proxy/ To view minikube dashboard url: minikube dashboard --url \ud83e\udd14 Verifying dashboard health ... \ud83d\ude80 Launching proxy ... \ud83e\udd14 Verifying proxy health ... http://127.0.0.1:42669/api/v1/namespaces/kubernetes-dashboard/services/http:kubernetes-dashboard:/proxy/ Expose Dashboard on NodePort instead of ClusterIP : -- Check the current port for kubernetes-dashboard : kubectl get services -n kubernetes-dashboard The output looks like below: kubectl edit service kubernetes-dashboard -n kubernetes-dashboard -- Replace type: \"ClusterIP\" to \"NodePort\": -- After saving the file: Test again: kubectl get services -n kubernetes-dashboard Now the output should look like below: So, now you can browser the K8s Dashboard, visit http://<Floating-IP>:<NodePort> i.e. http://140.247.152.235:31881 to view the Dashboard.","title":"Installing minikube"},{"location":"other-tools/kubernetes/minikube/#deploy-a-sample-nginx-application","text":"Create a deployment, in this case Nginx : A Kubernetes Pod is a group of one or more Containers, tied together for the purposes of administration and networking. The Pod in this tutorial has only one Container. A Kubernetes Deployment checks on the health of your Pod and restarts the Pod's Container if it terminates. Deployments are the recommended way to manage the creation and scaling of Pods. Let's check if the Kubernetes cluster is up and running: kubectl get all --all-namespaces kubectl get po -A kubectl get nodes kubectl create deployment --image nginx my-nginx To access the deployment we will need to expose it: kubectl expose deployment my-nginx --port=80 --type=NodePort To check which NodePort is opened and running the Nginx run: kubectl get svc The output will show: OR, minikube service list |----------------------|---------------------------|--------------|-------------| | NAMESPACE | NAME | TARGET PORT | URL | |----------------------|---------------------------|--------------|-------------| | default | kubernetes | No node port | | default | my-nginx | 80 | http:.:31081| | kube-system | kube-dns | No node port | | kubernetes-dashboard | dashboard-metrics-scraper | No node port | | kubernetes-dashboard | kubernetes-dashboard | 80 | http:.:31929| |----------------------|---------------------------|--------------|-------------| OR, kubectl get svc my-nginx minikube service my-nginx --url Once the deployment is up, you should be able to access the Nginx home page on the allocated NodePort from the node's Floating IP. Go to browser, visit http://<Floating-IP>:<NodePort> i.e. http://140.247.152.235:31081/ to check the nginx default page. For your example,","title":"Deploy A Sample Nginx Application"},{"location":"other-tools/kubernetes/minikube/#deploy-a-hello-minikube-application","text":"Use the kubectl create command to create a Deployment that manages a Pod. The Pod runs a Container based on the provided Docker image. kubectl create deployment hello-minikube --image=k8s.gcr.io/echoserver:1.4 kubectl expose deployment hello-minikube --type=NodePort --port=8080 View the port information: kubectl get svc hello-minikube minikube service hello-minikube --url Go to browser, visit http://<Floating-IP>:<NodePort> i.e. http://140.247.152.235:31293/ to check the hello minikube default page. For your example,","title":"Deploy A Hello Minikube Application"},{"location":"other-tools/kubernetes/minikube/#clean-up","text":"Now you can clean up the app resources you created in your cluster: kubectl delete service my-nginx kubectl delete deployment my-nginx kubectl delete service hello-minikube kubectl delete deployment hello-minikube","title":"Clean up"},{"location":"other-tools/kubernetes/minikube/#managing-minikube-cluster","text":"To stop the minikube, run minikube stop To delete the single node cluster: minikube delete To Start the minikube, run minikube start In case you want to start the minikube with higher resource like 8 GB RM and 4 CPU then execute following commands one after the another. minikube config set cpus 4 minikube config set memory 8192 minikube delete minikube start","title":"Managing Minikube Cluster"},{"location":"other-tools/kubernetes/minishift/","text":"Minishift What Is the OKD Architecture? OKD is a platform for developing and running containerized applications. OKD has a microservices-based architecture of smaller, decoupled units that work together. It runs on top of a Kubernetes cluster, with data about the objects stored in etcd, a reliable clustered key-value store. Those services are broken down by function: REST APIs, which expose each of the core objects. Controllers, which read those APIs, apply changes to other objects, and report status or write back to the object. For more information on the node types in the architecture overview, see Kubernetes Infrastructure . Minishift Quickstart OKD also offers a comprehensive web console and the custom OpenShift CLI (oc) interface. The interaction with OpenShift is with the command line tool oc which is copied to your host. Pre-requisite We will need 1 VM to create a single node kubernetes cluster using minishift . We are using following setting for this purpose: 1 Linux machine, ubuntu-22.04-x86_64 or your choice of Ubuntu OS image, cpu-a.4 flavor with 4vCPU, 8GB RAM, 20GB storage - also assign Floating IP to this VM. setup Unique hostname to the machine using the following command: echo \"<node_internal_IP> <host_name>\" >> /etc/hosts hostnamectl set-hostname <host_name> For example, echo \"192.168.0.134 minishift\" >> /etc/hosts hostnamectl set-hostname minishift Install MiniShift on Ubuntu Run the below command on the Ubuntu VM: SSH into minishift machine Switch to root user: sudo su You will need to set up the virtualization environment before installing MiniShift. Update the repositories and packages: apt-get update && apt-get upgrade -y Install libvirt and qemu-kvm on your system: apt install qemu-kvm libvirt-daemon libvirt-daemon-system -y Create group if does not exist: addgroup libvirtd adduser $(whoami) libvirtd Add yourself to the libvirt(d) group: usermod -a -G libvirt $(whoami) OR, usermod -a -G libvirt $USER Update your current session to apply the group change: newgrp libvirtd As root, install the KVM driver binary and make it executable as follows: curl -L https://github.com/dhiltgen/docker-machine-kvm/releases/download/v0.10.0/docker-machine-driver-kvm-ubuntu16.04 -o /usr/bin/docker-machine-driver-kvm chmod +x /usr/bin/docker-machine-driver-kvm Check the status of libvirtd : systemctl is-active libvirtd If libvirtd is not active, start the libvirtd service: systemctl start libvirtd Download the archive for your operating system from the Minishift Releases page. curl -LO https://github.com/minishift/minishift/releases/download/v1.34.3/minishift-1.34.3-linux-amd64.tgz Unzip and Copy MiniShift in your path tar -zxvf minishift-1.34.3-linux-amd64.tgz --strip=1 -C/usr/bin minishift-1.34.3-linux-amd64/minishift Starting Minishift By running the command: minishift start !!! note \"Note\": cpu, and start memory, If you do not specify a disk-size 2vCPU, 4GB memory, with 20GB disk. Other customized minishift can be started following this format: minishift start --openshift-version v3.11.0 --iso-url centos --cpus 4 \\ --memory 12GB --disk-size 60GB minishift start -- Starting local OpenShift cluster using 'kvm' hypervisor... ... OpenShift server started. The server is accessible via web console at: https://192.168.42.226:8443/console You are logged in as: User: developer Password: <any value> #pragma: allowlist secret To login as administrator: oc login -u system:admin Note The IP is dynamically generated for each OpenShift cluster. To check the IP, run the minishift ip command. By default, Minishift uses the driver most relevant to the host OS. To use a different driver, set the --vm-driver flag in minishift start . For example, to use VirtualBox instead of KVM on Linux operating systems, run minishift start --vm-driver=virtualbox . To persistent configuration so that you to run minishift start without explicitly passing i.e. in global scope the --vm-driver virtualbox flag each time, run: minishift config set vm-driver virtualbox . You can run this command in a shell after starting Minishift to get the URL of the Web console: minishift console --url Use minishift oc-env to display the command you need to type into your shell in order to add the oc binary to your PATH environment variable. The output of oc-env will differ depending on OS and shell type. $ minishift oc-env export PATH=\"/root/.minishift/cache/oc/v3.11.0/linux:$PATH\" # Run this command to configure your shell: # eval $(minishift oc-env) Deploying a Sample Application OpenShift provides various sample applications, such as templates, builder applications, and quickstarts. The following steps describe how to deploy a sample Node.js application from the command line. Create a Node.js example app: oc new-app https://github.com/sclorg/nodejs-ex -l name=myapp Track the build log until the app is built and deployed: oc logs -f bc/nodejs-ex Expose a route to the service: oc expose svc/nodejs-ex Access the application: minishift openshift service nodejs-ex --in-browser To stop Minishift, use the following command: minishift stop Stopping local OpenShift cluster... Stopping \"minishift\"... Updating Minishift minishift update Uninstalling Minishift Delete the Minishift VM and any VM-specific files: minishift delete This command deletes everything in the $MINISHIFT_HOME/.minishift/machines/minishift directory. Other cached data and the persistent configuration are not removed. To completely uninstall Minishift, delete everything in the MINISHIFT_HOME directory (default ~/.minishift ) and ~/.kube : rm -rf ~/.minishift rm -rf ~/.kube With your hypervisor management tool, confirm that there are no remaining artifacts related to the Minishift VM. For example, if you use KVM, you need to run the virsh command. Connecting to the Minishift VM with SSH You can use the minishift ssh command to interact with the Minishift VM. minishift ssh -- docker ps CONTAINER IMAGE COMMAND CREATED STATUS NAMES 71fe8ff16548 openshift/origin:... \"/usr/bin/openshift s\" 1 sec ago Up 1 second origin","title":"Minishift"},{"location":"other-tools/kubernetes/minishift/#minishift","text":"","title":"Minishift"},{"location":"other-tools/kubernetes/minishift/#what-is-the-okd-architecture","text":"OKD is a platform for developing and running containerized applications. OKD has a microservices-based architecture of smaller, decoupled units that work together. It runs on top of a Kubernetes cluster, with data about the objects stored in etcd, a reliable clustered key-value store. Those services are broken down by function: REST APIs, which expose each of the core objects. Controllers, which read those APIs, apply changes to other objects, and report status or write back to the object. For more information on the node types in the architecture overview, see Kubernetes Infrastructure .","title":"What Is the OKD Architecture?"},{"location":"other-tools/kubernetes/minishift/#minishift-quickstart","text":"OKD also offers a comprehensive web console and the custom OpenShift CLI (oc) interface. The interaction with OpenShift is with the command line tool oc which is copied to your host.","title":"Minishift Quickstart"},{"location":"other-tools/kubernetes/minishift/#pre-requisite","text":"We will need 1 VM to create a single node kubernetes cluster using minishift . We are using following setting for this purpose: 1 Linux machine, ubuntu-22.04-x86_64 or your choice of Ubuntu OS image, cpu-a.4 flavor with 4vCPU, 8GB RAM, 20GB storage - also assign Floating IP to this VM. setup Unique hostname to the machine using the following command: echo \"<node_internal_IP> <host_name>\" >> /etc/hosts hostnamectl set-hostname <host_name> For example, echo \"192.168.0.134 minishift\" >> /etc/hosts hostnamectl set-hostname minishift","title":"Pre-requisite"},{"location":"other-tools/kubernetes/minishift/#install-minishift-on-ubuntu","text":"Run the below command on the Ubuntu VM: SSH into minishift machine Switch to root user: sudo su You will need to set up the virtualization environment before installing MiniShift. Update the repositories and packages: apt-get update && apt-get upgrade -y Install libvirt and qemu-kvm on your system: apt install qemu-kvm libvirt-daemon libvirt-daemon-system -y Create group if does not exist: addgroup libvirtd adduser $(whoami) libvirtd Add yourself to the libvirt(d) group: usermod -a -G libvirt $(whoami) OR, usermod -a -G libvirt $USER Update your current session to apply the group change: newgrp libvirtd As root, install the KVM driver binary and make it executable as follows: curl -L https://github.com/dhiltgen/docker-machine-kvm/releases/download/v0.10.0/docker-machine-driver-kvm-ubuntu16.04 -o /usr/bin/docker-machine-driver-kvm chmod +x /usr/bin/docker-machine-driver-kvm Check the status of libvirtd : systemctl is-active libvirtd If libvirtd is not active, start the libvirtd service: systemctl start libvirtd Download the archive for your operating system from the Minishift Releases page. curl -LO https://github.com/minishift/minishift/releases/download/v1.34.3/minishift-1.34.3-linux-amd64.tgz Unzip and Copy MiniShift in your path tar -zxvf minishift-1.34.3-linux-amd64.tgz --strip=1 -C/usr/bin minishift-1.34.3-linux-amd64/minishift","title":"Install MiniShift on Ubuntu"},{"location":"other-tools/kubernetes/minishift/#starting-minishift","text":"By running the command: minishift start !!! note \"Note\": cpu, and start memory, If you do not specify a disk-size 2vCPU, 4GB memory, with 20GB disk. Other customized minishift can be started following this format: minishift start --openshift-version v3.11.0 --iso-url centos --cpus 4 \\ --memory 12GB --disk-size 60GB minishift start -- Starting local OpenShift cluster using 'kvm' hypervisor... ... OpenShift server started. The server is accessible via web console at: https://192.168.42.226:8443/console You are logged in as: User: developer Password: <any value> #pragma: allowlist secret To login as administrator: oc login -u system:admin Note The IP is dynamically generated for each OpenShift cluster. To check the IP, run the minishift ip command. By default, Minishift uses the driver most relevant to the host OS. To use a different driver, set the --vm-driver flag in minishift start . For example, to use VirtualBox instead of KVM on Linux operating systems, run minishift start --vm-driver=virtualbox . To persistent configuration so that you to run minishift start without explicitly passing i.e. in global scope the --vm-driver virtualbox flag each time, run: minishift config set vm-driver virtualbox . You can run this command in a shell after starting Minishift to get the URL of the Web console: minishift console --url Use minishift oc-env to display the command you need to type into your shell in order to add the oc binary to your PATH environment variable. The output of oc-env will differ depending on OS and shell type. $ minishift oc-env export PATH=\"/root/.minishift/cache/oc/v3.11.0/linux:$PATH\" # Run this command to configure your shell: # eval $(minishift oc-env)","title":"Starting Minishift"},{"location":"other-tools/kubernetes/minishift/#deploying-a-sample-application","text":"OpenShift provides various sample applications, such as templates, builder applications, and quickstarts. The following steps describe how to deploy a sample Node.js application from the command line. Create a Node.js example app: oc new-app https://github.com/sclorg/nodejs-ex -l name=myapp Track the build log until the app is built and deployed: oc logs -f bc/nodejs-ex Expose a route to the service: oc expose svc/nodejs-ex Access the application: minishift openshift service nodejs-ex --in-browser To stop Minishift, use the following command: minishift stop Stopping local OpenShift cluster... Stopping \"minishift\"...","title":"Deploying a Sample Application"},{"location":"other-tools/kubernetes/minishift/#updating-minishift","text":"minishift update","title":"Updating Minishift"},{"location":"other-tools/kubernetes/minishift/#uninstalling-minishift","text":"Delete the Minishift VM and any VM-specific files: minishift delete This command deletes everything in the $MINISHIFT_HOME/.minishift/machines/minishift directory. Other cached data and the persistent configuration are not removed. To completely uninstall Minishift, delete everything in the MINISHIFT_HOME directory (default ~/.minishift ) and ~/.kube : rm -rf ~/.minishift rm -rf ~/.kube With your hypervisor management tool, confirm that there are no remaining artifacts related to the Minishift VM. For example, if you use KVM, you need to run the virsh command.","title":"Uninstalling Minishift"},{"location":"other-tools/kubernetes/minishift/#connecting-to-the-minishift-vm-with-ssh","text":"You can use the minishift ssh command to interact with the Minishift VM. minishift ssh -- docker ps CONTAINER IMAGE COMMAND CREATED STATUS NAMES 71fe8ff16548 openshift/origin:... \"/usr/bin/openshift s\" 1 sec ago Up 1 second origin","title":"Connecting to the Minishift VM with SSH"},{"location":"other-tools/kubernetes/k3s/k3s-ha-cluster-using-k3d/","text":"Set up K3s in High Availability using k3d First, Kubernetes HA has two possible setups : embedded or external database (DB). We\u2019ll use the embedded DB in this HA K3s cluster setup. For which etcd is the default embedded DB. There are some strongly recommended Kubernetes HA best practices and also there is Automated HA master deployment doc . Pre-requisite Make sure you have already installed k3d following this . HA cluster with at least three control plane nodes k3d cluster create --servers 3 --image rancher/k3s:latest Here, --server 3 : specifies requests three nodes to be created with the role server and --image rancher/k3s:latest : specifies the K3s image to be used here we are using latest Switch context to the new cluster: kubectl config use-context k3d-k3s-default You can now check what has been created from the different points of view: kubectl get nodes --output wide The output will looks like: kubectl get pods --all-namespaces --output wide OR, kubectl get pods -A -o wide The output will looks like: Scale up the cluster You can quickly simulate the addition of another control plane node to the HA cluster: k3d node create extraCPnode --role=server --image=rancher/k3s:latest INFO[0000] Adding 1 node(s) to the runtime local cluster 'k3s-default'... INFO[0000] Starting Node 'k3d-extraCPnode-0' INFO[0018] Updating loadbalancer config to include new server node(s) INFO[0018] Successfully configured loadbalancer k3d-k3s-default-serverlb! INFO[0019] Successfully created 1 node(s)! Here, extraCPnode : specifies the name for the node, --role=server : sets the role for the node to be a control plane/server, --image rancher/k3s:latest : specifies the K3s image to be used here we are using latest kubectl get nodes NAME STATUS ROLES AGE VERSION k3d-extracpnode-0 Ready etcd,master 31m v1.19.3+k3s2 k3d-k3s-default-server-0 Ready etcd,master 47m v1.19.3+k3s2 k3d-k3s-default-server-1 Ready etcd,master 47m v1.19.3+k3s2 k3d-k3s-default-server-2 Ready etcd,master 47m v1.19.3+k3s2 OR, kubectl get nodes --output wide The output looks like below: Heavy Armored against crashes As we are working with containers, the best way to \"crash\" a node is to literally stop the container: docker stop k3d-k3s-default-server-0 Note The Docker and k3d commands will show the state change immediately. However, the Kubernetes (read: K8s or K3s) cluster needs a short time to see the state change to NotReady. kubectl get nodes NAME STATUS ROLES AGE VERSION k3d-extracpnode-0 Ready etcd,master 32m v1.19.3+k3s2 k3d-k3s-default-server-0 NotReady etcd,master 48m v1.19.3+k3s2 k3d-k3s-default-server-1 Ready etcd,master 48m v1.19.3+k3s2 k3d-k3s-default-server-2 Ready etcd,master 48m v1.19.3+k3s2 Now it is a good time to reference again the load balancer k3d uses and how it is critical in allowing us to continue accessing the K3s cluster. While the load balancer internally switched to the next available node, from an external connectivity point of view, we still use the same IP/host. This abstraction saves us quite some efforts and it\u2019s one of the most useful features of k3d. Let\u2019s look at the state of the cluster: kubectl get all --all-namespaces The output looks like below: Everything looks right. If we look at the pods more specifically, then we will see that K3s automatically self-healed by recreating pods running on the failed node on other nodes: kubectl get pods --all-namespaces --output wide As the output can be seen: Finally, to show the power of HA and how K3s manages it, let\u2019s restart the node0 and see it being re-included into the cluster as if nothing happened: docker start k3d-k3s-default-server-0 Our cluster is stable, and all the nodes are fully operational again as shown below: Cleaning the resources k3d cluster delete","title":"Multi-master K3s cluster setup using k3d"},{"location":"other-tools/kubernetes/k3s/k3s-ha-cluster-using-k3d/#set-up-k3s-in-high-availability-using-k3d","text":"First, Kubernetes HA has two possible setups : embedded or external database (DB). We\u2019ll use the embedded DB in this HA K3s cluster setup. For which etcd is the default embedded DB. There are some strongly recommended Kubernetes HA best practices and also there is Automated HA master deployment doc .","title":"Set up K3s in High Availability using k3d"},{"location":"other-tools/kubernetes/k3s/k3s-ha-cluster-using-k3d/#pre-requisite","text":"Make sure you have already installed k3d following this .","title":"Pre-requisite"},{"location":"other-tools/kubernetes/k3s/k3s-ha-cluster-using-k3d/#ha-cluster-with-at-least-three-control-plane-nodes","text":"k3d cluster create --servers 3 --image rancher/k3s:latest Here, --server 3 : specifies requests three nodes to be created with the role server and --image rancher/k3s:latest : specifies the K3s image to be used here we are using latest Switch context to the new cluster: kubectl config use-context k3d-k3s-default You can now check what has been created from the different points of view: kubectl get nodes --output wide The output will looks like: kubectl get pods --all-namespaces --output wide OR, kubectl get pods -A -o wide The output will looks like:","title":"HA cluster with at least three control plane nodes"},{"location":"other-tools/kubernetes/k3s/k3s-ha-cluster-using-k3d/#scale-up-the-cluster","text":"You can quickly simulate the addition of another control plane node to the HA cluster: k3d node create extraCPnode --role=server --image=rancher/k3s:latest INFO[0000] Adding 1 node(s) to the runtime local cluster 'k3s-default'... INFO[0000] Starting Node 'k3d-extraCPnode-0' INFO[0018] Updating loadbalancer config to include new server node(s) INFO[0018] Successfully configured loadbalancer k3d-k3s-default-serverlb! INFO[0019] Successfully created 1 node(s)! Here, extraCPnode : specifies the name for the node, --role=server : sets the role for the node to be a control plane/server, --image rancher/k3s:latest : specifies the K3s image to be used here we are using latest kubectl get nodes NAME STATUS ROLES AGE VERSION k3d-extracpnode-0 Ready etcd,master 31m v1.19.3+k3s2 k3d-k3s-default-server-0 Ready etcd,master 47m v1.19.3+k3s2 k3d-k3s-default-server-1 Ready etcd,master 47m v1.19.3+k3s2 k3d-k3s-default-server-2 Ready etcd,master 47m v1.19.3+k3s2 OR, kubectl get nodes --output wide The output looks like below:","title":"Scale up the cluster"},{"location":"other-tools/kubernetes/k3s/k3s-ha-cluster-using-k3d/#heavy-armored-against-crashes","text":"As we are working with containers, the best way to \"crash\" a node is to literally stop the container: docker stop k3d-k3s-default-server-0 Note The Docker and k3d commands will show the state change immediately. However, the Kubernetes (read: K8s or K3s) cluster needs a short time to see the state change to NotReady. kubectl get nodes NAME STATUS ROLES AGE VERSION k3d-extracpnode-0 Ready etcd,master 32m v1.19.3+k3s2 k3d-k3s-default-server-0 NotReady etcd,master 48m v1.19.3+k3s2 k3d-k3s-default-server-1 Ready etcd,master 48m v1.19.3+k3s2 k3d-k3s-default-server-2 Ready etcd,master 48m v1.19.3+k3s2 Now it is a good time to reference again the load balancer k3d uses and how it is critical in allowing us to continue accessing the K3s cluster. While the load balancer internally switched to the next available node, from an external connectivity point of view, we still use the same IP/host. This abstraction saves us quite some efforts and it\u2019s one of the most useful features of k3d. Let\u2019s look at the state of the cluster: kubectl get all --all-namespaces The output looks like below: Everything looks right. If we look at the pods more specifically, then we will see that K3s automatically self-healed by recreating pods running on the failed node on other nodes: kubectl get pods --all-namespaces --output wide As the output can be seen: Finally, to show the power of HA and how K3s manages it, let\u2019s restart the node0 and see it being re-included into the cluster as if nothing happened: docker start k3d-k3s-default-server-0 Our cluster is stable, and all the nodes are fully operational again as shown below:","title":"Heavy Armored against crashes"},{"location":"other-tools/kubernetes/k3s/k3s-ha-cluster-using-k3d/#cleaning-the-resources","text":"k3d cluster delete","title":"Cleaning the resources"},{"location":"other-tools/kubernetes/k3s/k3s-ha-cluster/","text":"K3s with High Availability setup First, Kubernetes HA has two possible setups : embedded or external database (DB). We\u2019ll use the external DB in this HA K3s cluster setup. For which MySQL is the external DB as shown here: In the diagram above, both the user running kubectl and each of the two agents connect to the TCP Load Balancer . The Load Balancer uses a list of private IP addresses to balance the traffic between the three servers . If one of the servers crashes, it is be removed from the list of IP addresses. The servers use the SQL data store to synchronize the cluster\u2019s state. Requirements i. Managed TCP Load Balancer ii. Managed MySQL service iii. Three VMs to run as K3s servers iv. Two VMs to run as K3s agents There are some strongly recommended Kubernetes HA best practices and also there is Automated HA master deployment doc . Managed TCP Load Balancer Create a load balancer using nginx : The nginx.conf located at etc/nginx/nginx.conf contains upstream that is pointing to the 3 K3s Servers on port 6443 as shown below: events {} ... stream { upstream k3s_servers { server <k3s_server1-Internal-IP>:6443; server <k3s_server2-Internal-IP>:6443; server <k3s_server3-Internal-IP>:6443; } server { listen 6443; proxy_pass k3s_servers; } } Managed MySQL service Create a MySQL database server with a new database and create a new mysql user and password with granted permission to read/write the new database. In this example, you can create: database name: <YOUR_DB_NAME> database user: <YOUR_DB_USER_NAME> database password: <YOUR_DB_USER_PASSWORD> #pragma: allowlist secret Three VMs to run as K3s servers Create 3 K3s Master VMs and perform the following steps on each of them: i. Export the datastore endpoint: export K3S_DATASTORE_ENDPOINT='mysql://<YOUR_DB_USER_NAME>:<YOUR_DB_USER_PASSWORD>@tcp(<MySQL-Server-Internal-IP>:3306)/<YOUR_DB_NAME>' ii. Install the K3s with setting not to deploy any pods on this server (opposite of affinity) unless critical addons and tls-san set <Loadbalancer-Internal-IP> as alternative name for that tls certificate. curl -sfL https://get.k3s.io | sh -s - server \\ --node-taint CriticalAddonsOnly=true:NoExecute \\ --tls-san <Loadbalancer-Internal-IP_or_Hostname> Verify all master nodes are visible to eachothers: sudo k3s kubectl get node Generate token from one of the K3s Master VMs: You need to extract a token form the master that will be used to join the nodes to the control plane by running following command on one of the K3s master node: sudo cat /var/lib/rancher/k3s/server/node-token You will then obtain a token that looks like: K1097aace305b0c1077fc854547f34a598d23330ff047ddeed8beb3c428b38a1ca7::server:6cc9fbb6c5c9de96f37fb14b5535c778 Two VMs to run as K3s agents Set the K3S_URL to point to the Loadbalancer\u2019s internal IP and set the K3S_TOKEN from the clipboard on both of the agent nodes: curl -sfL https://get.k3s.io | K3S_URL=https://<Loadbalancer-Internal-IP_or_Hostname>:6443 K3S_TOKEN=<Token_From_Master> sh - Once both Agents are running, if you run the following command on Master Server, you can see all nodes: `sudo k3s kubectl get node` Simulate a failure To simulate a failure, stop the K3s service on one or more of the K3s servers manually, then run the kubectl get nodes command: sudo systemctl stop k3s The third server will take over at this point. To restart servers manually: sudo systemctl restart k3s On your local development machine to access Kubernetes Cluster Remotely (Optional) Important Requirement Your local development machine must have installed kubectl . Copy kubernetes config to your local machine: Copy the kubeconfig file's content located at the K3s master node at /etc/rancher/k3s/k3s.yaml to your local machine's ~/.kube/config file. Before saving, please change the cluster server path from 127.0.0.1 to <Loadbalancer-Internal-IP> . This will allow your local machine to see the cluster nodes: kubectl get nodes Kubernetes Dashboard The Kubernetes Dashboard is a GUI tool to help you work more efficiently with K8s cluster. This is only accessible from within the cluster (at least not without some serious tweaking). check releases for the command to use for Installation : kubectl apply -f https://raw.githubusercontent.com/kubernetes/dashboard/v2.3.1/aio/deploy/recommended.yaml Dashboard RBAC Configuration: dashboard.admin-user.yml apiVersion: v1 kind: ServiceAccount metadata: name: admin-user namespace: kubernetes-dashboard dashboard.admin-user-role.yml apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRoleBinding metadata: name: admin-user roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: cluster-admin subjects: - kind: ServiceAccount name: admin-user namespace: kubernetes-dashboard Deploy the admin-user configuration: Important Note If you\u2019re doing this from your local development machine, remove sudo k3s and just use kubectl ) sudo k3s kubectl create -f dashboard.admin-user.yml -f dashboard.admin-user-role.yml Get bearer token sudo k3s kubectl -n kubernetes-dashboard describe secret admin-user-token | grep ^token Start dashboard locally: sudo k3s kubectl proxy Then you can sign in at this URL using your token we got in the previous step: http://localhost:8001/api/v1/namespaces/kubernetes-dashboard/services/https:kubernetes-dashboard:/proxy/ Deploying Nginx using deployment Create a deployment nginx.yaml : vi nginx.yaml Copy and paste the following conent on nginx.yaml : apiVersion: apps/v1 kind: Deployment metadata: name: mysite labels: app: mysite spec: replicas: 1 selector: matchLabels: app: mysite template: metadata: labels: app : mysite spec: containers: - name : mysite image: nginx ports: - containerPort: 80 sudo k3s kubectl apply -f nginx.yaml Verify the nginx pod is on Running state: sudo k3s kubectl get pods --all-namespaces OR, kubectl get pods --all-namespaces --output wide OR, kubectl get pods -A -o wide Scale the pods to available agents: sudo k3s kubectl scale --replicas=2 deploy/mysite View all deployment status: sudo k3s kubectl get deploy mysite NAME READY UP-TO-DATE AVAILABLE AGE mysite 2/2 2 2 85s Delete the nginx deployment and pod: sudo k3s kubectl delete -f nginx.yaml OR, sudo k3s kubectl delete deploy mysite","title":"K3s with High Availibility(HA) setup"},{"location":"other-tools/kubernetes/k3s/k3s-ha-cluster/#k3s-with-high-availability-setup","text":"First, Kubernetes HA has two possible setups : embedded or external database (DB). We\u2019ll use the external DB in this HA K3s cluster setup. For which MySQL is the external DB as shown here: In the diagram above, both the user running kubectl and each of the two agents connect to the TCP Load Balancer . The Load Balancer uses a list of private IP addresses to balance the traffic between the three servers . If one of the servers crashes, it is be removed from the list of IP addresses. The servers use the SQL data store to synchronize the cluster\u2019s state.","title":"K3s with High Availability setup"},{"location":"other-tools/kubernetes/k3s/k3s-ha-cluster/#requirements","text":"i. Managed TCP Load Balancer ii. Managed MySQL service iii. Three VMs to run as K3s servers iv. Two VMs to run as K3s agents There are some strongly recommended Kubernetes HA best practices and also there is Automated HA master deployment doc .","title":"Requirements"},{"location":"other-tools/kubernetes/k3s/k3s-ha-cluster/#managed-tcp-load-balancer","text":"Create a load balancer using nginx : The nginx.conf located at etc/nginx/nginx.conf contains upstream that is pointing to the 3 K3s Servers on port 6443 as shown below: events {} ... stream { upstream k3s_servers { server <k3s_server1-Internal-IP>:6443; server <k3s_server2-Internal-IP>:6443; server <k3s_server3-Internal-IP>:6443; } server { listen 6443; proxy_pass k3s_servers; } }","title":"Managed TCP Load Balancer"},{"location":"other-tools/kubernetes/k3s/k3s-ha-cluster/#managed-mysql-service","text":"Create a MySQL database server with a new database and create a new mysql user and password with granted permission to read/write the new database. In this example, you can create: database name: <YOUR_DB_NAME> database user: <YOUR_DB_USER_NAME> database password: <YOUR_DB_USER_PASSWORD> #pragma: allowlist secret","title":"Managed MySQL service"},{"location":"other-tools/kubernetes/k3s/k3s-ha-cluster/#three-vms-to-run-as-k3s-servers","text":"Create 3 K3s Master VMs and perform the following steps on each of them: i. Export the datastore endpoint: export K3S_DATASTORE_ENDPOINT='mysql://<YOUR_DB_USER_NAME>:<YOUR_DB_USER_PASSWORD>@tcp(<MySQL-Server-Internal-IP>:3306)/<YOUR_DB_NAME>' ii. Install the K3s with setting not to deploy any pods on this server (opposite of affinity) unless critical addons and tls-san set <Loadbalancer-Internal-IP> as alternative name for that tls certificate. curl -sfL https://get.k3s.io | sh -s - server \\ --node-taint CriticalAddonsOnly=true:NoExecute \\ --tls-san <Loadbalancer-Internal-IP_or_Hostname> Verify all master nodes are visible to eachothers: sudo k3s kubectl get node Generate token from one of the K3s Master VMs: You need to extract a token form the master that will be used to join the nodes to the control plane by running following command on one of the K3s master node: sudo cat /var/lib/rancher/k3s/server/node-token You will then obtain a token that looks like: K1097aace305b0c1077fc854547f34a598d23330ff047ddeed8beb3c428b38a1ca7::server:6cc9fbb6c5c9de96f37fb14b5535c778","title":"Three VMs to run as K3s servers"},{"location":"other-tools/kubernetes/k3s/k3s-ha-cluster/#two-vms-to-run-as-k3s-agents","text":"Set the K3S_URL to point to the Loadbalancer\u2019s internal IP and set the K3S_TOKEN from the clipboard on both of the agent nodes: curl -sfL https://get.k3s.io | K3S_URL=https://<Loadbalancer-Internal-IP_or_Hostname>:6443 K3S_TOKEN=<Token_From_Master> sh - Once both Agents are running, if you run the following command on Master Server, you can see all nodes: `sudo k3s kubectl get node`","title":"Two VMs to run as K3s agents"},{"location":"other-tools/kubernetes/k3s/k3s-ha-cluster/#simulate-a-failure","text":"To simulate a failure, stop the K3s service on one or more of the K3s servers manually, then run the kubectl get nodes command: sudo systemctl stop k3s The third server will take over at this point. To restart servers manually: sudo systemctl restart k3s","title":"Simulate a failure"},{"location":"other-tools/kubernetes/k3s/k3s-ha-cluster/#on-your-local-development-machine-to-access-kubernetes-cluster-remotely-optional","text":"Important Requirement Your local development machine must have installed kubectl . Copy kubernetes config to your local machine: Copy the kubeconfig file's content located at the K3s master node at /etc/rancher/k3s/k3s.yaml to your local machine's ~/.kube/config file. Before saving, please change the cluster server path from 127.0.0.1 to <Loadbalancer-Internal-IP> . This will allow your local machine to see the cluster nodes: kubectl get nodes","title":"On your local development machine to access Kubernetes Cluster Remotely (Optional)"},{"location":"other-tools/kubernetes/k3s/k3s-ha-cluster/#kubernetes-dashboard","text":"The Kubernetes Dashboard is a GUI tool to help you work more efficiently with K8s cluster. This is only accessible from within the cluster (at least not without some serious tweaking). check releases for the command to use for Installation : kubectl apply -f https://raw.githubusercontent.com/kubernetes/dashboard/v2.3.1/aio/deploy/recommended.yaml Dashboard RBAC Configuration: dashboard.admin-user.yml apiVersion: v1 kind: ServiceAccount metadata: name: admin-user namespace: kubernetes-dashboard dashboard.admin-user-role.yml apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRoleBinding metadata: name: admin-user roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: cluster-admin subjects: - kind: ServiceAccount name: admin-user namespace: kubernetes-dashboard Deploy the admin-user configuration: Important Note If you\u2019re doing this from your local development machine, remove sudo k3s and just use kubectl ) sudo k3s kubectl create -f dashboard.admin-user.yml -f dashboard.admin-user-role.yml Get bearer token sudo k3s kubectl -n kubernetes-dashboard describe secret admin-user-token | grep ^token Start dashboard locally: sudo k3s kubectl proxy Then you can sign in at this URL using your token we got in the previous step: http://localhost:8001/api/v1/namespaces/kubernetes-dashboard/services/https:kubernetes-dashboard:/proxy/","title":"Kubernetes Dashboard"},{"location":"other-tools/kubernetes/k3s/k3s-ha-cluster/#deploying-nginx-using-deployment","text":"Create a deployment nginx.yaml : vi nginx.yaml Copy and paste the following conent on nginx.yaml : apiVersion: apps/v1 kind: Deployment metadata: name: mysite labels: app: mysite spec: replicas: 1 selector: matchLabels: app: mysite template: metadata: labels: app : mysite spec: containers: - name : mysite image: nginx ports: - containerPort: 80 sudo k3s kubectl apply -f nginx.yaml Verify the nginx pod is on Running state: sudo k3s kubectl get pods --all-namespaces OR, kubectl get pods --all-namespaces --output wide OR, kubectl get pods -A -o wide Scale the pods to available agents: sudo k3s kubectl scale --replicas=2 deploy/mysite View all deployment status: sudo k3s kubectl get deploy mysite NAME READY UP-TO-DATE AVAILABLE AGE mysite 2/2 2 2 85s Delete the nginx deployment and pod: sudo k3s kubectl delete -f nginx.yaml OR, sudo k3s kubectl delete deploy mysite","title":"Deploying Nginx using deployment"},{"location":"other-tools/kubernetes/k3s/k3s-using-k3d/","text":"Setup K3s cluster Using k3d One of the most popular and second method of creating k3s cluster is by using k3d . By the name itself it suggests, K3s-in-docker , is a wrapper around K3s \u2013 Lightweight Kubernetes that runs it in docker. Please refer to this link to get brief insights of this wonderful tool. It provides a seamless experience working with K3s cluster management with some straight forward commands. k3d is efficient enough to create and manage K3s single node and well as K3s High Availability clusters just with few commands. Note For using k3d you must have docker installed in your system Install Docker Install container runtime - docker apt-get install docker.io -y Configure the Docker daemon, in particular to use systemd for the management of the container\u2019s cgroups cat <<EOF | sudo tee /etc/docker/daemon.json { \"exec-opts\": [\"native.cgroupdriver=systemd\"] } EOF systemctl enable --now docker usermod -aG docker ubuntu systemctl daemon-reload systemctl restart docker Install kubectl Install kubectl binary \u2022 kubectl : the command line util to talk to your cluster. snap install kubectl --classic This outputs: kubectl 1.22.2 from Canonical\u2713 installed Now verify the kubectl version: kubectl version -o yaml Installing k3d k3d Installation: The below command will install the k3d, in your system using the installation script. wget -q -O - https://raw.githubusercontent.com/rancher/k3d/main/install.sh | bash OR, curl -s https://raw.githubusercontent.com/rancher/k3d/main/install.sh | bash To verify the installation, please run the following command: k3d version k3d version v5.0.0 k3s version v1.21.5-k3s1 (default) After the successful installation, you are ready to create your cluster using k3d and run K3s in docker within seconds. Getting Started: Now let's directly jump into creating our K3s cluster using k3d . Create k3d Cluster: k3d cluster create k3d-demo-cluster This single command spawns a K3s cluster with two containers: A Kubernetes control-plane node(server) and a load balancer(serverlb) in front of it. It puts both of them in a dedicated Docker network and exposes the Kubernetes API on a randomly chosen free port on the Docker host. It also creates a named Docker volume in the background as a preparation for image imports. You can also look for advance syntax for cluster creation: k3d cluster create mycluster --api-port 127.0.0.1:6445 --servers 3 \\ --agents 2 --volume '/home/me/mycode:/code@agent[*]' --port '8080:80@loadbalancer' Here, the above single command spawns a K3s cluster with six containers: load balancer 3 servers (control-plane nodes) 2 agents (formerly worker nodes) With the --api-port 127.0.0.1:6445 , you tell k3d to map the Kubernetes API Port (6443 internally) to 127.0.0.1/localhost \u2019s port 6445 . That means that you will have this connection string in your Kubeconfig: server: https://127.0.0.1:6445 to connect to this cluster. This port will be mapped from the load balancer to your host system. From there, requests will be proxied to your server nodes, effectively simulating a production setup, where server nodes also can go down and you would want to failover to another server. The --volume /home/me/mycode:/code@agent[*] bind mounts your local directory /home/me/mycode to the path /code inside all ( [*] of your agent nodes). Replace * with an index (here: 0 or 1) to only mount it into one of them. The specification telling k3d which nodes it should mount the volume to is called \"node filter\" and it\u2019s also used for other flags, like the --port flag for port mappings. That said, --port '8080:80@loadbalancer' maps your local host\u2019s port 8080 to port 80 on the load balancer (serverlb), which can be used to forward HTTP ingress traffic to your cluster. For example, you can now deploy a web app into the cluster (Deployment), which is exposed (Service) externally via an Ingress such as myapp.k3d.localhost . Then (provided that everything is set up to resolve that domain to your local host IP), you can point your browser to http://myapp.k3d.localhost:8080 to access your app. Traffic then flows from your host through the Docker bridge interface to the load balancer. From there, it\u2019s proxied to the cluster, where it passes via Ingress and Service to your application Pod. Note You have to have some mechanism set up to route to resolve myapp.k3d.localhost to your local host IP ( 127.0.0.1 ). The most common way is using entries of the form 127.0.0.1 myapp.k3d.localhost in your /etc/hosts file ( C:\\Windows\\System32\\drivers\\etc\\hosts on Windows). However, this does not allow for wildcard entries ( *.localhost ), so it may become a bit cumbersome after a while, so you may want to have a look at tools like dnsmasq (MacOS/UNIX) or Acrylic (Windows) to ease the burden. Getting the cluster\u2019s kubeconfig: Get the new cluster\u2019s connection details merged into your default kubeconfig (usually specified using the KUBECONFIG environment variable or the default path $HOME/.kube/config ) and directly switch to the new context: k3d kubeconfig merge k3d-demo-cluster --kubeconfig-switch-context This outputs: /root/.k3d/kubeconfig-k3d-demo-cluster.yaml Checking the nodes running on k3d cluster: k3d node list You can see here two nodes. The (very) smart implementation here is that while the cluster is running on its node k3d-k3s-default-server-0 , there is another \"node\" that acts as the load balancer i.e. k3d-k3d-demo-cluster-serverlb . Firing Kubectl commands that allows you to run commands against Kubernetes: i. The below command will list down the nodes available in our cluster: kubectl get nodes -o wide OR, kubectl get nodes --output wide The output will looks like: ii. To look at what\u2019s inside the K3s cluster (pods, services, deployments, etc.): kubectl get all --all-namespaces The output will looks like: We can see that, in addition of the Kubernetes service, K3s deploys DNS, metrics and ingress (traefik) services when we use the defaults. iii. List the active k3d clusters: k3d cluster list iv. Check the cluster connectivity: kubectl cluster-info To further debug and diagnose cluster problems, use 'kubectl cluster-info dump'. Check the active containers: docker ps Now as you can observe, the cluster is up and running and we can play around the cluster, you can create and deploy your applications over the cluster. Deleting Cluster: k3d cluster delete k3d-demo-cluster INFO[0000] Deleting cluster 'k3d-demo-cluster' INFO[0000] Deleted k3d-k3d-demo-cluster-serverlb INFO[0001] Deleted k3d-k3d-demo-cluster-server-0 INFO[0001] Deleting cluster network 'k3d-k3d-demo-cluster' INFO[0001] Deleting image volume 'k3d-k3d-demo-cluster-images' INFO[0001] Removing cluster details from default kubeconfig... INFO[0001] Removing standalone kubeconfig file (if there is one)... INFO[0001] Successfully deleted cluster k3d-demo-cluster! You can also create a k3d High Availability cluster and add as many nodes you want within seconds.","title":"Single-Node K3s Cluster using k3d"},{"location":"other-tools/kubernetes/k3s/k3s-using-k3d/#setup-k3s-cluster-using-k3d","text":"One of the most popular and second method of creating k3s cluster is by using k3d . By the name itself it suggests, K3s-in-docker , is a wrapper around K3s \u2013 Lightweight Kubernetes that runs it in docker. Please refer to this link to get brief insights of this wonderful tool. It provides a seamless experience working with K3s cluster management with some straight forward commands. k3d is efficient enough to create and manage K3s single node and well as K3s High Availability clusters just with few commands. Note For using k3d you must have docker installed in your system","title":"Setup K3s cluster Using k3d"},{"location":"other-tools/kubernetes/k3s/k3s-using-k3d/#install-docker","text":"Install container runtime - docker apt-get install docker.io -y Configure the Docker daemon, in particular to use systemd for the management of the container\u2019s cgroups cat <<EOF | sudo tee /etc/docker/daemon.json { \"exec-opts\": [\"native.cgroupdriver=systemd\"] } EOF systemctl enable --now docker usermod -aG docker ubuntu systemctl daemon-reload systemctl restart docker","title":"Install Docker"},{"location":"other-tools/kubernetes/k3s/k3s-using-k3d/#install-kubectl","text":"Install kubectl binary \u2022 kubectl : the command line util to talk to your cluster. snap install kubectl --classic This outputs: kubectl 1.22.2 from Canonical\u2713 installed Now verify the kubectl version: kubectl version -o yaml","title":"Install kubectl"},{"location":"other-tools/kubernetes/k3s/k3s-using-k3d/#installing-k3d","text":"k3d Installation: The below command will install the k3d, in your system using the installation script. wget -q -O - https://raw.githubusercontent.com/rancher/k3d/main/install.sh | bash OR, curl -s https://raw.githubusercontent.com/rancher/k3d/main/install.sh | bash To verify the installation, please run the following command: k3d version k3d version v5.0.0 k3s version v1.21.5-k3s1 (default) After the successful installation, you are ready to create your cluster using k3d and run K3s in docker within seconds. Getting Started: Now let's directly jump into creating our K3s cluster using k3d . Create k3d Cluster: k3d cluster create k3d-demo-cluster This single command spawns a K3s cluster with two containers: A Kubernetes control-plane node(server) and a load balancer(serverlb) in front of it. It puts both of them in a dedicated Docker network and exposes the Kubernetes API on a randomly chosen free port on the Docker host. It also creates a named Docker volume in the background as a preparation for image imports. You can also look for advance syntax for cluster creation: k3d cluster create mycluster --api-port 127.0.0.1:6445 --servers 3 \\ --agents 2 --volume '/home/me/mycode:/code@agent[*]' --port '8080:80@loadbalancer' Here, the above single command spawns a K3s cluster with six containers: load balancer 3 servers (control-plane nodes) 2 agents (formerly worker nodes) With the --api-port 127.0.0.1:6445 , you tell k3d to map the Kubernetes API Port (6443 internally) to 127.0.0.1/localhost \u2019s port 6445 . That means that you will have this connection string in your Kubeconfig: server: https://127.0.0.1:6445 to connect to this cluster. This port will be mapped from the load balancer to your host system. From there, requests will be proxied to your server nodes, effectively simulating a production setup, where server nodes also can go down and you would want to failover to another server. The --volume /home/me/mycode:/code@agent[*] bind mounts your local directory /home/me/mycode to the path /code inside all ( [*] of your agent nodes). Replace * with an index (here: 0 or 1) to only mount it into one of them. The specification telling k3d which nodes it should mount the volume to is called \"node filter\" and it\u2019s also used for other flags, like the --port flag for port mappings. That said, --port '8080:80@loadbalancer' maps your local host\u2019s port 8080 to port 80 on the load balancer (serverlb), which can be used to forward HTTP ingress traffic to your cluster. For example, you can now deploy a web app into the cluster (Deployment), which is exposed (Service) externally via an Ingress such as myapp.k3d.localhost . Then (provided that everything is set up to resolve that domain to your local host IP), you can point your browser to http://myapp.k3d.localhost:8080 to access your app. Traffic then flows from your host through the Docker bridge interface to the load balancer. From there, it\u2019s proxied to the cluster, where it passes via Ingress and Service to your application Pod. Note You have to have some mechanism set up to route to resolve myapp.k3d.localhost to your local host IP ( 127.0.0.1 ). The most common way is using entries of the form 127.0.0.1 myapp.k3d.localhost in your /etc/hosts file ( C:\\Windows\\System32\\drivers\\etc\\hosts on Windows). However, this does not allow for wildcard entries ( *.localhost ), so it may become a bit cumbersome after a while, so you may want to have a look at tools like dnsmasq (MacOS/UNIX) or Acrylic (Windows) to ease the burden. Getting the cluster\u2019s kubeconfig: Get the new cluster\u2019s connection details merged into your default kubeconfig (usually specified using the KUBECONFIG environment variable or the default path $HOME/.kube/config ) and directly switch to the new context: k3d kubeconfig merge k3d-demo-cluster --kubeconfig-switch-context This outputs: /root/.k3d/kubeconfig-k3d-demo-cluster.yaml Checking the nodes running on k3d cluster: k3d node list You can see here two nodes. The (very) smart implementation here is that while the cluster is running on its node k3d-k3s-default-server-0 , there is another \"node\" that acts as the load balancer i.e. k3d-k3d-demo-cluster-serverlb . Firing Kubectl commands that allows you to run commands against Kubernetes: i. The below command will list down the nodes available in our cluster: kubectl get nodes -o wide OR, kubectl get nodes --output wide The output will looks like: ii. To look at what\u2019s inside the K3s cluster (pods, services, deployments, etc.): kubectl get all --all-namespaces The output will looks like: We can see that, in addition of the Kubernetes service, K3s deploys DNS, metrics and ingress (traefik) services when we use the defaults. iii. List the active k3d clusters: k3d cluster list iv. Check the cluster connectivity: kubectl cluster-info To further debug and diagnose cluster problems, use 'kubectl cluster-info dump'. Check the active containers: docker ps Now as you can observe, the cluster is up and running and we can play around the cluster, you can create and deploy your applications over the cluster. Deleting Cluster: k3d cluster delete k3d-demo-cluster INFO[0000] Deleting cluster 'k3d-demo-cluster' INFO[0000] Deleted k3d-k3d-demo-cluster-serverlb INFO[0001] Deleted k3d-k3d-demo-cluster-server-0 INFO[0001] Deleting cluster network 'k3d-k3d-demo-cluster' INFO[0001] Deleting image volume 'k3d-k3d-demo-cluster-images' INFO[0001] Removing cluster details from default kubeconfig... INFO[0001] Removing standalone kubeconfig file (if there is one)... INFO[0001] Successfully deleted cluster k3d-demo-cluster! You can also create a k3d High Availability cluster and add as many nodes you want within seconds.","title":"Installing k3d"},{"location":"other-tools/kubernetes/k3s/k3s-using-k3sup/","text":"K3s cluster setup using k3sup k3sup (pronounced ketchup ) is a popular open source tool to install K3s over SSH. Bootstrap the cluster The two most important commands in k3sup are: i. install: install K3s to a new server and create a join token for the cluster ii. join: fetch the join token from a server, then use it to install K3s to an agent Download k3sup curl -sLS https://get.k3sup.dev | sh sudo install k3sup /usr/bin/ k3sup --help Other options for install : --cluster - start this server in clustering mode using embedded etcd (embedded HA) --skip-install - if you already have k3s installed, you can just run this command to get the kubeconfig --ssh-key - specify a specific path for the SSH key for remote login --local-path - default is ./kubeconfig - set the file where you want to save your cluster's kubeconfig . By default this file will be overwritten. --merge - Merge config into existing file instead of overwriting (e.g. to add config to the default kubectl config, use --local-path ~/.kube/config --merge ). --context - default is default - set the name of the kubeconfig context. --ssh-port - default is 22, but you can specify an alternative port i.e. 2222 --k3s-extra-args - Optional extra arguments to pass to k3s installer, wrapped in quotes, i.e. --k3s-extra-args '--no-deploy traefik' or --k3s-extra-args '--docker' . For multiple args combine then within single quotes --k3s-extra-args --no-deploy traefik --docker . --k3s-version - set the specific version of k3s, i.e. v0.9.1 --ipsec - Enforces the optional extra argument for k3s: --flannel-backend option: ipsec --print-command - Prints out the command, sent over SSH to the remote computer --datastore - used to pass a SQL connection-string to the --datastore-endpoint flag of k3s. See even more install options by running k3sup install --help . On Master Node: export SERVER_IP=<Master-Internal-IP> export USER=root k3sup install --ip $SERVER_IP --user $USER On Agent Node: Next join one or more agents to the cluster: export AGENT_IP=<Agent-Internal-IP> export SERVER_IP=<Master-Internal-IP> export USER=root k3sup join --ip $AGENT_IP --server-ip $SERVER_IP --user $USER Create a multi-master (HA) setup with external SQL export LB_IP='<Loadbalancer-Internal-IP_or_Hostname>' export DATASTORE='mysql://<YOUR_DB_USER_NAME>:<YOUR_DB_USER_PASSWORD>@tcp(<MySQL-Server-Internal-IP>:3306)/<YOUR_DB_NAME>' export CHANNEL=latest Before continuing, check that your environment variables are still populated from earlier, and if not, trace back and populate them. echo $LB_IP echo $DATASTORE echo $CHANNEL k3sup install --user root --ip $SERVER1 \\ --k3s-channel $CHANNEL \\ --print-command \\ --datastore='${DATASTORE}' \\ --tls-san $LB_IP k3sup install --user root --ip $SERVER2 \\ --k3s-channel $CHANNEL \\ --print-command \\ --datastore='${DATASTORE}' \\ --tls-san $LB_IP k3sup install --user root --ip $SERVER3 \\ --k3s-channel $CHANNEL \\ --print-command \\ --datastore='${DATASTORE}' \\ --tls-san $LB_IP k3sup join --user root --server-ip $LB_IP --ip $AGENT1 \\ --k3s-channel $CHANNEL \\ --print-command k3sup join --user root --server-ip $LB_IP --ip $AGENT2 \\ --k3s-channel $CHANNEL \\ --print-command There will be a kubeconfig file created in the current working directory with the IP address of the LoadBalancer set for kubectl to use. Check the nodes have joined: export KUBECONFIG=`pwd`/kubeconfig kubectl get node","title":"Multi-master HA K3s cluster using k3sup"},{"location":"other-tools/kubernetes/k3s/k3s-using-k3sup/#k3s-cluster-setup-using-k3sup","text":"k3sup (pronounced ketchup ) is a popular open source tool to install K3s over SSH. Bootstrap the cluster The two most important commands in k3sup are: i. install: install K3s to a new server and create a join token for the cluster ii. join: fetch the join token from a server, then use it to install K3s to an agent","title":"K3s cluster setup using k3sup"},{"location":"other-tools/kubernetes/k3s/k3s-using-k3sup/#download-k3sup","text":"curl -sLS https://get.k3sup.dev | sh sudo install k3sup /usr/bin/ k3sup --help Other options for install : --cluster - start this server in clustering mode using embedded etcd (embedded HA) --skip-install - if you already have k3s installed, you can just run this command to get the kubeconfig --ssh-key - specify a specific path for the SSH key for remote login --local-path - default is ./kubeconfig - set the file where you want to save your cluster's kubeconfig . By default this file will be overwritten. --merge - Merge config into existing file instead of overwriting (e.g. to add config to the default kubectl config, use --local-path ~/.kube/config --merge ). --context - default is default - set the name of the kubeconfig context. --ssh-port - default is 22, but you can specify an alternative port i.e. 2222 --k3s-extra-args - Optional extra arguments to pass to k3s installer, wrapped in quotes, i.e. --k3s-extra-args '--no-deploy traefik' or --k3s-extra-args '--docker' . For multiple args combine then within single quotes --k3s-extra-args --no-deploy traefik --docker . --k3s-version - set the specific version of k3s, i.e. v0.9.1 --ipsec - Enforces the optional extra argument for k3s: --flannel-backend option: ipsec --print-command - Prints out the command, sent over SSH to the remote computer --datastore - used to pass a SQL connection-string to the --datastore-endpoint flag of k3s. See even more install options by running k3sup install --help . On Master Node: export SERVER_IP=<Master-Internal-IP> export USER=root k3sup install --ip $SERVER_IP --user $USER On Agent Node: Next join one or more agents to the cluster: export AGENT_IP=<Agent-Internal-IP> export SERVER_IP=<Master-Internal-IP> export USER=root k3sup join --ip $AGENT_IP --server-ip $SERVER_IP --user $USER","title":"Download k3sup"},{"location":"other-tools/kubernetes/k3s/k3s-using-k3sup/#create-a-multi-master-ha-setup-with-external-sql","text":"export LB_IP='<Loadbalancer-Internal-IP_or_Hostname>' export DATASTORE='mysql://<YOUR_DB_USER_NAME>:<YOUR_DB_USER_PASSWORD>@tcp(<MySQL-Server-Internal-IP>:3306)/<YOUR_DB_NAME>' export CHANNEL=latest Before continuing, check that your environment variables are still populated from earlier, and if not, trace back and populate them. echo $LB_IP echo $DATASTORE echo $CHANNEL k3sup install --user root --ip $SERVER1 \\ --k3s-channel $CHANNEL \\ --print-command \\ --datastore='${DATASTORE}' \\ --tls-san $LB_IP k3sup install --user root --ip $SERVER2 \\ --k3s-channel $CHANNEL \\ --print-command \\ --datastore='${DATASTORE}' \\ --tls-san $LB_IP k3sup install --user root --ip $SERVER3 \\ --k3s-channel $CHANNEL \\ --print-command \\ --datastore='${DATASTORE}' \\ --tls-san $LB_IP k3sup join --user root --server-ip $LB_IP --ip $AGENT1 \\ --k3s-channel $CHANNEL \\ --print-command k3sup join --user root --server-ip $LB_IP --ip $AGENT2 \\ --k3s-channel $CHANNEL \\ --print-command There will be a kubeconfig file created in the current working directory with the IP address of the LoadBalancer set for kubectl to use. Check the nodes have joined: export KUBECONFIG=`pwd`/kubeconfig kubectl get node","title":"Create a multi-master (HA) setup with external SQL"},{"location":"other-tools/kubernetes/k3s/k3s/","text":"K3s Features Lightweight certified K8s distro Built for production operations 40MB binary, 250MB memeory consumption Single process w/ integrated K8s master, Kubelet, and containerd Supports not only etcd to hold the cluster state, but also SQLite (for single-node, simpler setups) or external DBs like MySQL and PostgreSQL Open source project Components and architecure High-Availability K3s Server with an External DB: or, For this kind of high availability k3s setup read this . Pre-requisite We will need 1 control-plane(master) and 2 worker nodes to create a single control-plane kubernetes cluster using k3s . We are using following setting for this purpose: 1 Linux machine for master, ubuntu-22.04-x86_64 or your choice of Ubuntu OS image, cpu-a.2 flavor with 2vCPU, 4GB RAM, 20GB storage - also assign Floating IP to the master node. 2 Linux machines for worker, ubuntu-22.04-x86_64 or your choice of Ubuntu OS image, cpu-a.1 flavor with 1vCPU, 2GB RAM, 20GB storage. ssh access to all machines: Read more here on how to setup SSH to your remote VMs. Networking The K3s server needs port 6443 to be accessible by all nodes. The nodes need to be able to reach other nodes over UDP port 8472 when Flannel VXLAN overlay networking is used. The node should not listen on any other port. K3s uses reverse tunneling such that the nodes make outbound connections to the server and all kubelet traffic runs through that tunnel. However, if you do not use Flannel and provide your own custom CNI, then port 8472 is not needed by K3s. If you wish to utilize the metrics server , you will need to open port 10250 on each node. If you plan on achieving high availability with embedded etcd , server nodes must be accessible to each other on ports 2379 and 2380 . Create 1 security group with appropriate Inbound Rules for K3s Server Nodes that will be used by all 3 nodes: Important Note The VXLAN overlay networking port on nodes should not be exposed to the world as it opens up your cluster network to be accessed by anyone. Run your nodes behind a firewall/security group that disables access to port 8472 . setup Unique hostname to each machine using the following command: echo \"<node_internal_IP> <host_name>\" >> /etc/hosts hostnamectl set-hostname <host_name> For example, echo \"192.168.0.235 k3s-master\" >> /etc/hosts hostnamectl set-hostname k3s-master In this step, you will install kubelet and kubeadm on the below nodes k3s-master k3s-worker1 k3s-worker2 The below steps will be performed on all the above mentioned nodes: SSH into all the 3 machines Switch as root: sudo su Update the repositories and packages: apt-get update && apt-get upgrade -y Install curl and apt-transport-https apt-get update && apt-get install -y apt-transport-https curl Install Docker Install container runtime - docker apt-get install docker.io -y Configure the Docker daemon, in particular to use systemd for the management of the container\u2019s cgroups cat <<EOF | sudo tee /etc/docker/daemon.json { \"exec-opts\": [\"native.cgroupdriver=systemd\"] } EOF systemctl enable --now docker usermod -aG docker ubuntu systemctl daemon-reload systemctl restart docker Configure K3s to bootstrap the cluster on master node Run the below command on the master node i.e. k3s-master that you want to setup as control plane. SSH into k3s-master machine Switch to root user: sudo su Execute the below command to initialize the cluster: curl -sfL https://get.k3s.io | sh -s - --kubelet-arg 'cgroup-driver=systemd' \\ --node-taint CriticalAddonsOnly=true:NoExecute --docker OR, If you don't want to setup the K3s cluster without using docker as the container runtime, then just run without supplying the --docker argument. curl -sfL https://get.k3s.io | sh - After running this installation: The K3s service will be configured to automatically restart after node reboots or if the process crashes or is killed Additional utilities will be installed, including kubectl , crictl , ctr , k3s-killall.sh , and k3s-uninstall.sh A kubeconfig file will be written to /etc/rancher/k3s/k3s.yaml and the kubectl installed by K3s will automatically use it. To check if the service installed successfully, you can use: systemctl status k3s The output looks like: OR, k3s --version kubectl version Note If you want to taint the node i.e. not to deploy pods on this node after installation then run: kubectl taint nodes <master_node_name> k3s-controlplane=true:NoExecure i.e. kubectl taint nodes k3s-master k3s-controlplane=true:NoExecure You can check if the master node is working by: k3s kubectl get nodes NAME STATUS ROLES AGE VERSION k3s-master Ready control-plane,master 37s v1.21.5+k3s2 kubectl config get-clusters NAME default kubectl cluster-info Kubernetes control plane is running at https://127.0.0.1:6443 CoreDNS is running at https://127.0.0.1:6443/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy Metrics-server is running at https://127.0.0.1:6443/api/v1/namespaces/kube-system/services/https:metrics-server:/proxy To further debug and diagnose cluster problems, use 'kubectl cluster-info dump'. kubectl get namespaces NAME STATUS AGE default Active 27m kube-system Active 27m kube-public Active 27m kube-node-lease Active 27m kubectl get endpoints -n kube-system NAME ENDPOINTS AGE kube-dns 10.42.0.4:53,10.42.0.4:53,10.42.0.4:9153 27m metrics-server 10.42.0.3:443 27m rancher.io-local-path <none> 27m kubectl get pods -n kube-system NAME READY STATUS RESTARTS AGE helm-install-traefik-crd-ql7j2 0/1 Pending 0 32m helm-install-traefik-mr65j 0/1 Pending 0 32m coredns-7448499f4d-x57z7 1/1 Running 0 32m metrics-server-86cbb8457f-cg2fs 1/1 Running 0 32m local-path-provisioner-5ff76fc89d-kdfcl 1/1 Running 0 32m You need to extract a token form the master that will be used to join the nodes to the master. On the master node: sudo cat /var/lib/rancher/k3s/server/node-token You will then obtain a token that looks like: K1097aace305b0c1077fc854547f34a598d23330ff047ddeed8beb3c428b38a1ca7::server:6cc9fbb6c5c9de96f37fb14b5535c778 Configure K3s on worker nodes to join the cluster Run the below command on both of the worker nodes i.e. k3s-worker1 and k3s-worker2 that you want to join the cluster. SSH into k3s-worker1 and k3s-worker1 machine Switch to root user: sudo su Execute the below command to join the cluster using the token obtained from the master node: To install K3s on worker nodes and add them to the cluster, run the installation script with the K3S_URL and K3S_TOKEN environment variables. Here is an example showing how to join a worker node: curl -sfL https://get.k3s.io | K3S_URL=https://<Master_IP>:6443 \\ K3S_TOKEN=<Join_Token> sh - Where is the Internal IP of the master node and is the token obtained from the master node. For example, curl -sfL https://get.k3s.io | K3S_URL=https://192.168.0.154:6443 \\ K3S_TOKEN=K1019827f88b77cc5e1dce04d692d445c1015a578dafdc56aca829b2f 501df9359a::server:1bf0d61c85c6dac6d5a0081da55f44ba sh - You can verify if the k3s-agent on both of the worker nodes is running by: systemctl status k3s-agent The output looks like: To verify that our nodes have successfully been added to the cluster, run the following command on master node: k3s kubectl get nodes OR, k3s kubectl get nodes -o wide Your output should look like: k3s kubectl get nodes NAME STATUS ROLES AGE VERSION k3s-worker1 Ready <none> 5m16s v1.21.5+k3s2 k3s-worker2 Ready <none> 5m5s v1.21.5+k3s2 k3s-master Ready control-plane,master 9m33s v1.21.5+k3s2 This shows that we have successfully setup our K3s cluster ready to deploy applications to it. Deploying Nginx using deployment Create a deployment nginx.yaml on master node vi nginx.yaml The nginx.yaml looks like this: apiVersion: apps/v1 kind: Deployment metadata: name: mysite labels: app: mysite spec: replicas: 1 selector: matchLabels: app: mysite template: metadata: labels: app : mysite spec: containers: - name : mysite image: nginx ports: - containerPort: 80 kubectl apply -f nginx.yaml Verify the nginx pod is on Running state: sudo k3s kubectl get pods --all-namespaces Scale the pods to available agents: sudo k3s kubectl scale --replicas=2 deploy/mysite View all deployment status: sudo k3s kubectl get deploy mysite NAME READY UP-TO-DATE AVAILABLE AGE mysite 2/2 2 2 85s Delete the nginx deployment and pod: sudo k3s kubectl delete -f nginx.yaml OR, sudo k3s kubectl delete deploy mysite Note Instead of apply manually any new deployment yaml, you can just copy the yaml file to the /var/lib/rancher/k3s/server/manifests/ folder i.e. sudo cp nginx.yaml /var/lib/rancher/k3s/server/manifests/. . This will automatically deploy the newly copied deployment on your cluster. Deploy Addons to K3s K3s is a lightweight kubernetes tool that doesn\u2019t come packaged with all the tools but you can install them separately. Install Helm Commandline tool on K3s: i. Download the latest version of Helm commandline tool using wget from this page . wget https://get.helm.sh/helm-v3.7.0-linux-amd64.tar.gz ii. Unpack it: tar -zxvf helm-v3.7.0-linux-amd64.tar.gz iii. Find the helm binary in the unpacked directory, and move it to its desired destination mv linux-amd64/helm /usr/bin/helm chmod +x /usr/bin/helm OR, Using Snap: snap install helm --classic OR, Using Apt (Debian/Ubuntu): curl https://baltocdn.com/helm/signing.asc | sudo apt-key add - sudo apt-get install apt-transport-https --yes echo \"deb https://baltocdn.com/helm/stable/debian/ all main\" | sudo tee /etc/apt/sources.list.d/helm-stable-debian.list sudo apt-get update sudo apt-get install helm Verify the Helm installation: helm version version.BuildInfo{Version:\"v3.7.0\", GitCommit:\"eeac83883cb4014fe60267ec63735 70374ce770b\", GitTreeState:\"clean\", GoVersion:\"go1.16.8\"} Add the helm chart repository to allow installation of applications using helm: helm repo add stable https://charts.helm.sh/stable helm repo update Deploy A Sample Nginx Application using Helm Nginx can be used as a web proxy to expose ingress web traffic routes in and out of the cluster. You can install \"nginx web-proxy\" using Helm: export KUBECONFIG=/etc/rancher/k3s/k3s.yaml helm repo add ingress-nginx https://kubernetes.github.io/ingress-nginx helm repo list helm repo update helm install stable ingress-nginx/ingress-nginx --namespace kube-system \\ --set defaultBackend.enabled=false --set controller.publishService.enabled=true We can test if the application has been installed by: k3s kubectl get pods -n kube-system -l app=nginx-ingress -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES nginx.. 1/1 Running 0 19m 10.42.1.5 k3s-worker1 <none> <none> We have successfully deployed nginx web-proxy on k3s. Go to browser, visit http://<Master-Floating-IP> i.e. http://128.31.25.246 to check the nginx default page. Upgrade K3s Using the Installation Script To upgrade K3s from an older version you can re-run the installation script using the same flags, for example: curl -sfL https://get.k3s.io | sh - This will upgrade to a newer version in the stable channel by default. If you want to upgrade to a newer version in a specific channel (such as latest) you can specify the channel: curl -sfL https://get.k3s.io | INSTALL_K3S_CHANNEL=latest sh - If you want to upgrade to a specific version you can run the following command: curl -sfL https://get.k3s.io | INSTALL_K3S_VERSION=vX.Y.Z-rc1 sh - From non root user's terminal to install the latest version, you do not need to pass INSTALL_K3S_VERSION that by default loads the Latest version . curl -sfL https://get.k3s.io | INSTALL_K3S_EXEC=\"--write-kubeconfig-mode 644\" \\ sh - Note For more about on \"How to use flags and environment variables\" read this . Restarting K3s Restarting K3s is supported by the installation script for systemd and OpenRC . Using systemd : To restart servers manually: sudo systemctl restart k3s To restart agents manually: sudo systemctl restart k3s-agent Using OpenRC : To restart servers manually: sudo service k3s restart To restart agents manually: sudo service k3s-agent restart Uninstalling If you installed K3s with the help of the install.sh script, an uninstall script is generated during installation. The script is created on your master node at /usr/bin/k3s-uninstall.sh or as k3s-agent-uninstall.sh on your worker nodes. To remove K3s on the worker nodes, execute: sudo /usr/bin/k3s-agent-uninstall.sh sudo rm -rf /var/lib/rancher To remove k3s on the master node, execute: sudo /usr/bin/k3s-uninstall.sh sudo rm -rf /var/lib/rancher","title":"K3s"},{"location":"other-tools/kubernetes/k3s/k3s/#k3s","text":"","title":"K3s"},{"location":"other-tools/kubernetes/k3s/k3s/#features","text":"Lightweight certified K8s distro Built for production operations 40MB binary, 250MB memeory consumption Single process w/ integrated K8s master, Kubelet, and containerd Supports not only etcd to hold the cluster state, but also SQLite (for single-node, simpler setups) or external DBs like MySQL and PostgreSQL Open source project","title":"Features"},{"location":"other-tools/kubernetes/k3s/k3s/#components-and-architecure","text":"High-Availability K3s Server with an External DB: or, For this kind of high availability k3s setup read this .","title":"Components and architecure"},{"location":"other-tools/kubernetes/k3s/k3s/#pre-requisite","text":"We will need 1 control-plane(master) and 2 worker nodes to create a single control-plane kubernetes cluster using k3s . We are using following setting for this purpose: 1 Linux machine for master, ubuntu-22.04-x86_64 or your choice of Ubuntu OS image, cpu-a.2 flavor with 2vCPU, 4GB RAM, 20GB storage - also assign Floating IP to the master node. 2 Linux machines for worker, ubuntu-22.04-x86_64 or your choice of Ubuntu OS image, cpu-a.1 flavor with 1vCPU, 2GB RAM, 20GB storage. ssh access to all machines: Read more here on how to setup SSH to your remote VMs.","title":"Pre-requisite"},{"location":"other-tools/kubernetes/k3s/k3s/#networking","text":"The K3s server needs port 6443 to be accessible by all nodes. The nodes need to be able to reach other nodes over UDP port 8472 when Flannel VXLAN overlay networking is used. The node should not listen on any other port. K3s uses reverse tunneling such that the nodes make outbound connections to the server and all kubelet traffic runs through that tunnel. However, if you do not use Flannel and provide your own custom CNI, then port 8472 is not needed by K3s. If you wish to utilize the metrics server , you will need to open port 10250 on each node. If you plan on achieving high availability with embedded etcd , server nodes must be accessible to each other on ports 2379 and 2380 . Create 1 security group with appropriate Inbound Rules for K3s Server Nodes that will be used by all 3 nodes: Important Note The VXLAN overlay networking port on nodes should not be exposed to the world as it opens up your cluster network to be accessed by anyone. Run your nodes behind a firewall/security group that disables access to port 8472 . setup Unique hostname to each machine using the following command: echo \"<node_internal_IP> <host_name>\" >> /etc/hosts hostnamectl set-hostname <host_name> For example, echo \"192.168.0.235 k3s-master\" >> /etc/hosts hostnamectl set-hostname k3s-master In this step, you will install kubelet and kubeadm on the below nodes k3s-master k3s-worker1 k3s-worker2 The below steps will be performed on all the above mentioned nodes: SSH into all the 3 machines Switch as root: sudo su Update the repositories and packages: apt-get update && apt-get upgrade -y Install curl and apt-transport-https apt-get update && apt-get install -y apt-transport-https curl","title":"Networking"},{"location":"other-tools/kubernetes/k3s/k3s/#install-docker","text":"Install container runtime - docker apt-get install docker.io -y Configure the Docker daemon, in particular to use systemd for the management of the container\u2019s cgroups cat <<EOF | sudo tee /etc/docker/daemon.json { \"exec-opts\": [\"native.cgroupdriver=systemd\"] } EOF systemctl enable --now docker usermod -aG docker ubuntu systemctl daemon-reload systemctl restart docker","title":"Install Docker"},{"location":"other-tools/kubernetes/k3s/k3s/#configure-k3s-to-bootstrap-the-cluster-on-master-node","text":"Run the below command on the master node i.e. k3s-master that you want to setup as control plane. SSH into k3s-master machine Switch to root user: sudo su Execute the below command to initialize the cluster: curl -sfL https://get.k3s.io | sh -s - --kubelet-arg 'cgroup-driver=systemd' \\ --node-taint CriticalAddonsOnly=true:NoExecute --docker OR, If you don't want to setup the K3s cluster without using docker as the container runtime, then just run without supplying the --docker argument. curl -sfL https://get.k3s.io | sh - After running this installation: The K3s service will be configured to automatically restart after node reboots or if the process crashes or is killed Additional utilities will be installed, including kubectl , crictl , ctr , k3s-killall.sh , and k3s-uninstall.sh A kubeconfig file will be written to /etc/rancher/k3s/k3s.yaml and the kubectl installed by K3s will automatically use it. To check if the service installed successfully, you can use: systemctl status k3s The output looks like: OR, k3s --version kubectl version Note If you want to taint the node i.e. not to deploy pods on this node after installation then run: kubectl taint nodes <master_node_name> k3s-controlplane=true:NoExecure i.e. kubectl taint nodes k3s-master k3s-controlplane=true:NoExecure You can check if the master node is working by: k3s kubectl get nodes NAME STATUS ROLES AGE VERSION k3s-master Ready control-plane,master 37s v1.21.5+k3s2 kubectl config get-clusters NAME default kubectl cluster-info Kubernetes control plane is running at https://127.0.0.1:6443 CoreDNS is running at https://127.0.0.1:6443/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy Metrics-server is running at https://127.0.0.1:6443/api/v1/namespaces/kube-system/services/https:metrics-server:/proxy To further debug and diagnose cluster problems, use 'kubectl cluster-info dump'. kubectl get namespaces NAME STATUS AGE default Active 27m kube-system Active 27m kube-public Active 27m kube-node-lease Active 27m kubectl get endpoints -n kube-system NAME ENDPOINTS AGE kube-dns 10.42.0.4:53,10.42.0.4:53,10.42.0.4:9153 27m metrics-server 10.42.0.3:443 27m rancher.io-local-path <none> 27m kubectl get pods -n kube-system NAME READY STATUS RESTARTS AGE helm-install-traefik-crd-ql7j2 0/1 Pending 0 32m helm-install-traefik-mr65j 0/1 Pending 0 32m coredns-7448499f4d-x57z7 1/1 Running 0 32m metrics-server-86cbb8457f-cg2fs 1/1 Running 0 32m local-path-provisioner-5ff76fc89d-kdfcl 1/1 Running 0 32m You need to extract a token form the master that will be used to join the nodes to the master. On the master node: sudo cat /var/lib/rancher/k3s/server/node-token You will then obtain a token that looks like: K1097aace305b0c1077fc854547f34a598d23330ff047ddeed8beb3c428b38a1ca7::server:6cc9fbb6c5c9de96f37fb14b5535c778","title":"Configure K3s to bootstrap the cluster on master node"},{"location":"other-tools/kubernetes/k3s/k3s/#configure-k3s-on-worker-nodes-to-join-the-cluster","text":"Run the below command on both of the worker nodes i.e. k3s-worker1 and k3s-worker2 that you want to join the cluster. SSH into k3s-worker1 and k3s-worker1 machine Switch to root user: sudo su Execute the below command to join the cluster using the token obtained from the master node: To install K3s on worker nodes and add them to the cluster, run the installation script with the K3S_URL and K3S_TOKEN environment variables. Here is an example showing how to join a worker node: curl -sfL https://get.k3s.io | K3S_URL=https://<Master_IP>:6443 \\ K3S_TOKEN=<Join_Token> sh - Where is the Internal IP of the master node and is the token obtained from the master node. For example, curl -sfL https://get.k3s.io | K3S_URL=https://192.168.0.154:6443 \\ K3S_TOKEN=K1019827f88b77cc5e1dce04d692d445c1015a578dafdc56aca829b2f 501df9359a::server:1bf0d61c85c6dac6d5a0081da55f44ba sh - You can verify if the k3s-agent on both of the worker nodes is running by: systemctl status k3s-agent The output looks like: To verify that our nodes have successfully been added to the cluster, run the following command on master node: k3s kubectl get nodes OR, k3s kubectl get nodes -o wide Your output should look like: k3s kubectl get nodes NAME STATUS ROLES AGE VERSION k3s-worker1 Ready <none> 5m16s v1.21.5+k3s2 k3s-worker2 Ready <none> 5m5s v1.21.5+k3s2 k3s-master Ready control-plane,master 9m33s v1.21.5+k3s2 This shows that we have successfully setup our K3s cluster ready to deploy applications to it.","title":"Configure K3s on worker nodes to join the cluster"},{"location":"other-tools/kubernetes/k3s/k3s/#deploying-nginx-using-deployment","text":"Create a deployment nginx.yaml on master node vi nginx.yaml The nginx.yaml looks like this: apiVersion: apps/v1 kind: Deployment metadata: name: mysite labels: app: mysite spec: replicas: 1 selector: matchLabels: app: mysite template: metadata: labels: app : mysite spec: containers: - name : mysite image: nginx ports: - containerPort: 80 kubectl apply -f nginx.yaml Verify the nginx pod is on Running state: sudo k3s kubectl get pods --all-namespaces Scale the pods to available agents: sudo k3s kubectl scale --replicas=2 deploy/mysite View all deployment status: sudo k3s kubectl get deploy mysite NAME READY UP-TO-DATE AVAILABLE AGE mysite 2/2 2 2 85s Delete the nginx deployment and pod: sudo k3s kubectl delete -f nginx.yaml OR, sudo k3s kubectl delete deploy mysite Note Instead of apply manually any new deployment yaml, you can just copy the yaml file to the /var/lib/rancher/k3s/server/manifests/ folder i.e. sudo cp nginx.yaml /var/lib/rancher/k3s/server/manifests/. . This will automatically deploy the newly copied deployment on your cluster.","title":"Deploying Nginx using deployment"},{"location":"other-tools/kubernetes/k3s/k3s/#deploy-addons-to-k3s","text":"K3s is a lightweight kubernetes tool that doesn\u2019t come packaged with all the tools but you can install them separately. Install Helm Commandline tool on K3s: i. Download the latest version of Helm commandline tool using wget from this page . wget https://get.helm.sh/helm-v3.7.0-linux-amd64.tar.gz ii. Unpack it: tar -zxvf helm-v3.7.0-linux-amd64.tar.gz iii. Find the helm binary in the unpacked directory, and move it to its desired destination mv linux-amd64/helm /usr/bin/helm chmod +x /usr/bin/helm OR, Using Snap: snap install helm --classic OR, Using Apt (Debian/Ubuntu): curl https://baltocdn.com/helm/signing.asc | sudo apt-key add - sudo apt-get install apt-transport-https --yes echo \"deb https://baltocdn.com/helm/stable/debian/ all main\" | sudo tee /etc/apt/sources.list.d/helm-stable-debian.list sudo apt-get update sudo apt-get install helm Verify the Helm installation: helm version version.BuildInfo{Version:\"v3.7.0\", GitCommit:\"eeac83883cb4014fe60267ec63735 70374ce770b\", GitTreeState:\"clean\", GoVersion:\"go1.16.8\"} Add the helm chart repository to allow installation of applications using helm: helm repo add stable https://charts.helm.sh/stable helm repo update","title":"Deploy Addons to K3s"},{"location":"other-tools/kubernetes/k3s/k3s/#deploy-a-sample-nginx-application-using-helm","text":"Nginx can be used as a web proxy to expose ingress web traffic routes in and out of the cluster. You can install \"nginx web-proxy\" using Helm: export KUBECONFIG=/etc/rancher/k3s/k3s.yaml helm repo add ingress-nginx https://kubernetes.github.io/ingress-nginx helm repo list helm repo update helm install stable ingress-nginx/ingress-nginx --namespace kube-system \\ --set defaultBackend.enabled=false --set controller.publishService.enabled=true We can test if the application has been installed by: k3s kubectl get pods -n kube-system -l app=nginx-ingress -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES nginx.. 1/1 Running 0 19m 10.42.1.5 k3s-worker1 <none> <none> We have successfully deployed nginx web-proxy on k3s. Go to browser, visit http://<Master-Floating-IP> i.e. http://128.31.25.246 to check the nginx default page.","title":"Deploy A Sample Nginx Application using Helm"},{"location":"other-tools/kubernetes/k3s/k3s/#upgrade-k3s-using-the-installation-script","text":"To upgrade K3s from an older version you can re-run the installation script using the same flags, for example: curl -sfL https://get.k3s.io | sh - This will upgrade to a newer version in the stable channel by default. If you want to upgrade to a newer version in a specific channel (such as latest) you can specify the channel: curl -sfL https://get.k3s.io | INSTALL_K3S_CHANNEL=latest sh - If you want to upgrade to a specific version you can run the following command: curl -sfL https://get.k3s.io | INSTALL_K3S_VERSION=vX.Y.Z-rc1 sh - From non root user's terminal to install the latest version, you do not need to pass INSTALL_K3S_VERSION that by default loads the Latest version . curl -sfL https://get.k3s.io | INSTALL_K3S_EXEC=\"--write-kubeconfig-mode 644\" \\ sh - Note For more about on \"How to use flags and environment variables\" read this .","title":"Upgrade K3s Using the Installation Script"},{"location":"other-tools/kubernetes/k3s/k3s/#restarting-k3s","text":"Restarting K3s is supported by the installation script for systemd and OpenRC . Using systemd : To restart servers manually: sudo systemctl restart k3s To restart agents manually: sudo systemctl restart k3s-agent Using OpenRC : To restart servers manually: sudo service k3s restart To restart agents manually: sudo service k3s-agent restart","title":"Restarting K3s"},{"location":"other-tools/kubernetes/k3s/k3s/#uninstalling","text":"If you installed K3s with the help of the install.sh script, an uninstall script is generated during installation. The script is created on your master node at /usr/bin/k3s-uninstall.sh or as k3s-agent-uninstall.sh on your worker nodes. To remove K3s on the worker nodes, execute: sudo /usr/bin/k3s-agent-uninstall.sh sudo rm -rf /var/lib/rancher To remove k3s on the master node, execute: sudo /usr/bin/k3s-uninstall.sh sudo rm -rf /var/lib/rancher","title":"Uninstalling"},{"location":"other-tools/kubernetes/kubeadm/HA-clusters-with-kubeadm/","text":"Highly Available Kubernetes Cluster using kubeadm Objectives Install a multi control-plane(master) Kubernetes cluster Install a Pod network on the cluster so that your Pods can talk to each other Deploy and test a sample app Deploy K8s Dashboard to view all cluster's components Components and architecure This shows components and architecture of a highly-available, production-grade Kubernetes cluster. You can learn about each component from Kubernetes Componets . Pre-requisite You will need 2 control-plane(master node) and 2 worker nodes to create a multi-master kubernetes cluster using kubeadm . You are going to use the following set up for this purpose: 2 Linux machines for master, ubuntu-22.04-x86_64 or your choice of Ubuntu OS image, cpu-a.2 flavor with 2vCPU, 4GB RAM, 20GB storage. 2 Linux machines for worker, ubuntu-22.04-x86_64 or your choice of Ubuntu OS image, cpu-a.1 flavor with 1vCPU, 2GB RAM, 20GB storage - also assign Floating IPs to both of the worker nodes. 1 Linux machine for loadbalancer, ubuntu-22.04-x86_64 or your choice of Ubuntu OS image, cpu-a.1 flavor with 1vCPU, 2GB RAM, 20GB storage. ssh access to all machines: Read more here on how to setup SSH to your remote VMs. Create 2 security groups with appropriate ports and protocols : i. To be used by the master nodes: ii. To be used by the worker nodes: - setup Unique hostname to each machine using the following command: echo \"<node_internal_IP> <host_name>\" >> /etc/hosts hostnamectl set-hostname <host_name> For example, echo \"192.168.0.167 loadbalancer\" >> /etc/hosts hostnamectl set-hostname loadbalancer Steps Prepare the Loadbalancer node to communicate with the two master nodes' apiservers on their IPs via port 6443. Do following in all the nodes except the Loadbalancer node: Disable swap. Install kubelet and kubeadm . Install container runtime - you will be using Docker . Initiate kubeadm control plane configuration on one of the master nodes. Save the new master and worker node join commands with the token. Join the second master node to the control plane using the join command. Join the worker nodes to the control plane using the join command. Configure kubeconfig( $HOME/.kube/config ) on loadbalancer node. Install kubectl on Loadbalancer node. Install CNI network plugin i.e. Flannel on Loadbalancer node. Validate all cluster components and nodes are visible on Loadbalancer node. Deploy a sample app and validate the app from Loadbalancer node. Setting up loadbalancer You will use HAPROXY as the primary loadbalancer, but you can use any other options as well. This node will be not part of the K8s cluster but will be outside of the cluster and interacts with the cluster using ports. You have 2 master nodes. Which means the user can connect to either of the 2 apiservers. The loadbalancer will be used to loadbalance between the 2 apiservers. Login to the loadbalancer node Switch as root - sudo su Update your repository and your system sudo apt-get update && sudo apt-get upgrade -y Install haproxy sudo apt-get install haproxy -y Edit haproxy configuration vi /etc/haproxy/haproxy.cfg Add the below lines to create a frontend configuration for loadbalancer - frontend fe-apiserver bind 0.0.0.0:6443 mode tcp option tcplog default_backend be-apiserver Add the below lines to create a backend configuration for master1 and master2 nodes at port 6443 . Note 6443 is the default port of kube-apiserver backend be-apiserver mode tcp option tcplog option tcp-check balance roundrobin default-server inter 10s downinter 5s rise 2 fall 2 slowstart 60s maxconn 250 maxqueue 256 weight 100 #<!-- markdownlint-disable --> server master1 10.138.0.15:6443 check server master2 10.138.0.16:6443 check Here - master1 and master2 are the hostnames of the master nodes and 10.138.0.15 and 10.138.0.16 are the corresponding internal IP addresses. Restart and Verify haproxy systemctl restart haproxy systemctl status haproxy Ensure haproxy config file is correctly formatted: haproxy -c -q -V -f /etc/haproxy/haproxy.cfg Ensure haproxy is in running status. Run nc command as below: nc -v localhost 6443 Connection to localhost 6443 port [tcp/*] succeeded! Note If you see failures for master1 and master2 connectivity, you can ignore them for time being as you have not yet installed anything on the servers. Install kubeadm, kubelet and docker on master and worker nodes kubeadm will not install or manage kubelet or kubectl for you, so you will need to ensure they match the version of the Kubernetes control plane you want kubeadm to install for you. You will install these packages on all of your machines: \u2022 kubeadm : the command to bootstrap the cluster. \u2022 kubelet : the component that runs on all of the machines in your cluster and does things like starting pods and containers. \u2022 kubectl : the command line util to talk to your cluster. In this step, you will install kubelet and kubeadm on the below nodes master1 master2 worker1 worker2 The below steps will be performed on all the above mentioned nodes: SSH into all the 4 machines Switch as root: sudo su Update the repositories and packages: apt-get update && apt-get upgrade -y Turn off swap swapoff -a sed -i '/ swap / s/^/#/' /etc/fstab Install curl and apt-transport-https apt-get update && apt-get install -y apt-transport-https curl Download the Google Cloud public signing key and add key to verify releases curl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | apt-key add - add kubernetes apt repo cat <<EOF | sudo tee /etc/apt/sources.list.d/kubernetes.list deb https://apt.kubernetes.io/ kubernetes-xenial main EOF Install kubelet and kubeadm apt-get update apt-get install -y kubelet kubeadm apt-mark hold is used so that these packages will not be updated/removed automatically apt-mark hold kubelet kubeadm Install Docker on master and worker nodes Install container runtime - docker sudo apt-get install docker.io -y Configure the Docker daemon, in particular to use systemd for the management of the container\u2019s cgroups cat <<EOF | sudo tee /etc/docker/daemon.json { \"exec-opts\": [\"native.cgroupdriver=systemd\"] } EOF systemctl enable --now docker usermod -aG docker ubuntu systemctl daemon-reload systemctl restart docker Ensure net.bridge.bridge-nf-call-iptables is set to 1 in your sysctl config For more Read this . sysctl net.bridge.bridge-nf-call-iptables=1 Configure kubeadm to bootstrap the cluster You will start off by initializing only one master node. For this purpose, you choose master1 to initialize our first control plane but you can also do the same in master2 . SSH into master1 machine Switch to root user: sudo su Execute the below command to initialize the cluster: kubeadm config images pull kubeadm init --control-plane-endpoint \"LOAD_BALANCER_IP_OR_HOSTNAME:LOAD_BALANCER_PORT\" --upload-certs --pod-network-cidr=10.244.0.0/16 Here, you can use either the IP address or the hostname of the loadbalancer in place of . You have not enabled the hostname of the server, i.e. loadbalancer as the LOAD_BALANCER_IP_OR_HOSTNAME that is visible from the master1 node. so instead of using not resolvable hostnames across your network, you will be using the IP address of the Loadbalancer server. The is the front end configuration port defined in HAPROXY configuration. For this, you have kept the port as 6443 which is the default apiserver port. Important Note --pod-network-cidr value depends upon what CNI plugin you going to use so need to be very careful while setting this CIDR values. In our case, you are going to use Flannel CNI network plugin so you will use: --pod-network-cidr=10.244.0.0/16 . If you are opted to use Calico CNI network plugin then you need to use: --pod-network-cidr=192.168.0.0/16 and if you are opted to use Weave Net no need to pass this parameter. For example, our Flannel CNI network plugin based kubeadm init command with loadbalancer node with internal IP: 192.168.0.167 look like below: kubeadm config images pull kubeadm init --control-plane-endpoint \"192.168.0.167:6443\" --upload-certs --pod-network-cidr=10.244.0.0/16 Save the output in some secure file for future use. This will show an unique token to join the control plane. The output from kubeadm init should looks like below: Your Kubernetes control-plane has initialized successfully! To start using your cluster, you need to run the following as a regular user: mkdir -p $HOME/.kube sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config sudo chown $(id -u):$(id -g) $HOME/.kube/config Alternatively, if you are the root user, you can run: export KUBECONFIG=/etc/kubernetes/admin.conf You should now deploy a pod network to the cluster. Run \"kubectl apply -f [podnetwork].yaml\" with one of the options listed at: https://kubernetes.io/docs/concepts/cluster-administration/addons/ You can now join any number of the control-plane node running the following command on each as root: kubeadm join 192.168.0.167:6443 --token cnslau.kd5fjt96jeuzymzb \\ --discovery-token-ca-cert-hash sha256:871ab3f050bc9790c977daee9e44cf52e15ee3 7ab9834567333b939458a5bfb5 \\ --control-plane --certificate-key 824d9a0e173a810416b4bca7038fb33b616108c17abcbc5eaef8651f11e3d146 Please note that the certificate-key gives access to cluster sensitive data, keep it secret! As a safeguard, uploaded-certs will be deleted in two hours; If necessary, you can use \"kubeadm init phase upload-certs --upload-certs\" to reload certs afterward. Then you can join any number of worker nodes by running the following on each as root: kubeadm join 192.168.0.167:6443 --token cnslau.kd5fjt96jeuzymzb \\ --discovery-token-ca-cert-hash sha256:871ab3f050bc9790c977daee9e44cf52e15ee37ab9834567333b939458a5bfb5 The output consists of 3 major tasks: A. Setup kubeconfig using on current master node: As you are running as root user so you need to run the following command: export KUBECONFIG=/etc/kubernetes/admin.conf We need to run the below commands as a normal user to use the kubectl from terminal. mkdir -p $HOME/.kube sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config sudo chown $(id -u):$(id -g) $HOME/.kube/config Now the machine is initialized as master. Warning Kubeadm signs the certificate in the admin.conf to have Subject: O = system:masters, CN = kubernetes-admin. system:masters is a break-glass, super user group that bypasses the authorization layer (e.g. RBAC). Do not share the admin.conf file with anyone and instead grant users custom permissions by generating them a kubeconfig file using the kubeadm kubeconfig user command. B. Setup a new control plane (master) i.e. master2 by running following command on master2 node: kubeadm join 192.168.0.167:6443 --token cnslau.kd5fjt96jeuzymzb \\ --discovery-token-ca-cert-hash sha256:871ab3f050bc9790c977daee9e44cf52e1 5ee37ab9834567333b939458a5bfb5 \\ --control-plane --certificate-key 824d9a0e173a810416b4bca7038fb33b616108c17abcbc5eaef8651f11e3d146 C. Join worker nodes running following command on individual workder nodes: kubeadm join 192.168.0.167:6443 --token cnslau.kd5fjt96jeuzymzb \\ --discovery-token-ca-cert-hash sha256:871ab3f050bc9790c977daee9e44cf52e15ee37ab9834567333b939458a5bfb5 Important Note Your output will be different than what is provided here. While performing the rest of the demo, ensure that you are executing the command provided by your output and dont copy and paste from here. If you do not have the token, you can get it by running the following command on the control-plane node: kubeadm token list The output is similar to this: TOKEN TTL EXPIRES USAGES DESCRIPTION EXTRA GROUPS 8ewj1p... 23h 2018-06-12 authentication, The default bootstrap system: signing token generated by bootstrappers: 'kubeadm init'. kubeadm: default-node-token If you missed the join command, execute the following command kubeadm token create --print-join-command in the master node to recreate the token with the join command. root@master:~$ kubeadm token create --print-join-command kubeadm join 10.2.0.4:6443 --token xyzeyi.wxer3eg9vj8hcpp2 \\ --discovery-token-ca-cert-hash sha256:ccfc92b2a31b002c3151cdbab77ff4dc32ef13b213fa3a9876e126831c76f7fa By default, tokens expire after 24 hours. If you are joining a node to the cluster after the current token has expired, you can create a new token by running the following command on the control-plane node: kubeadm token create The output is similar to this: 5didvk.d09sbcov8ph2amjw We can use this new token to join: $ kubeadm join <master-ip>:<master-port> --token <token> \\ --discovery-token-ca-cert-hash sha256:<hash> SSH into master2 Switch to root user: sudo su Check the command provided by the output of master1 : You can now use the below command to add another control-plane node(master) to the control plane: kubeadm join 192.168.0.167:6443 --token cnslau.kd5fjt96jeuzymzb --discovery-token-ca-cert-hash sha256:871ab3f050bc9790c977daee9e44cf52e15ee3 7ab9834567333b939458a5bfb5 \\ --control-plane --certificate-key 824d9a0e173a810416b4bca7038fb33b616108c17abcbc5eaef8651f11e3d146 Execute the kubeadm join command for control plane on master2 Your output should look like: This node has joined the cluster and a new control plane instance was created: * Certificate signing request was sent to apiserver and approval was received. * The Kubelet was informed of the new secure connection details. * Control plane (master) label and taint were applied to the new node. * The Kubernetes control plane instances scaled up. * A new etcd member was added to the local/stacked etcd cluster. Now that you have initialized both the masters - you can now work on bootstrapping the worker nodes. SSH into worker1 and worker2 Switch to root user on both the machines: sudo su Check the output given by the init command on master1 to join worker node: kubeadm join 192.168.0.167:6443 --token cnslau.kd5fjt96jeuzymzb \\ --discovery-token-ca-cert-hash sha256:871ab3f050bc9790c977daee9e44cf52e15ee37ab9834567333b939458a5bfb5 Execute the above command on both the nodes: Your output should look like: This node has joined the cluster: * Certificate signing request was sent to apiserver and a response was received. * The Kubelet was informed of the new secure connection details. Configure kubeconfig on loadbalancer node Now that you have configured the master and the worker nodes, its now time to configure Kubeconfig ( .kube ) on the loadbalancer node. It is completely up to you if you want to use the loadbalancer node to setup kubeconfig. kubeconfig can also be setup externally on a separate machine which has access to loadbalancer node. For the purpose of this demo you will use loadbalancer node to host kubeconfig and kubectl . SSH into loadbalancer node Switch to root user: sudo su Create a directory: .kube at $HOME of root user mkdir -p $HOME/.kube SCP configuration file from any one master node to loadbalancer node scp master1:/etc/kubernetes/admin.conf $HOME/.kube/config Important Note If you havent setup ssh connection between master node and loadbalancer, you can manually copy content of the file /etc/kubernetes/admin.conf from master1 node and then paste it to $HOME/.kube/config file on the loadbalancer node. Ensure that the kubeconfig file path is $HOME/.kube/config on the loadbalancer node. Provide appropriate ownership to the copied file chown $(id -u):$(id -g) $HOME/.kube/config Install kubectl Install kubectl binary \u2022 kubectl : the command line util to talk to your cluster. snap install kubectl --classic This outputs: kubectl 1.22.2 from Canonical\u2713 installed Verify the cluster kubectl get nodes NAME STATUS ROLES AGE VERSION master1 NotReady control-plane,master 21m v1.16.2 master2 NotReady control-plane,master 15m v1.16.2 worker1 Ready <none> 9m17s v1.16.2 worker2 Ready <none> 9m25s v1.16.2 Install CNI network plugin CNI overview Managing a network where containers can interoperate efficiently is very important. Kubernetes has adopted the Container Network Interface(CNI) specification for managing network resources on a cluster. This relatively simple specification makes it easy for Kubernetes to interact with a wide range of CNI-based software solutions. Using this CNI plugin allows Kubernetes pods to have the same IP address inside the pod as they do on the VPC network. Make sure the configuration corresponds to the Pod CIDR specified in the kubeadm configuration file if applicable. You must deploy a CNI based Pod network add-on so that your Pods can communicate with each other. Cluster DNS (CoreDNS) will not start up before a network is installed. To verify you can run this command: kubectl get po -n kube-system : You should see the following output. You will see the two coredns-* pods in a pending state. It is the expected behavior. Once we install the network plugin, it will be in a Running state. Output Example: root@loadbalancer:~$ kubectl get po -n kube-system NAME READY STATUS RESTARTS AGE coredns-558bd4d5db-5jktc 0/1 Pending 0 10m coredns-558bd4d5db-xdc5x 0/1 Pending 0 10m etcd-master1 1/1 Running 0 11m kube-apiserver-master1 1/1 Running 0 11m kube-controller-manager-master1 1/1 Running 0 11m kube-proxy-5jfh5 1/1 Running 0 10m kube-scheduler-master1 1/1 Running 0 11m Supported CNI options To read more about the currently supported base CNI solutions for Kubernetes read here and also read this . The below command can be run on the Loadbalancer node to install the CNI plugin: kubectl apply -f https://github.com/coreos/flannel/raw/master/Documentation/kube-flannel.yml As you had passed --pod-network-cidr=10.244.0.0/16 with kubeadm init so this should work for Flannel CNI. Using Other CNI Options For Calico CNI plugin to work correctly, you need to pass --pod-network-cidr=192.168.0.0/16 with kubeadm init and then you can run: kubectl apply -f https://docs.projectcalico.org/v3.8/manifests/calico.yaml For Weave Net CNI plugin to work correctly, you don't need to pass --pod-network-cidr with kubeadm init and then you can run: kubectl apply -f \"https://cloud.weave.works/k8s/net?k8s-version=$(kubectl version | base64 | tr -d '\\n')\" Dual Network: It is highly recommended to follow an internal/external network layout for your cluster, as showed in this diagram: To enable this just give two different names to the internal and external interface, according to your distro of choiche naming scheme: external_interface: eth0 internal_interface: eth1 Also you can decide here what CIDR should your cluster use cluster_cidr: 10.43.0.0/16 service_cidr: 10.44.0.0/16 Once you successfully installed the Flannel CNI component to your cluster. You can now verify your HA cluster running: kubectl get nodes NAME STATUS ROLES AGE VERSION master1 Ready control-plane,master 22m v1.16.2 master2 Ready control-plane,master 17m v1.16.2 worker1 Ready <none> 10m v1.16.2 worker2 Ready <none> 10m v1.16.2 Deploy A Sample Nginx Application Now that we have all the components to make the cluster and applications work, let\u2019s deploy a sample Nginx application and see if we can access it over a NodePort that has port range of 30000-32767 . The below command can be run on: kubectl run nginx --image=nginx --port=80 kubectl expose pod nginx --port=80 --type=NodePort To check which NodePort is opened and running the Nginx run: kubectl get svc The output will show: Once the deployment is up, you should be able to access the Nginx home page on the allocated NodePort from either of the worker nodes' Floating IP. To check which worker node is serving nginx , you can check NODE column running the following command: kubectl get pods --all-namespaces --output wide OR, kubectl get pods -A -o wide This will shows like below: Go to browser, visit http://<Worker-Floating-IP>:<NodePort> i.e. http://128.31.25.246:32713 to check the nginx default page. Here Worker_Floating-IP corresponds to the Floating IP of the nginx pod running worker node i.e. worker2 . For your example, Deploy A K8s Dashboard You will going to setup K8dash/Skooner to view a dashboard that shows all your K8s cluster components. SSH into loadbalancer node Switch to root user: sudo su Apply available deployment by running the following command: kubectl apply -f https://raw.githubusercontent.com/skooner-k8s/skooner/master/kubernetes-skooner-nodeport.yaml This will map Skooner port 4654 to a randomly selected port on the running node. The assigned NodePort can be found running: kubectl get svc --namespace=kube-system OR, kubectl get po,svc -n kube-system To check which worker node is serving skooner-* , you can check NODE column running the following command: kubectl get pods --all-namespaces --output wide OR, kubectl get pods -A -o wide This will shows like below: Go to browser, visit http://<Worker-Floating-IP>:<NodePort> i.e. http://128.31.25.246:30495 to check the skooner dashboard page. Here Worker_Floating-IP corresponds to the Floating IP of the skooner-* pod running worker node i.e. worker2 . Setup the Service Account Token to access the Skooner Dashboard: The first (and easiest) option is to create a dedicated service account. Run the following commands: Create the service account in the current namespace (we assume default) kubectl create serviceaccount skooner-sa Give that service account root on the cluster kubectl create clusterrolebinding skooner-sa --clusterrole=cluster-admin --serviceaccount=default:skooner-sa Find the secret that was created to hold the token for the SA kubectl get secrets Show the contents of the secret to extract the token kubectl describe secret skooner-sa-token-xxxxx Copy the token value from the secret and enter it into the login screen to access the dashboard. Watch Demo Video Here\u2019s a recorded demo video on how to setup HA K8s cluster using kubeadm . Clean Up To view the Cluster info: kubectl cluster-info To delete your local references to the cluster: kubectl config delete-cluster How to Remove the node? Talking to the control-plane node with the appropriate credentials, run: kubectl drain <node name> --delete-emptydir-data --force --ignore-daemonsets Before removing the node, reset the state installed by kubeadm: kubeadm reset The reset process does not reset or clean up iptables rules or IPVS tables. If you wish to reset iptables, you must do so manually: iptables -F && iptables -t nat -F && iptables -t mangle -F && iptables -X If you want to reset the IPVS tables, you must run the following command: ipvsadm -C Now remove the node: kubectl delete node <node name> If you wish to start over, run kubeadm init or kubeadm join with the appropriate arguments.","title":"Creating a HA cluster with kubeadm"},{"location":"other-tools/kubernetes/kubeadm/HA-clusters-with-kubeadm/#highly-available-kubernetes-cluster-using-kubeadm","text":"","title":"Highly Available Kubernetes Cluster using kubeadm"},{"location":"other-tools/kubernetes/kubeadm/HA-clusters-with-kubeadm/#objectives","text":"Install a multi control-plane(master) Kubernetes cluster Install a Pod network on the cluster so that your Pods can talk to each other Deploy and test a sample app Deploy K8s Dashboard to view all cluster's components","title":"Objectives"},{"location":"other-tools/kubernetes/kubeadm/HA-clusters-with-kubeadm/#components-and-architecure","text":"This shows components and architecture of a highly-available, production-grade Kubernetes cluster. You can learn about each component from Kubernetes Componets .","title":"Components and architecure"},{"location":"other-tools/kubernetes/kubeadm/HA-clusters-with-kubeadm/#pre-requisite","text":"You will need 2 control-plane(master node) and 2 worker nodes to create a multi-master kubernetes cluster using kubeadm . You are going to use the following set up for this purpose: 2 Linux machines for master, ubuntu-22.04-x86_64 or your choice of Ubuntu OS image, cpu-a.2 flavor with 2vCPU, 4GB RAM, 20GB storage. 2 Linux machines for worker, ubuntu-22.04-x86_64 or your choice of Ubuntu OS image, cpu-a.1 flavor with 1vCPU, 2GB RAM, 20GB storage - also assign Floating IPs to both of the worker nodes. 1 Linux machine for loadbalancer, ubuntu-22.04-x86_64 or your choice of Ubuntu OS image, cpu-a.1 flavor with 1vCPU, 2GB RAM, 20GB storage. ssh access to all machines: Read more here on how to setup SSH to your remote VMs. Create 2 security groups with appropriate ports and protocols : i. To be used by the master nodes: ii. To be used by the worker nodes: - setup Unique hostname to each machine using the following command: echo \"<node_internal_IP> <host_name>\" >> /etc/hosts hostnamectl set-hostname <host_name> For example, echo \"192.168.0.167 loadbalancer\" >> /etc/hosts hostnamectl set-hostname loadbalancer","title":"Pre-requisite"},{"location":"other-tools/kubernetes/kubeadm/HA-clusters-with-kubeadm/#steps","text":"Prepare the Loadbalancer node to communicate with the two master nodes' apiservers on their IPs via port 6443. Do following in all the nodes except the Loadbalancer node: Disable swap. Install kubelet and kubeadm . Install container runtime - you will be using Docker . Initiate kubeadm control plane configuration on one of the master nodes. Save the new master and worker node join commands with the token. Join the second master node to the control plane using the join command. Join the worker nodes to the control plane using the join command. Configure kubeconfig( $HOME/.kube/config ) on loadbalancer node. Install kubectl on Loadbalancer node. Install CNI network plugin i.e. Flannel on Loadbalancer node. Validate all cluster components and nodes are visible on Loadbalancer node. Deploy a sample app and validate the app from Loadbalancer node.","title":"Steps"},{"location":"other-tools/kubernetes/kubeadm/HA-clusters-with-kubeadm/#setting-up-loadbalancer","text":"You will use HAPROXY as the primary loadbalancer, but you can use any other options as well. This node will be not part of the K8s cluster but will be outside of the cluster and interacts with the cluster using ports. You have 2 master nodes. Which means the user can connect to either of the 2 apiservers. The loadbalancer will be used to loadbalance between the 2 apiservers. Login to the loadbalancer node Switch as root - sudo su Update your repository and your system sudo apt-get update && sudo apt-get upgrade -y Install haproxy sudo apt-get install haproxy -y Edit haproxy configuration vi /etc/haproxy/haproxy.cfg Add the below lines to create a frontend configuration for loadbalancer - frontend fe-apiserver bind 0.0.0.0:6443 mode tcp option tcplog default_backend be-apiserver Add the below lines to create a backend configuration for master1 and master2 nodes at port 6443 . Note 6443 is the default port of kube-apiserver backend be-apiserver mode tcp option tcplog option tcp-check balance roundrobin default-server inter 10s downinter 5s rise 2 fall 2 slowstart 60s maxconn 250 maxqueue 256 weight 100 #<!-- markdownlint-disable --> server master1 10.138.0.15:6443 check server master2 10.138.0.16:6443 check Here - master1 and master2 are the hostnames of the master nodes and 10.138.0.15 and 10.138.0.16 are the corresponding internal IP addresses. Restart and Verify haproxy systemctl restart haproxy systemctl status haproxy Ensure haproxy config file is correctly formatted: haproxy -c -q -V -f /etc/haproxy/haproxy.cfg Ensure haproxy is in running status. Run nc command as below: nc -v localhost 6443 Connection to localhost 6443 port [tcp/*] succeeded! Note If you see failures for master1 and master2 connectivity, you can ignore them for time being as you have not yet installed anything on the servers.","title":"Setting up loadbalancer"},{"location":"other-tools/kubernetes/kubeadm/HA-clusters-with-kubeadm/#install-kubeadm-kubelet-and-docker-on-master-and-worker-nodes","text":"kubeadm will not install or manage kubelet or kubectl for you, so you will need to ensure they match the version of the Kubernetes control plane you want kubeadm to install for you. You will install these packages on all of your machines: \u2022 kubeadm : the command to bootstrap the cluster. \u2022 kubelet : the component that runs on all of the machines in your cluster and does things like starting pods and containers. \u2022 kubectl : the command line util to talk to your cluster. In this step, you will install kubelet and kubeadm on the below nodes master1 master2 worker1 worker2 The below steps will be performed on all the above mentioned nodes: SSH into all the 4 machines Switch as root: sudo su Update the repositories and packages: apt-get update && apt-get upgrade -y Turn off swap swapoff -a sed -i '/ swap / s/^/#/' /etc/fstab Install curl and apt-transport-https apt-get update && apt-get install -y apt-transport-https curl Download the Google Cloud public signing key and add key to verify releases curl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | apt-key add - add kubernetes apt repo cat <<EOF | sudo tee /etc/apt/sources.list.d/kubernetes.list deb https://apt.kubernetes.io/ kubernetes-xenial main EOF Install kubelet and kubeadm apt-get update apt-get install -y kubelet kubeadm apt-mark hold is used so that these packages will not be updated/removed automatically apt-mark hold kubelet kubeadm","title":"Install kubeadm, kubelet and docker on master and worker nodes"},{"location":"other-tools/kubernetes/kubeadm/HA-clusters-with-kubeadm/#install-docker-on-master-and-worker-nodes","text":"Install container runtime - docker sudo apt-get install docker.io -y Configure the Docker daemon, in particular to use systemd for the management of the container\u2019s cgroups cat <<EOF | sudo tee /etc/docker/daemon.json { \"exec-opts\": [\"native.cgroupdriver=systemd\"] } EOF systemctl enable --now docker usermod -aG docker ubuntu systemctl daemon-reload systemctl restart docker Ensure net.bridge.bridge-nf-call-iptables is set to 1 in your sysctl config For more Read this . sysctl net.bridge.bridge-nf-call-iptables=1","title":"Install Docker on master and worker nodes"},{"location":"other-tools/kubernetes/kubeadm/HA-clusters-with-kubeadm/#configure-kubeadm-to-bootstrap-the-cluster","text":"You will start off by initializing only one master node. For this purpose, you choose master1 to initialize our first control plane but you can also do the same in master2 . SSH into master1 machine Switch to root user: sudo su Execute the below command to initialize the cluster: kubeadm config images pull kubeadm init --control-plane-endpoint \"LOAD_BALANCER_IP_OR_HOSTNAME:LOAD_BALANCER_PORT\" --upload-certs --pod-network-cidr=10.244.0.0/16 Here, you can use either the IP address or the hostname of the loadbalancer in place of . You have not enabled the hostname of the server, i.e. loadbalancer as the LOAD_BALANCER_IP_OR_HOSTNAME that is visible from the master1 node. so instead of using not resolvable hostnames across your network, you will be using the IP address of the Loadbalancer server. The is the front end configuration port defined in HAPROXY configuration. For this, you have kept the port as 6443 which is the default apiserver port. Important Note --pod-network-cidr value depends upon what CNI plugin you going to use so need to be very careful while setting this CIDR values. In our case, you are going to use Flannel CNI network plugin so you will use: --pod-network-cidr=10.244.0.0/16 . If you are opted to use Calico CNI network plugin then you need to use: --pod-network-cidr=192.168.0.0/16 and if you are opted to use Weave Net no need to pass this parameter. For example, our Flannel CNI network plugin based kubeadm init command with loadbalancer node with internal IP: 192.168.0.167 look like below: kubeadm config images pull kubeadm init --control-plane-endpoint \"192.168.0.167:6443\" --upload-certs --pod-network-cidr=10.244.0.0/16 Save the output in some secure file for future use. This will show an unique token to join the control plane. The output from kubeadm init should looks like below: Your Kubernetes control-plane has initialized successfully! To start using your cluster, you need to run the following as a regular user: mkdir -p $HOME/.kube sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config sudo chown $(id -u):$(id -g) $HOME/.kube/config Alternatively, if you are the root user, you can run: export KUBECONFIG=/etc/kubernetes/admin.conf You should now deploy a pod network to the cluster. Run \"kubectl apply -f [podnetwork].yaml\" with one of the options listed at: https://kubernetes.io/docs/concepts/cluster-administration/addons/ You can now join any number of the control-plane node running the following command on each as root: kubeadm join 192.168.0.167:6443 --token cnslau.kd5fjt96jeuzymzb \\ --discovery-token-ca-cert-hash sha256:871ab3f050bc9790c977daee9e44cf52e15ee3 7ab9834567333b939458a5bfb5 \\ --control-plane --certificate-key 824d9a0e173a810416b4bca7038fb33b616108c17abcbc5eaef8651f11e3d146 Please note that the certificate-key gives access to cluster sensitive data, keep it secret! As a safeguard, uploaded-certs will be deleted in two hours; If necessary, you can use \"kubeadm init phase upload-certs --upload-certs\" to reload certs afterward. Then you can join any number of worker nodes by running the following on each as root: kubeadm join 192.168.0.167:6443 --token cnslau.kd5fjt96jeuzymzb \\ --discovery-token-ca-cert-hash sha256:871ab3f050bc9790c977daee9e44cf52e15ee37ab9834567333b939458a5bfb5 The output consists of 3 major tasks: A. Setup kubeconfig using on current master node: As you are running as root user so you need to run the following command: export KUBECONFIG=/etc/kubernetes/admin.conf We need to run the below commands as a normal user to use the kubectl from terminal. mkdir -p $HOME/.kube sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config sudo chown $(id -u):$(id -g) $HOME/.kube/config Now the machine is initialized as master. Warning Kubeadm signs the certificate in the admin.conf to have Subject: O = system:masters, CN = kubernetes-admin. system:masters is a break-glass, super user group that bypasses the authorization layer (e.g. RBAC). Do not share the admin.conf file with anyone and instead grant users custom permissions by generating them a kubeconfig file using the kubeadm kubeconfig user command. B. Setup a new control plane (master) i.e. master2 by running following command on master2 node: kubeadm join 192.168.0.167:6443 --token cnslau.kd5fjt96jeuzymzb \\ --discovery-token-ca-cert-hash sha256:871ab3f050bc9790c977daee9e44cf52e1 5ee37ab9834567333b939458a5bfb5 \\ --control-plane --certificate-key 824d9a0e173a810416b4bca7038fb33b616108c17abcbc5eaef8651f11e3d146 C. Join worker nodes running following command on individual workder nodes: kubeadm join 192.168.0.167:6443 --token cnslau.kd5fjt96jeuzymzb \\ --discovery-token-ca-cert-hash sha256:871ab3f050bc9790c977daee9e44cf52e15ee37ab9834567333b939458a5bfb5 Important Note Your output will be different than what is provided here. While performing the rest of the demo, ensure that you are executing the command provided by your output and dont copy and paste from here. If you do not have the token, you can get it by running the following command on the control-plane node: kubeadm token list The output is similar to this: TOKEN TTL EXPIRES USAGES DESCRIPTION EXTRA GROUPS 8ewj1p... 23h 2018-06-12 authentication, The default bootstrap system: signing token generated by bootstrappers: 'kubeadm init'. kubeadm: default-node-token If you missed the join command, execute the following command kubeadm token create --print-join-command in the master node to recreate the token with the join command. root@master:~$ kubeadm token create --print-join-command kubeadm join 10.2.0.4:6443 --token xyzeyi.wxer3eg9vj8hcpp2 \\ --discovery-token-ca-cert-hash sha256:ccfc92b2a31b002c3151cdbab77ff4dc32ef13b213fa3a9876e126831c76f7fa By default, tokens expire after 24 hours. If you are joining a node to the cluster after the current token has expired, you can create a new token by running the following command on the control-plane node: kubeadm token create The output is similar to this: 5didvk.d09sbcov8ph2amjw We can use this new token to join: $ kubeadm join <master-ip>:<master-port> --token <token> \\ --discovery-token-ca-cert-hash sha256:<hash> SSH into master2 Switch to root user: sudo su Check the command provided by the output of master1 : You can now use the below command to add another control-plane node(master) to the control plane: kubeadm join 192.168.0.167:6443 --token cnslau.kd5fjt96jeuzymzb --discovery-token-ca-cert-hash sha256:871ab3f050bc9790c977daee9e44cf52e15ee3 7ab9834567333b939458a5bfb5 \\ --control-plane --certificate-key 824d9a0e173a810416b4bca7038fb33b616108c17abcbc5eaef8651f11e3d146 Execute the kubeadm join command for control plane on master2 Your output should look like: This node has joined the cluster and a new control plane instance was created: * Certificate signing request was sent to apiserver and approval was received. * The Kubelet was informed of the new secure connection details. * Control plane (master) label and taint were applied to the new node. * The Kubernetes control plane instances scaled up. * A new etcd member was added to the local/stacked etcd cluster. Now that you have initialized both the masters - you can now work on bootstrapping the worker nodes. SSH into worker1 and worker2 Switch to root user on both the machines: sudo su Check the output given by the init command on master1 to join worker node: kubeadm join 192.168.0.167:6443 --token cnslau.kd5fjt96jeuzymzb \\ --discovery-token-ca-cert-hash sha256:871ab3f050bc9790c977daee9e44cf52e15ee37ab9834567333b939458a5bfb5 Execute the above command on both the nodes: Your output should look like: This node has joined the cluster: * Certificate signing request was sent to apiserver and a response was received. * The Kubelet was informed of the new secure connection details.","title":"Configure kubeadm to bootstrap the cluster"},{"location":"other-tools/kubernetes/kubeadm/HA-clusters-with-kubeadm/#configure-kubeconfig-on-loadbalancer-node","text":"Now that you have configured the master and the worker nodes, its now time to configure Kubeconfig ( .kube ) on the loadbalancer node. It is completely up to you if you want to use the loadbalancer node to setup kubeconfig. kubeconfig can also be setup externally on a separate machine which has access to loadbalancer node. For the purpose of this demo you will use loadbalancer node to host kubeconfig and kubectl . SSH into loadbalancer node Switch to root user: sudo su Create a directory: .kube at $HOME of root user mkdir -p $HOME/.kube SCP configuration file from any one master node to loadbalancer node scp master1:/etc/kubernetes/admin.conf $HOME/.kube/config Important Note If you havent setup ssh connection between master node and loadbalancer, you can manually copy content of the file /etc/kubernetes/admin.conf from master1 node and then paste it to $HOME/.kube/config file on the loadbalancer node. Ensure that the kubeconfig file path is $HOME/.kube/config on the loadbalancer node. Provide appropriate ownership to the copied file chown $(id -u):$(id -g) $HOME/.kube/config","title":"Configure kubeconfig on loadbalancer node"},{"location":"other-tools/kubernetes/kubeadm/HA-clusters-with-kubeadm/#install-kubectl","text":"Install kubectl binary \u2022 kubectl : the command line util to talk to your cluster. snap install kubectl --classic This outputs: kubectl 1.22.2 from Canonical\u2713 installed Verify the cluster kubectl get nodes NAME STATUS ROLES AGE VERSION master1 NotReady control-plane,master 21m v1.16.2 master2 NotReady control-plane,master 15m v1.16.2 worker1 Ready <none> 9m17s v1.16.2 worker2 Ready <none> 9m25s v1.16.2","title":"Install kubectl"},{"location":"other-tools/kubernetes/kubeadm/HA-clusters-with-kubeadm/#install-cni-network-plugin","text":"","title":"Install CNI network plugin"},{"location":"other-tools/kubernetes/kubeadm/HA-clusters-with-kubeadm/#cni-overview","text":"Managing a network where containers can interoperate efficiently is very important. Kubernetes has adopted the Container Network Interface(CNI) specification for managing network resources on a cluster. This relatively simple specification makes it easy for Kubernetes to interact with a wide range of CNI-based software solutions. Using this CNI plugin allows Kubernetes pods to have the same IP address inside the pod as they do on the VPC network. Make sure the configuration corresponds to the Pod CIDR specified in the kubeadm configuration file if applicable. You must deploy a CNI based Pod network add-on so that your Pods can communicate with each other. Cluster DNS (CoreDNS) will not start up before a network is installed. To verify you can run this command: kubectl get po -n kube-system : You should see the following output. You will see the two coredns-* pods in a pending state. It is the expected behavior. Once we install the network plugin, it will be in a Running state. Output Example: root@loadbalancer:~$ kubectl get po -n kube-system NAME READY STATUS RESTARTS AGE coredns-558bd4d5db-5jktc 0/1 Pending 0 10m coredns-558bd4d5db-xdc5x 0/1 Pending 0 10m etcd-master1 1/1 Running 0 11m kube-apiserver-master1 1/1 Running 0 11m kube-controller-manager-master1 1/1 Running 0 11m kube-proxy-5jfh5 1/1 Running 0 10m kube-scheduler-master1 1/1 Running 0 11m","title":"CNI overview"},{"location":"other-tools/kubernetes/kubeadm/HA-clusters-with-kubeadm/#supported-cni-options","text":"To read more about the currently supported base CNI solutions for Kubernetes read here and also read this . The below command can be run on the Loadbalancer node to install the CNI plugin: kubectl apply -f https://github.com/coreos/flannel/raw/master/Documentation/kube-flannel.yml As you had passed --pod-network-cidr=10.244.0.0/16 with kubeadm init so this should work for Flannel CNI. Using Other CNI Options For Calico CNI plugin to work correctly, you need to pass --pod-network-cidr=192.168.0.0/16 with kubeadm init and then you can run: kubectl apply -f https://docs.projectcalico.org/v3.8/manifests/calico.yaml For Weave Net CNI plugin to work correctly, you don't need to pass --pod-network-cidr with kubeadm init and then you can run: kubectl apply -f \"https://cloud.weave.works/k8s/net?k8s-version=$(kubectl version | base64 | tr -d '\\n')\" Dual Network: It is highly recommended to follow an internal/external network layout for your cluster, as showed in this diagram: To enable this just give two different names to the internal and external interface, according to your distro of choiche naming scheme: external_interface: eth0 internal_interface: eth1 Also you can decide here what CIDR should your cluster use cluster_cidr: 10.43.0.0/16 service_cidr: 10.44.0.0/16 Once you successfully installed the Flannel CNI component to your cluster. You can now verify your HA cluster running: kubectl get nodes NAME STATUS ROLES AGE VERSION master1 Ready control-plane,master 22m v1.16.2 master2 Ready control-plane,master 17m v1.16.2 worker1 Ready <none> 10m v1.16.2 worker2 Ready <none> 10m v1.16.2","title":"Supported CNI options"},{"location":"other-tools/kubernetes/kubeadm/HA-clusters-with-kubeadm/#deploy-a-sample-nginx-application","text":"Now that we have all the components to make the cluster and applications work, let\u2019s deploy a sample Nginx application and see if we can access it over a NodePort that has port range of 30000-32767 . The below command can be run on: kubectl run nginx --image=nginx --port=80 kubectl expose pod nginx --port=80 --type=NodePort To check which NodePort is opened and running the Nginx run: kubectl get svc The output will show: Once the deployment is up, you should be able to access the Nginx home page on the allocated NodePort from either of the worker nodes' Floating IP. To check which worker node is serving nginx , you can check NODE column running the following command: kubectl get pods --all-namespaces --output wide OR, kubectl get pods -A -o wide This will shows like below: Go to browser, visit http://<Worker-Floating-IP>:<NodePort> i.e. http://128.31.25.246:32713 to check the nginx default page. Here Worker_Floating-IP corresponds to the Floating IP of the nginx pod running worker node i.e. worker2 . For your example,","title":"Deploy A Sample Nginx Application"},{"location":"other-tools/kubernetes/kubeadm/HA-clusters-with-kubeadm/#deploy-a-k8s-dashboard","text":"You will going to setup K8dash/Skooner to view a dashboard that shows all your K8s cluster components. SSH into loadbalancer node Switch to root user: sudo su Apply available deployment by running the following command: kubectl apply -f https://raw.githubusercontent.com/skooner-k8s/skooner/master/kubernetes-skooner-nodeport.yaml This will map Skooner port 4654 to a randomly selected port on the running node. The assigned NodePort can be found running: kubectl get svc --namespace=kube-system OR, kubectl get po,svc -n kube-system To check which worker node is serving skooner-* , you can check NODE column running the following command: kubectl get pods --all-namespaces --output wide OR, kubectl get pods -A -o wide This will shows like below: Go to browser, visit http://<Worker-Floating-IP>:<NodePort> i.e. http://128.31.25.246:30495 to check the skooner dashboard page. Here Worker_Floating-IP corresponds to the Floating IP of the skooner-* pod running worker node i.e. worker2 . Setup the Service Account Token to access the Skooner Dashboard: The first (and easiest) option is to create a dedicated service account. Run the following commands: Create the service account in the current namespace (we assume default) kubectl create serviceaccount skooner-sa Give that service account root on the cluster kubectl create clusterrolebinding skooner-sa --clusterrole=cluster-admin --serviceaccount=default:skooner-sa Find the secret that was created to hold the token for the SA kubectl get secrets Show the contents of the secret to extract the token kubectl describe secret skooner-sa-token-xxxxx Copy the token value from the secret and enter it into the login screen to access the dashboard.","title":"Deploy A K8s Dashboard"},{"location":"other-tools/kubernetes/kubeadm/HA-clusters-with-kubeadm/#watch-demo-video","text":"Here\u2019s a recorded demo video on how to setup HA K8s cluster using kubeadm .","title":"Watch Demo Video"},{"location":"other-tools/kubernetes/kubeadm/HA-clusters-with-kubeadm/#clean-up","text":"To view the Cluster info: kubectl cluster-info To delete your local references to the cluster: kubectl config delete-cluster","title":"Clean Up"},{"location":"other-tools/kubernetes/kubeadm/HA-clusters-with-kubeadm/#how-to-remove-the-node","text":"Talking to the control-plane node with the appropriate credentials, run: kubectl drain <node name> --delete-emptydir-data --force --ignore-daemonsets Before removing the node, reset the state installed by kubeadm: kubeadm reset The reset process does not reset or clean up iptables rules or IPVS tables. If you wish to reset iptables, you must do so manually: iptables -F && iptables -t nat -F && iptables -t mangle -F && iptables -X If you want to reset the IPVS tables, you must run the following command: ipvsadm -C Now remove the node: kubectl delete node <node name> If you wish to start over, run kubeadm init or kubeadm join with the appropriate arguments.","title":"How to Remove the node?"},{"location":"other-tools/kubernetes/kubeadm/single-master-clusters-with-kubeadm/","text":"Creating a Single Master cluster with kubeadm Objectives Install a single control-plane(master) Kubernetes cluster Install a Pod network on the cluster so that your Pods can talk to each other Deploy and test a sample app Deploy K8s Dashboard to view all cluster's components Components and architecure You can learn about each component from Kubernetes Componets . Pre-requisite We will need 1 control-plane(master) and 2 worker node to create a single control-plane kubernetes cluster using kubeadm . We are using following setting for this purpose: 1 Linux machine for master, ubuntu-20.04-x86_64, cpu-a.2 flavor with 2vCPU, 4GB RAM, 20GB storage. 2 Linux machines for worker, ubuntu-20.04-x86_64, cpu-a.1 flavor with 1vCPU, 2GB RAM, 20GB storage - also assign Floating IPs to both of the worker nodes. ssh access to all machines: Read more here on how to setup SSH to your remote VMs. Create 2 security groups with appropriate ports and protocols : i. To be used by the master nodes: ii. To be used by the worker nodes: - setup Unique hostname to each machine using the following command: echo \"<node_internal_IP> <host_name>\" >> /etc/hosts hostnamectl set-hostname <host_name> For example, echo \"192.168.0.167 master\" >> /etc/hosts hostnamectl set-hostname master Steps Disable swap on all nodes. Install kubeadm , kubelet , and kubectl on all the nodes. Install container runtime on all nodes- you will be using Docker . Initiate kubeadm control plane configuration on the master node. Save the worker node join command with the token. Install CNI network plugin i.e. Flannel on master node. Join the worker node to the master node (control plane) using the join command. Validate all cluster components and nodes are visible on master node. Deploy a sample app and validate the app from master node. Install kubeadm, kubelet and docker on master and worker nodes kubeadm will not install or manage kubelet or kubectl for you, so you will need to ensure they match the version of the Kubernetes control plane you want kubeadm to install for you. You will install these packages on all of your machines: \u2022 kubeadm : the command to bootstrap the cluster. \u2022 kubelet : the component that runs on all of the machines in your cluster and does things like starting pods and containers. \u2022 kubectl : the command line util to talk to your cluster. In this step, you will install kubelet and kubeadm on the below nodes master worker1 worker2 The below steps will be performed on all the above mentioned nodes: SSH into all the 3 machines Switch as root: sudo su Update the repositories and packages: apt-get update && apt-get upgrade -y Turn off swap swapoff -a sed -i '/ swap / s/^/#/' /etc/fstab Install curl and apt-transport-https apt-get update && apt-get install -y apt-transport-https curl Download the Google Cloud public signing key and add key to verify releases curl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | apt-key add - add kubernetes apt repo cat <<EOF | sudo tee /etc/apt/sources.list.d/kubernetes.list deb https://apt.kubernetes.io/ kubernetes-xenial main EOF Install kubelet, kubeadm, and kubectl apt-get update apt-get install -y kubelet kubeadm kubectl apt-mark hold is used so that these packages will not be updated/removed automatically apt-mark hold kubelet kubeadm kubectl Install Docker Install container runtime - docker sudo apt-get install docker.io -y Configure the Docker daemon, in particular to use systemd for the management of the container\u2019s cgroups cat <<EOF | sudo tee /etc/docker/daemon.json { \"exec-opts\": [\"native.cgroupdriver=systemd\"] } EOF systemctl enable --now docker usermod -aG docker ubuntu systemctl daemon-reload systemctl restart docker Ensure net.bridge.bridge-nf-call-iptables is set to 1 in your sysctl config For more Read this . sysctl net.bridge.bridge-nf-call-iptables=1 Configure kubeadm to bootstrap the cluster on master node Run the below command on the master node i.e. master that you want to setup as control plane. SSH into master machine Switch to root user: sudo su Execute the below command to initialize the cluster: export MASTER_IP=<Master-Internal-IP> kubeadm config images pull kubeadm init --apiserver-advertise-address=${MASTER_IP} --pod-network-cidr=10.244.0.0/16 Important Note Please make sure you replace the correct IP of the node with <Master-Internal-IP> which is the Internal IP of master node. --pod-network-cidr value depends upon what CNI plugin you going to use so need to be very careful while setting this CIDR values. In our case, you are going to use Flannel CNI network plugin so you will use: --pod-network-cidr=10.244.0.0/16 . If you are opted to use Calico CNI network plugin then you need to use: --pod-network-cidr=192.168.0.0/16 and if you are opted to use Weave Net no need to pass this parameter. For example, our Flannel CNI network plugin based kubeadm init command with master node with internal IP: 192.168.0.167 look like below: For example, export MASTER_IP=192.168.0.167 kubeadm config images pull kubeadm init --apiserver-advertise-address=${MASTER_IP} --pod-network-cidr=10.244.0.0/16 Save the output in some secure file for future use. This will show an unique token to join the control plane. The output from kubeadm init should looks like below: Your Kubernetes control-plane has initialized successfully! To start using your cluster, you need to run the following as a regular user: mkdir -p $HOME/.kube sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config sudo chown $(id -u):$(id -g) $HOME/.kube/config Alternatively, if you are the root user, you can run: export KUBECONFIG=/etc/kubernetes/admin.conf You should now deploy a pod network to the cluster. Run \"kubectl apply -f [podnetwork].yaml\" with one of the options listed at: https://kubernetes.io/docs/concepts/cluster-administration/addons/ Please note that the certificate-key gives access to cluster sensitive data, keep it secret! As a safeguard, uploaded-certs will be deleted in two hours; If necessary, you can use \"kubeadm init phase upload-certs --upload-certs\" to reload certs afterward. Then you can join any number of worker nodes by running the following on each as root: kubeadm join 192.168.0.167:6443 --token cnslau.kd5fjt96jeuzymzb \\ --discovery-token-ca-cert-hash sha256:871ab3f050bc9790c977daee9e44cf52e15ee37ab9834567333b939458a5bfb5 The output consists of 2 major tasks: A. Setup kubeconfig using on current master node: As you are running as root user so you need to run the following command: export KUBECONFIG=/etc/kubernetes/admin.conf We need to run the below commands as a normal user to use the kubectl from terminal. mkdir -p $HOME/.kube sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config sudo chown $(id -u):$(id -g) $HOME/.kube/config Now the machine is initialized as master. Warning Kubeadm signs the certificate in the admin.conf to have Subject: O = system:masters, CN = kubernetes-admin. system:masters is a break-glass, super user group that bypasses the authorization layer (e.g. RBAC). Do not share the admin.conf file with anyone and instead grant users custom permissions by generating them a kubeconfig file using the kubeadm kubeconfig user command. B. Join worker nodes running following command on individual workder nodes: kubeadm join 192.168.0.167:6443 --token cnslau.kd5fjt96jeuzymzb \\ --discovery-token-ca-cert-hash sha256:871ab3f050bc9790c977daee9e44cf52e15ee37ab9834567333b939458a5bfb5 Important Note Your output will be different than what is provided here. While performing the rest of the demo, ensure that you are executing the command provided by your output and dont copy and paste from here. If you do not have the token, you can get it by running the following command on the control-plane node: kubeadm token list The output is similar to this: TOKEN TTL EXPIRES USAGES DESCRIPTION EXTRA GROUPS 8ewj1p... 23h 2018-06-12 authentication, The default bootstrap system: signing token generated by bootstrappers: 'kubeadm init'. kubeadm: default-node-token If you missed the join command, execute the following command kubeadm token create --print-join-command in the master node to recreate the token with the join command. root@master:~$ kubeadm token create --print-join-command kubeadm join 10.2.0.4:6443 --token xyzeyi.wxer3eg9vj8hcpp2 \\ --discovery-token-ca-cert-hash sha256:ccfc92b2a31b002c3151cdbab77ff4dc32ef13b213fa3a9876e126831c76f7fa By default, tokens expire after 24 hours. If you are joining a node to the cluster after the current token has expired, you can create a new token by running the following command on the control-plane node: kubeadm token create The output is similar to this: 5didvk.d09sbcov8ph2amjw We can use this new token to join: $ kubeadm join <master-ip>:<master-port> --token <token> \\ --discovery-token-ca-cert-hash sha256:<hash> Now that you have initialized the master - you can now work on bootstrapping the worker nodes. SSH into worker1 and worker2 Switch to root user on both the machines: sudo su Check the output given by the init command on master to join worker node: kubeadm join 192.168.0.167:6443 --token cnslau.kd5fjt96jeuzymzb \\ --discovery-token-ca-cert-hash sha256:871ab3f050bc9790c977daee9e44cf52e15ee37ab9834567333b939458a5bfb5 Execute the above command on both the nodes: Your output should look like: This node has joined the cluster: * Certificate signing request was sent to apiserver and a response was received. * The Kubelet was informed of the new secure connection details. Validate all cluster components and nodes are visible on all nodes Verify the cluster kubectl get nodes NAME STATUS ROLES AGE VERSION master NotReady control-plane,master 21m v1.16.2 worker1 Ready <none> 9m17s v1.16.2 worker2 Ready <none> 9m25s v1.16.2 Install CNI network plugin CNI overview Managing a network where containers can interoperate efficiently is very important. Kubernetes has adopted the Container Network Interface(CNI) specification for managing network resources on a cluster. This relatively simple specification makes it easy for Kubernetes to interact with a wide range of CNI-based software solutions. Using this CNI plugin allows Kubernetes pods to have the same IP address inside the pod as they do on the VPC network. Make sure the configuration corresponds to the Pod CIDR specified in the kubeadm configuration file if applicable. You must deploy a CNI based Pod network add-on so that your Pods can communicate with each other. Cluster DNS (CoreDNS) will not start up before a network is installed. To verify you can run this command: kubectl get po -n kube-system : You should see the following output. You will see the two coredns-* pods in a pending state. It is the expected behavior. Once we install the network plugin, it will be in a Running state. Output Example: root@master:~$ kubectl get po -n kube-system NAME READY STATUS RESTARTS AGE coredns-558bd4d5db-5jktc 0/1 Pending 0 10m coredns-558bd4d5db-xdc5x 0/1 Pending 0 10m etcd-master1 1/1 Running 0 11m kube-apiserver-master1 1/1 Running 0 11m kube-controller-manager-master1 1/1 Running 0 11m kube-proxy-5jfh5 1/1 Running 0 10m kube-scheduler-master1 1/1 Running 0 11m Supported CNI options To read more about the currently supported base CNI solutions for Kubernetes read here and also read this . The below command can be run on the master node to install the CNI plugin: kubectl apply -f https://github.com/coreos/flannel/raw/master/Documentation/kube-flannel.yml As you had passed --pod-network-cidr=10.244.0.0/16 with kubeadm init so this should work for Flannel CNI. Using Other CNI Options For Calico CNI plugin to work correctly, you need to pass --pod-network-cidr=192.168.0.0/16 with kubeadm init and then you can run: kubectl apply -f https://docs.projectcalico.org/v3.8/manifests/calico.yaml For Weave Net CNI plugin to work correctly, you don't need to pass --pod-network-cidr with kubeadm init and then you can run: kubectl apply -f \"https://cloud.weave.works/k8s/net?k8s-version=$(kubectl version | base64 | tr -d '\\n')\" Dual Network: It is highly recommended to follow an internal/external network layout for your cluster, as showed in this diagram: To enable this just give two different names to the internal and external interface, according to your distro of choiche naming scheme: external_interface: eth0 internal_interface: eth1 Also you can decide here what CIDR should your cluster use cluster_cidr: 10.43.0.0/16 service_cidr: 10.44.0.0/16 Once you successfully installed the Flannel CNI component to your cluster. You can now verify your HA cluster running: kubectl get nodes NAME STATUS ROLES AGE VERSION master Ready control-plane,master 22m v1.16.2 worker1 Ready <none> 10m v1.16.2 worker2 Ready <none> 10m v1.16.2 Watch Recoded Video Here\u2019s a quick recorded demo video upto this point where we successfully setup single master K8s cluster using Kubeadm. Deploy A Sample Nginx Application Now that we have all the components to make the cluster and applications work, let\u2019s deploy a sample Nginx application and see if we can access it over a NodePort that has port range of 30000-32767 . The below command can be run on: kubectl run nginx --image=nginx --port=80 kubectl expose pod nginx --port=80 --type=NodePort To check which NodePort is opened and running the Nginx run: kubectl get svc The output will show: Once the deployment is up, you should be able to access the Nginx home page on the allocated NodePort from either of the worker nodes' Floating IP. To check which worker node is serving nginx , you can check NODE column running the following command: kubectl get pods --all-namespaces --output wide OR, kubectl get pods -A -o wide This will shows like below: Go to browser, visit http://<Worker-Floating-IP>:<NodePort> i.e. http://128.31.25.246:32713 to check the nginx default page. Here Worker_Floating-IP corresponds to the Floating IP of the nginx pod running worker node i.e. worker2 . For your example, Deploy A K8s Dashboard You will going to setup K8dash/Skooner to view a dashboard that shows all your K8s cluster components. SSH into master node Switch to root user: sudo su Apply available deployment by running the following command: kubectl apply -f https://raw.githubusercontent.com/skooner-k8s/skooner/master/kubernetes-skooner-nodeport.yaml This will map Skooner port 4654 to a randomly selected port from the master node. The assigned NodePort on the master node can be found running: kubectl get svc --namespace=kube-system OR, kubectl get po,svc -n kube-system To check which worker node is serving skooner-* , you can check NODE column running the following command: kubectl get pods --all-namespaces --output wide OR, kubectl get pods -A -o wide This will shows like below: Go to browser, visit http://<Worker-Floating-IP>:<NodePort> i.e. http://128.31.25.246:30495 to check the skooner dashboard page. Here Worker_Floating-IP corresponds to the Floating IP of the skooner-* pod running worker node i.e. worker2 . Setup the Service Account Token to access the Skooner Dashboard: The first (and easiest) option is to create a dedicated service account. Run the following commands: Create the service account in the current namespace (we assume default) kubectl create serviceaccount skooner-sa Give that service account root on the cluster kubectl create clusterrolebinding skooner-sa --clusterrole=cluster-admin --serviceaccount=default:skooner-sa Find the secret that was created to hold the token for the SA kubectl get secrets Show the contents of the secret to extract the token kubectl describe secret skooner-sa-token-xxxxx Copy the token value from the secret and enter it into the login screen to access the dashboard. Watch Demo Video Here\u2019s a recorded demo video on how to deploy applications on top of setup single master K8s cluster as explained above. Clean Up To view the Cluster info: kubectl cluster-info To delete your local references to the cluster: kubectl config delete-cluster How to Remove the node? Talking to the control-plane node with the appropriate credentials, run: kubectl drain <node name> --delete-emptydir-data --force --ignore-daemonsets Before removing the node, reset the state installed by kubeadm: kubeadm reset The reset process does not reset or clean up iptables rules or IPVS tables. If you wish to reset iptables, you must do so manually: iptables -F && iptables -t nat -F && iptables -t mangle -F && iptables -X If you want to reset the IPVS tables, you must run the following command: ipvsadm -C Now remove the node: kubectl delete node <node name> If you wish to start over, run kubeadm init or kubeadm join with the appropriate arguments.","title":"Bootstrapping cluster with kubeadm"},{"location":"other-tools/kubernetes/kubeadm/single-master-clusters-with-kubeadm/#creating-a-single-master-cluster-with-kubeadm","text":"","title":"Creating a Single Master cluster with kubeadm"},{"location":"other-tools/kubernetes/kubeadm/single-master-clusters-with-kubeadm/#objectives","text":"Install a single control-plane(master) Kubernetes cluster Install a Pod network on the cluster so that your Pods can talk to each other Deploy and test a sample app Deploy K8s Dashboard to view all cluster's components","title":"Objectives"},{"location":"other-tools/kubernetes/kubeadm/single-master-clusters-with-kubeadm/#components-and-architecure","text":"You can learn about each component from Kubernetes Componets .","title":"Components and architecure"},{"location":"other-tools/kubernetes/kubeadm/single-master-clusters-with-kubeadm/#pre-requisite","text":"We will need 1 control-plane(master) and 2 worker node to create a single control-plane kubernetes cluster using kubeadm . We are using following setting for this purpose: 1 Linux machine for master, ubuntu-20.04-x86_64, cpu-a.2 flavor with 2vCPU, 4GB RAM, 20GB storage. 2 Linux machines for worker, ubuntu-20.04-x86_64, cpu-a.1 flavor with 1vCPU, 2GB RAM, 20GB storage - also assign Floating IPs to both of the worker nodes. ssh access to all machines: Read more here on how to setup SSH to your remote VMs. Create 2 security groups with appropriate ports and protocols : i. To be used by the master nodes: ii. To be used by the worker nodes: - setup Unique hostname to each machine using the following command: echo \"<node_internal_IP> <host_name>\" >> /etc/hosts hostnamectl set-hostname <host_name> For example, echo \"192.168.0.167 master\" >> /etc/hosts hostnamectl set-hostname master","title":"Pre-requisite"},{"location":"other-tools/kubernetes/kubeadm/single-master-clusters-with-kubeadm/#steps","text":"Disable swap on all nodes. Install kubeadm , kubelet , and kubectl on all the nodes. Install container runtime on all nodes- you will be using Docker . Initiate kubeadm control plane configuration on the master node. Save the worker node join command with the token. Install CNI network plugin i.e. Flannel on master node. Join the worker node to the master node (control plane) using the join command. Validate all cluster components and nodes are visible on master node. Deploy a sample app and validate the app from master node.","title":"Steps"},{"location":"other-tools/kubernetes/kubeadm/single-master-clusters-with-kubeadm/#install-kubeadm-kubelet-and-docker-on-master-and-worker-nodes","text":"kubeadm will not install or manage kubelet or kubectl for you, so you will need to ensure they match the version of the Kubernetes control plane you want kubeadm to install for you. You will install these packages on all of your machines: \u2022 kubeadm : the command to bootstrap the cluster. \u2022 kubelet : the component that runs on all of the machines in your cluster and does things like starting pods and containers. \u2022 kubectl : the command line util to talk to your cluster. In this step, you will install kubelet and kubeadm on the below nodes master worker1 worker2 The below steps will be performed on all the above mentioned nodes: SSH into all the 3 machines Switch as root: sudo su Update the repositories and packages: apt-get update && apt-get upgrade -y Turn off swap swapoff -a sed -i '/ swap / s/^/#/' /etc/fstab Install curl and apt-transport-https apt-get update && apt-get install -y apt-transport-https curl Download the Google Cloud public signing key and add key to verify releases curl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | apt-key add - add kubernetes apt repo cat <<EOF | sudo tee /etc/apt/sources.list.d/kubernetes.list deb https://apt.kubernetes.io/ kubernetes-xenial main EOF Install kubelet, kubeadm, and kubectl apt-get update apt-get install -y kubelet kubeadm kubectl apt-mark hold is used so that these packages will not be updated/removed automatically apt-mark hold kubelet kubeadm kubectl","title":"Install kubeadm, kubelet and docker on master and worker nodes"},{"location":"other-tools/kubernetes/kubeadm/single-master-clusters-with-kubeadm/#install-docker","text":"Install container runtime - docker sudo apt-get install docker.io -y Configure the Docker daemon, in particular to use systemd for the management of the container\u2019s cgroups cat <<EOF | sudo tee /etc/docker/daemon.json { \"exec-opts\": [\"native.cgroupdriver=systemd\"] } EOF systemctl enable --now docker usermod -aG docker ubuntu systemctl daemon-reload systemctl restart docker Ensure net.bridge.bridge-nf-call-iptables is set to 1 in your sysctl config For more Read this . sysctl net.bridge.bridge-nf-call-iptables=1","title":"Install Docker"},{"location":"other-tools/kubernetes/kubeadm/single-master-clusters-with-kubeadm/#configure-kubeadm-to-bootstrap-the-cluster-on-master-node","text":"Run the below command on the master node i.e. master that you want to setup as control plane. SSH into master machine Switch to root user: sudo su Execute the below command to initialize the cluster: export MASTER_IP=<Master-Internal-IP> kubeadm config images pull kubeadm init --apiserver-advertise-address=${MASTER_IP} --pod-network-cidr=10.244.0.0/16 Important Note Please make sure you replace the correct IP of the node with <Master-Internal-IP> which is the Internal IP of master node. --pod-network-cidr value depends upon what CNI plugin you going to use so need to be very careful while setting this CIDR values. In our case, you are going to use Flannel CNI network plugin so you will use: --pod-network-cidr=10.244.0.0/16 . If you are opted to use Calico CNI network plugin then you need to use: --pod-network-cidr=192.168.0.0/16 and if you are opted to use Weave Net no need to pass this parameter. For example, our Flannel CNI network plugin based kubeadm init command with master node with internal IP: 192.168.0.167 look like below: For example, export MASTER_IP=192.168.0.167 kubeadm config images pull kubeadm init --apiserver-advertise-address=${MASTER_IP} --pod-network-cidr=10.244.0.0/16 Save the output in some secure file for future use. This will show an unique token to join the control plane. The output from kubeadm init should looks like below: Your Kubernetes control-plane has initialized successfully! To start using your cluster, you need to run the following as a regular user: mkdir -p $HOME/.kube sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config sudo chown $(id -u):$(id -g) $HOME/.kube/config Alternatively, if you are the root user, you can run: export KUBECONFIG=/etc/kubernetes/admin.conf You should now deploy a pod network to the cluster. Run \"kubectl apply -f [podnetwork].yaml\" with one of the options listed at: https://kubernetes.io/docs/concepts/cluster-administration/addons/ Please note that the certificate-key gives access to cluster sensitive data, keep it secret! As a safeguard, uploaded-certs will be deleted in two hours; If necessary, you can use \"kubeadm init phase upload-certs --upload-certs\" to reload certs afterward. Then you can join any number of worker nodes by running the following on each as root: kubeadm join 192.168.0.167:6443 --token cnslau.kd5fjt96jeuzymzb \\ --discovery-token-ca-cert-hash sha256:871ab3f050bc9790c977daee9e44cf52e15ee37ab9834567333b939458a5bfb5 The output consists of 2 major tasks: A. Setup kubeconfig using on current master node: As you are running as root user so you need to run the following command: export KUBECONFIG=/etc/kubernetes/admin.conf We need to run the below commands as a normal user to use the kubectl from terminal. mkdir -p $HOME/.kube sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config sudo chown $(id -u):$(id -g) $HOME/.kube/config Now the machine is initialized as master. Warning Kubeadm signs the certificate in the admin.conf to have Subject: O = system:masters, CN = kubernetes-admin. system:masters is a break-glass, super user group that bypasses the authorization layer (e.g. RBAC). Do not share the admin.conf file with anyone and instead grant users custom permissions by generating them a kubeconfig file using the kubeadm kubeconfig user command. B. Join worker nodes running following command on individual workder nodes: kubeadm join 192.168.0.167:6443 --token cnslau.kd5fjt96jeuzymzb \\ --discovery-token-ca-cert-hash sha256:871ab3f050bc9790c977daee9e44cf52e15ee37ab9834567333b939458a5bfb5 Important Note Your output will be different than what is provided here. While performing the rest of the demo, ensure that you are executing the command provided by your output and dont copy and paste from here. If you do not have the token, you can get it by running the following command on the control-plane node: kubeadm token list The output is similar to this: TOKEN TTL EXPIRES USAGES DESCRIPTION EXTRA GROUPS 8ewj1p... 23h 2018-06-12 authentication, The default bootstrap system: signing token generated by bootstrappers: 'kubeadm init'. kubeadm: default-node-token If you missed the join command, execute the following command kubeadm token create --print-join-command in the master node to recreate the token with the join command. root@master:~$ kubeadm token create --print-join-command kubeadm join 10.2.0.4:6443 --token xyzeyi.wxer3eg9vj8hcpp2 \\ --discovery-token-ca-cert-hash sha256:ccfc92b2a31b002c3151cdbab77ff4dc32ef13b213fa3a9876e126831c76f7fa By default, tokens expire after 24 hours. If you are joining a node to the cluster after the current token has expired, you can create a new token by running the following command on the control-plane node: kubeadm token create The output is similar to this: 5didvk.d09sbcov8ph2amjw We can use this new token to join: $ kubeadm join <master-ip>:<master-port> --token <token> \\ --discovery-token-ca-cert-hash sha256:<hash> Now that you have initialized the master - you can now work on bootstrapping the worker nodes. SSH into worker1 and worker2 Switch to root user on both the machines: sudo su Check the output given by the init command on master to join worker node: kubeadm join 192.168.0.167:6443 --token cnslau.kd5fjt96jeuzymzb \\ --discovery-token-ca-cert-hash sha256:871ab3f050bc9790c977daee9e44cf52e15ee37ab9834567333b939458a5bfb5 Execute the above command on both the nodes: Your output should look like: This node has joined the cluster: * Certificate signing request was sent to apiserver and a response was received. * The Kubelet was informed of the new secure connection details.","title":"Configure kubeadm to bootstrap the cluster on master node"},{"location":"other-tools/kubernetes/kubeadm/single-master-clusters-with-kubeadm/#validate-all-cluster-components-and-nodes-are-visible-on-all-nodes","text":"Verify the cluster kubectl get nodes NAME STATUS ROLES AGE VERSION master NotReady control-plane,master 21m v1.16.2 worker1 Ready <none> 9m17s v1.16.2 worker2 Ready <none> 9m25s v1.16.2","title":"Validate all cluster components and nodes are visible on all nodes"},{"location":"other-tools/kubernetes/kubeadm/single-master-clusters-with-kubeadm/#install-cni-network-plugin","text":"","title":"Install CNI network plugin"},{"location":"other-tools/kubernetes/kubeadm/single-master-clusters-with-kubeadm/#cni-overview","text":"Managing a network where containers can interoperate efficiently is very important. Kubernetes has adopted the Container Network Interface(CNI) specification for managing network resources on a cluster. This relatively simple specification makes it easy for Kubernetes to interact with a wide range of CNI-based software solutions. Using this CNI plugin allows Kubernetes pods to have the same IP address inside the pod as they do on the VPC network. Make sure the configuration corresponds to the Pod CIDR specified in the kubeadm configuration file if applicable. You must deploy a CNI based Pod network add-on so that your Pods can communicate with each other. Cluster DNS (CoreDNS) will not start up before a network is installed. To verify you can run this command: kubectl get po -n kube-system : You should see the following output. You will see the two coredns-* pods in a pending state. It is the expected behavior. Once we install the network plugin, it will be in a Running state. Output Example: root@master:~$ kubectl get po -n kube-system NAME READY STATUS RESTARTS AGE coredns-558bd4d5db-5jktc 0/1 Pending 0 10m coredns-558bd4d5db-xdc5x 0/1 Pending 0 10m etcd-master1 1/1 Running 0 11m kube-apiserver-master1 1/1 Running 0 11m kube-controller-manager-master1 1/1 Running 0 11m kube-proxy-5jfh5 1/1 Running 0 10m kube-scheduler-master1 1/1 Running 0 11m","title":"CNI overview"},{"location":"other-tools/kubernetes/kubeadm/single-master-clusters-with-kubeadm/#supported-cni-options","text":"To read more about the currently supported base CNI solutions for Kubernetes read here and also read this . The below command can be run on the master node to install the CNI plugin: kubectl apply -f https://github.com/coreos/flannel/raw/master/Documentation/kube-flannel.yml As you had passed --pod-network-cidr=10.244.0.0/16 with kubeadm init so this should work for Flannel CNI. Using Other CNI Options For Calico CNI plugin to work correctly, you need to pass --pod-network-cidr=192.168.0.0/16 with kubeadm init and then you can run: kubectl apply -f https://docs.projectcalico.org/v3.8/manifests/calico.yaml For Weave Net CNI plugin to work correctly, you don't need to pass --pod-network-cidr with kubeadm init and then you can run: kubectl apply -f \"https://cloud.weave.works/k8s/net?k8s-version=$(kubectl version | base64 | tr -d '\\n')\" Dual Network: It is highly recommended to follow an internal/external network layout for your cluster, as showed in this diagram: To enable this just give two different names to the internal and external interface, according to your distro of choiche naming scheme: external_interface: eth0 internal_interface: eth1 Also you can decide here what CIDR should your cluster use cluster_cidr: 10.43.0.0/16 service_cidr: 10.44.0.0/16 Once you successfully installed the Flannel CNI component to your cluster. You can now verify your HA cluster running: kubectl get nodes NAME STATUS ROLES AGE VERSION master Ready control-plane,master 22m v1.16.2 worker1 Ready <none> 10m v1.16.2 worker2 Ready <none> 10m v1.16.2","title":"Supported CNI options"},{"location":"other-tools/kubernetes/kubeadm/single-master-clusters-with-kubeadm/#watch-recoded-video","text":"Here\u2019s a quick recorded demo video upto this point where we successfully setup single master K8s cluster using Kubeadm.","title":"Watch Recoded Video"},{"location":"other-tools/kubernetes/kubeadm/single-master-clusters-with-kubeadm/#deploy-a-sample-nginx-application","text":"Now that we have all the components to make the cluster and applications work, let\u2019s deploy a sample Nginx application and see if we can access it over a NodePort that has port range of 30000-32767 . The below command can be run on: kubectl run nginx --image=nginx --port=80 kubectl expose pod nginx --port=80 --type=NodePort To check which NodePort is opened and running the Nginx run: kubectl get svc The output will show: Once the deployment is up, you should be able to access the Nginx home page on the allocated NodePort from either of the worker nodes' Floating IP. To check which worker node is serving nginx , you can check NODE column running the following command: kubectl get pods --all-namespaces --output wide OR, kubectl get pods -A -o wide This will shows like below: Go to browser, visit http://<Worker-Floating-IP>:<NodePort> i.e. http://128.31.25.246:32713 to check the nginx default page. Here Worker_Floating-IP corresponds to the Floating IP of the nginx pod running worker node i.e. worker2 . For your example,","title":"Deploy A Sample Nginx Application"},{"location":"other-tools/kubernetes/kubeadm/single-master-clusters-with-kubeadm/#deploy-a-k8s-dashboard","text":"You will going to setup K8dash/Skooner to view a dashboard that shows all your K8s cluster components. SSH into master node Switch to root user: sudo su Apply available deployment by running the following command: kubectl apply -f https://raw.githubusercontent.com/skooner-k8s/skooner/master/kubernetes-skooner-nodeport.yaml This will map Skooner port 4654 to a randomly selected port from the master node. The assigned NodePort on the master node can be found running: kubectl get svc --namespace=kube-system OR, kubectl get po,svc -n kube-system To check which worker node is serving skooner-* , you can check NODE column running the following command: kubectl get pods --all-namespaces --output wide OR, kubectl get pods -A -o wide This will shows like below: Go to browser, visit http://<Worker-Floating-IP>:<NodePort> i.e. http://128.31.25.246:30495 to check the skooner dashboard page. Here Worker_Floating-IP corresponds to the Floating IP of the skooner-* pod running worker node i.e. worker2 . Setup the Service Account Token to access the Skooner Dashboard: The first (and easiest) option is to create a dedicated service account. Run the following commands: Create the service account in the current namespace (we assume default) kubectl create serviceaccount skooner-sa Give that service account root on the cluster kubectl create clusterrolebinding skooner-sa --clusterrole=cluster-admin --serviceaccount=default:skooner-sa Find the secret that was created to hold the token for the SA kubectl get secrets Show the contents of the secret to extract the token kubectl describe secret skooner-sa-token-xxxxx Copy the token value from the secret and enter it into the login screen to access the dashboard.","title":"Deploy A K8s Dashboard"},{"location":"other-tools/kubernetes/kubeadm/single-master-clusters-with-kubeadm/#watch-demo-video","text":"Here\u2019s a recorded demo video on how to deploy applications on top of setup single master K8s cluster as explained above.","title":"Watch Demo Video"},{"location":"other-tools/kubernetes/kubeadm/single-master-clusters-with-kubeadm/#clean-up","text":"To view the Cluster info: kubectl cluster-info To delete your local references to the cluster: kubectl config delete-cluster","title":"Clean Up"},{"location":"other-tools/kubernetes/kubeadm/single-master-clusters-with-kubeadm/#how-to-remove-the-node","text":"Talking to the control-plane node with the appropriate credentials, run: kubectl drain <node name> --delete-emptydir-data --force --ignore-daemonsets Before removing the node, reset the state installed by kubeadm: kubeadm reset The reset process does not reset or clean up iptables rules or IPVS tables. If you wish to reset iptables, you must do so manually: iptables -F && iptables -t nat -F && iptables -t mangle -F && iptables -X If you want to reset the IPVS tables, you must run the following command: ipvsadm -C Now remove the node: kubectl delete node <node name> If you wish to start over, run kubeadm init or kubeadm join with the appropriate arguments.","title":"How to Remove the node?"}]}